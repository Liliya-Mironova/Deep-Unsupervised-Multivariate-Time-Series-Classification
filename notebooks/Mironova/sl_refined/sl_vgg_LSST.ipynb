{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"VGG\"\n",
    "magic_dim = 9216\n",
    "\n",
    "# model_name = \"ResNet\"\n",
    "# magic_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"LSST\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"/root/data/Multivariate_ts\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 20   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './resnet1d_exp' # experiments results dir\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 500\n",
    "lr=0.003     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 10\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sktime.utils.load_data import load_from_tsfile_to_dataframe\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_torch(X):\n",
    "    X = X.applymap(np.array)\n",
    "    dimensions_lst = []\n",
    "\n",
    "    for dim in X.columns:\n",
    "        dimensions_lst.append(np.dstack(list(X[dim].values))[0])\n",
    "\n",
    "    dimensions_lst = np.array(dimensions_lst)\n",
    "    X = torch.from_numpy(np.array(dimensions_lst, dtype=np.float64))\n",
    "    X = X.transpose(0, 2)\n",
    "    X = X.transpose(1, 2)\n",
    "    X = F.normalize(X, dim=1)\n",
    "    return X.float()\n",
    "\n",
    "def answers_to_torch(y):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    y = torch.from_numpy(np.array(y, dtype=np.int32))\n",
    "    y = y.long()\n",
    "    return y\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TRAIN.ts')\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TEST.ts')\n",
    "\n",
    "X_train = features_to_torch(X_train)\n",
    "X_test = features_to_torch(X_test)\n",
    "\n",
    "y_train = answers_to_torch(y_train)\n",
    "y_test = answers_to_torch(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_steps: 36\n",
      "train samples_num: 2459\n",
      "dims_num: 6\n",
      "num_classes: 14\n"
     ]
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "time_steps = X_train.shape[2]\n",
    "dims_num = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print('time_steps:', time_steps)\n",
    "print('train samples_num:', N)\n",
    "print('dims_num:', dims_num)\n",
    "print('num_classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters (AlexNet in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10                 # number of heads\n",
    "ncl=num_classes       # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# # (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "# CFG = {\n",
    "#     'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "#     'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['resnetv1','resnetv1_18']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, in_channel=3, width=1, num_classes=[1000]):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        self.base = int(16 * width)\n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(in_channel, 16, kernel_size=3, padding=1, bias=False), # [100, 16, 36]\n",
    "                            nn.BatchNorm1d(16),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            self._make_layer(block, self.base, layers[0]),                   # [100, 16, 36]\n",
    "                            self._make_layer(block, self.base * 2, layers[1]),               # [100, 32, 36]\n",
    "                            self._make_layer(block, self.base * 4, layers[2]),               # [100, 64, 36]\n",
    "                            self._make_layer(block, self.base * 8, layers[3]),               # [100, 128, 36]\n",
    "                            nn.AvgPool1d(2),                                                 # [100, 128, 18]\n",
    "        ])\n",
    "    \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnetv1_18(num_classes=[1000]):\n",
    "    \"\"\"Encoder for instance discrimination and MoCo\"\"\"\n",
    "    return resnet18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        \n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(dims_num, 64, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.AvgPool1d(kernel_size=2)#, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "        ])\n",
    "        \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())  # [50, 10, 400] -> [50, 512, 12]\n",
    "        out = out.view(out.size(0), -1) # [50, magic_dim]\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "                print (out.size())\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "        \n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((N, ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((N, magic_dim)) # knn_dim\n",
    "    \n",
    "    for batch_idx, (data, _, _selected) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data.float())\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy() # p: [20, magic_dim]\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(acc, model_name, dataset_name):\n",
    "    step = 3\n",
    "    x = np.arange(1, epochs//step + 1)\n",
    "    acc = acc[::step]\n",
    "    plt.plot(x*step, acc[1:])\n",
    "    plt.xlabel(\"# epoch\")\n",
    "    plt.ylabel(\"Accuracy, %\")\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Train accuracy, self-labeling, {model_name}, {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(model_name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N // batch_size + batch_idx\n",
    "        if niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h], selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "#         if True:\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N // batch_size, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "#             writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*N/batch_size)\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG created\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"ResNet\":\n",
    "    model = resnet18(num_classes=numc, in_channel=dims_num)\n",
    "else:\n",
    "    model = VGG(num_classes=numc)\n",
    "print (model_name, \"created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [30.0, 21.0, 20.95, 20.89, 20.84, 20.79, 20.74, 20.68, 20.63, 20.58, 20.53, 20.47, 20.42, 20.37, 20.32, 20.26, 20.21, 20.16, 20.11, 20.05, 20.0, 19.95, 19.89, 19.84, 19.79, 19.74, 19.68, 19.63, 19.58, 19.53, 19.47, 19.42, 19.37, 19.32, 19.26, 19.21, 19.16, 19.11, 19.05, 19.0, 18.95, 18.89, 18.84, 18.79, 18.74, 18.68, 18.63, 18.58, 18.53, 18.47, 18.42, 18.37, 18.32, 18.26, 18.21, 18.16, 18.11, 18.05, 18.0, 17.95, 17.89, 17.84, 17.79, 17.74, 17.68, 17.63, 17.58, 17.53, 17.47, 17.42, 17.37, 17.32, 17.26, 17.21, 17.16, 17.11, 17.05, 17.0, 16.95, 16.89, 16.84, 16.79, 16.74, 16.68, 16.63, 16.58, 16.53, 16.47, 16.42, 16.37, 16.32, 16.26, 16.21, 16.16, 16.11, 16.05, 16.0, 15.95, 15.89, 15.84, 15.79, 15.74, 15.68, 15.63, 15.58, 15.53, 15.47, 15.42, 15.37, 15.32, 15.26, 15.21, 15.16, 15.11, 15.05, 15.0, 14.95, 14.89, 14.84, 14.79, 14.74, 14.68, 14.63, 14.58, 14.53, 14.47, 14.42, 14.37, 14.32, 14.26, 14.21, 14.16, 14.11, 14.05, 14.0, 13.95, 13.89, 13.84, 13.79, 13.74, 13.68, 13.63, 13.58, 13.53, 13.47, 13.42, 13.37, 13.32, 13.26, 13.21, 13.16, 13.11, 13.05, 13.0, 12.95, 12.89, 12.84, 12.79, 12.74, 12.68, 12.63, 12.58, 12.53, 12.47, 12.42, 12.37, 12.32, 12.26, 12.21, 12.16, 12.11, 12.05, 12.0, 11.95, 11.89, 11.84, 11.79, 11.74, 11.68, 11.63, 11.58, 11.53, 11.47, 11.42, 11.37, 11.32, 11.26, 11.21, 11.16, 11.11, 11.05, 11.0, 10.95, 10.89, 10.84, 10.79, 10.74, 10.68, 10.63, 10.58, 10.53, 10.47, 10.42, 10.37, 10.32, 10.26, 10.21, 10.16, 10.11, 10.05, 10.0, 9.95, 9.89, 9.84, 9.79, 9.74, 9.68, 9.63, 9.58, 9.53, 9.47, 9.42, 9.37, 9.32, 9.26, 9.21, 9.16, 9.11, 9.05, 9.0, 8.95, 8.89, 8.84, 8.79, 8.74, 8.68, 8.63, 8.58, 8.53, 8.47, 8.42, 8.37, 8.32, 8.26, 8.21, 8.16, 8.11, 8.05, 8.0, 7.95, 7.89, 7.84, 7.79, 7.74, 7.68, 7.63, 7.58, 7.53, 7.47, 7.42, 7.37, 7.32, 7.26, 7.21, 7.16, 7.11, 7.05, 7.0, 6.95, 6.89, 6.84, 6.79, 6.74, 6.68, 6.63, 6.58, 6.53, 6.47, 6.42, 6.37, 6.32, 6.26, 6.21, 6.16, 6.11, 6.05, 6.0, 5.95, 5.89, 5.84, 5.79, 5.74, 5.68, 5.63, 5.58, 5.53, 5.47, 5.42, 5.37, 5.32, 5.26, 5.21, 5.16, 5.11, 5.05, 5.0, 4.95, 4.89, 4.84, 4.79, 4.74, 4.68, 4.63, 4.58, 4.53, 4.47, 4.42, 4.37, 4.32, 4.26, 4.21, 4.16, 4.11, 4.05, 4.0, 3.95, 3.89, 3.84, 3.79, 3.74, 3.68, 3.63, 3.58, 3.53, 3.47, 3.42, 3.37, 3.32, 3.26, 3.21, 3.16, 3.11, 3.05, 3.0, 2.95, 2.89, 2.84, 2.79, 2.74, 2.68, 2.63, 2.58, 2.53, 2.47, 2.42, 2.37, 2.32, 2.26, 2.21, 2.16, 2.11, 2.05, 2.0, 1.95, 1.89, 1.84, 1.79, 1.74, 1.68, 1.63, 1.58, 1.53, 1.47, 1.42, 1.37, 1.32, 1.26, 1.21, 1.16, 1.11, 1.05, 1.0, 0.95, 0.89, 0.84, 0.79, 0.74, 0.68, 0.63, 0.58, 0.53, 0.47, 0.42, 0.37, 0.32, 0.26, 0.21, 0.16, 0.11, 0.05, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'./runs/{dataset_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(net, K, sigma=0.1, dim=128, use_pca=False):\n",
    "    net.eval()\n",
    "    # this part is ugly but made to be backwards-compatible. there was a change in cifar dataset's structure.\n",
    "    trainLabels = y_train\n",
    "    LEN = N\n",
    "    C = trainLabels.max() + 1\n",
    "\n",
    "    trainFeatures = torch.zeros((magic_dim, LEN))  # , device='cuda:0') # dim\n",
    "    normalize = Normalize()\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=False)):\n",
    "        batchSize = batch_size\n",
    "        inputs = inputs.cuda()\n",
    "        features = net(inputs.float())\n",
    "        if not use_pca:\n",
    "            features = normalize(features)\n",
    "        trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu()\n",
    "        \n",
    "    if use_pca:\n",
    "        comps = 4\n",
    "        print('doing PCA with %s components'%comps, end=' ')\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=comps, whiten=False)\n",
    "        trainFeatures = pca.fit_transform(trainFeatures.numpy().T)\n",
    "        trainFeatures = torch.Tensor(trainFeatures)\n",
    "        trainFeatures = normalize(trainFeatures).t()\n",
    "        print('..done')\n",
    "    def eval_k_s(K_,sigma_):\n",
    "        total = 0\n",
    "        top1 = 0.\n",
    "        top5 = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            retrieval_one_hot = torch.zeros(K_, C)# .cuda()\n",
    "            for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=False)):\n",
    "                targets = targets # .cuda(async=True) # or without async for py3.7\n",
    "                inputs = inputs.cuda()\n",
    "                batchSize = batch_size\n",
    "                features = net(inputs)\n",
    "                if use_pca:\n",
    "                    features = pca.transform(features.cpu().numpy())\n",
    "                    features = torch.Tensor(features).cuda()\n",
    "                features = normalize(features).cpu()\n",
    "\n",
    "                dist = torch.mm(features, trainFeatures)\n",
    "\n",
    "                yd, yi = dist.topk(K_, dim=1, largest=True, sorted=True)\n",
    "                candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "                retrieval = torch.gather(candidates, 1, yi).long()\n",
    "\n",
    "                retrieval_one_hot.resize_(batchSize * K_, C).zero_()\n",
    "                retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1.)\n",
    "                \n",
    "                yd_transform = yd.clone().div_(sigma_).exp_()\n",
    "                probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C),\n",
    "                                            yd_transform.view(batchSize, -1, 1)),\n",
    "                                  1)\n",
    "                _, predictions = probs.sort(1, True)\n",
    "\n",
    "                # Find which predictions match the target\n",
    "                correct = predictions.eq(targets.data.view(-1, 1))\n",
    "\n",
    "                top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "                top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        print(f\"{K_}-NN,s={sigma_}: TOP1: \", top1 * 100. / total)\n",
    "        return top1 / total\n",
    "\n",
    "    if isinstance(K, list):\n",
    "        res = []\n",
    "        for K_ in K:\n",
    "            for sigma_ in sigma:\n",
    "                res.append(eval_k_s(K_, sigma_))\n",
    "        return res\n",
    "    else:\n",
    "        res = eval_k_s(K, sigma)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "VGG\n",
      "error:  0.0 step  11\n",
      "cost:  2.6355855209450105\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [0][0/4]Time: 0.292 (0.292) Data: 0.202 (0.202) Loss: 2.6387 (2.6387)\n",
      "error:  3.6222874699731733e-09 step  31\n",
      "cost:  3.514087336898735\n",
      "opt took 0.00min,   31iters\n",
      "error:  1.251115217981713e-06 step  11\n",
      "cost:  2.5011527757794134\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.791978706064626e-07 step  11\n",
      "cost:  2.5201848989108364\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  25.6\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 4 components ..done\n",
      "50-NN,s=0.1: TOP1:  26.35\n",
      "50-NN,s=0.5: TOP1:  26.35\n",
      "10-NN,s=0.1: TOP1:  24.5\n",
      "10-NN,s=0.5: TOP1:  24.55\n",
      "best accuracy: 25.60\n",
      "\n",
      "Epoch: 1\n",
      "VGG\n",
      "error:  1.4075383081291193e-09 step  11\n",
      "cost:  2.4979447697746746\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [1][0/4]Time: 0.235 (0.235) Data: 0.170 (0.170) Loss: 2.6138 (2.6138)\n",
      "error:  0.00039365705268556983 step  11\n",
      "cost:  2.299137568940669\n",
      "opt took 0.00min,   11iters\n",
      "error:  5.92175612289525e-05 step  11\n",
      "cost:  2.3379773772011823\n",
      "opt took 0.00min,   11iters\n",
      "error:  7.886164260617612e-06 step  11\n",
      "cost:  2.2667940251880596\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  23.75\n",
      "best accuracy: 25.60\n",
      "\n",
      "Epoch: 2\n",
      "VGG\n",
      "error:  6.578463884743257e-08 step  11\n",
      "cost:  2.5106181340403797\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [2][0/4]Time: 0.215 (0.215) Data: 0.149 (0.149) Loss: 2.5269 (2.5269)\n",
      "error:  8.095339573932492e-05 step  11\n",
      "cost:  2.378153622988311\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.4656065445458637e-07 step  21\n",
      "cost:  2.233291837577961\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.0004923643987295723 step  21\n",
      "cost:  2.1808323960382583\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  24.2\n",
      "best accuracy: 25.60\n",
      "\n",
      "Epoch: 3\n",
      "VGG\n",
      "error:  6.1716095867137e-07 step  21\n",
      "cost:  2.459252437490185\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [3][0/4]Time: 0.240 (0.240) Data: 0.165 (0.165) Loss: 2.2880 (2.2880)\n",
      "error:  3.450678203298363e-05 step  11\n",
      "cost:  2.296651568758664\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.00015766035164954673 step  11\n",
      "cost:  2.2037174703384443\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.003611798996613791 step  21\n",
      "cost:  2.1416128349855206\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  21.3\n",
      "best accuracy: 25.60\n",
      "\n",
      "Epoch: 4\n",
      "VGG\n",
      "error:  9.411547168891587e-05 step  11\n",
      "cost:  2.4743209441495453\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [4][0/4]Time: 0.236 (0.236) Data: 0.169 (0.169) Loss: 2.1462 (2.1462)\n",
      "error:  1.31435335722907e-05 step  11\n",
      "cost:  2.2515742086814674\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0008244586578706192 step  11\n",
      "cost:  2.171508260487001\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.246420851616861e-07 step  21\n",
      "cost:  2.0026801034076023\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  29.15\n",
      "Saving..\n",
      "best accuracy: 29.15\n",
      "\n",
      "Epoch: 5\n",
      "VGG\n",
      "error:  0.0017634084500502967 step  11\n",
      "cost:  2.326563594712514\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [5][0/4]Time: 0.221 (0.221) Data: 0.155 (0.155) Loss: 1.9892 (1.9892)\n",
      "error:  1.5683991997361701e-06 step  21\n",
      "cost:  2.1578159754539334\n",
      "opt took 0.00min,   21iters\n",
      "error:  2.562582051057749e-05 step  21\n",
      "cost:  2.4158020678274377\n",
      "opt took 0.00min,   21iters\n",
      "error:  3.784723506827792e-05 step  21\n",
      "cost:  2.152616162925934\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  33.85\n",
      "Saving..\n",
      "best accuracy: 33.85\n",
      "\n",
      "Epoch: 6\n",
      "VGG\n",
      "error:  1.620139044034019e-07 step  21\n",
      "cost:  2.4038890547947425\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [6][0/4]Time: 0.235 (0.235) Data: 0.168 (0.168) Loss: 1.8566 (1.8566)\n",
      "error:  1.8011072622559077e-05 step  21\n",
      "cost:  2.4097311829644634\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.00010136270203442432 step  21\n",
      "cost:  2.3244986778023686\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.00015465786815760385 step  21\n",
      "cost:  2.2078762745933567\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  33.65\n",
      "best accuracy: 33.85\n",
      "\n",
      "Epoch: 7\n",
      "VGG\n",
      "error:  2.9653007604935055e-08 step  11\n",
      "cost:  2.519794910578894\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [7][0/4]Time: 0.227 (0.227) Data: 0.161 (0.161) Loss: 1.7686 (1.7686)\n",
      "error:  0.00021064876127874221 step  11\n",
      "cost:  2.311964785508619\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0029512233736660454 step  11\n",
      "cost:  2.099803725117322\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.5058872199501536e-08 step  21\n",
      "cost:  1.9270637321277193\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  34.0\n",
      "Saving..\n",
      "best accuracy: 34.00\n",
      "\n",
      "Epoch: 8\n",
      "VGG\n",
      "error:  2.5068803477523716e-07 step  11\n",
      "cost:  2.5205777067807023\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [8][0/4]Time: 0.221 (0.221) Data: 0.156 (0.156) Loss: 1.5988 (1.5988)\n",
      "error:  4.691401257472805e-07 step  11\n",
      "cost:  2.3318212636980586\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.00026453469450171685 step  11\n",
      "cost:  2.0466997541543006\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.32698585231239e-07 step  21\n",
      "cost:  1.8131777333593833\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  30.4\n",
      "best accuracy: 34.00\n",
      "\n",
      "Epoch: 9\n",
      "VGG\n",
      "error:  0.00010624024314287439 step  11\n",
      "cost:  2.470393480304821\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [9][0/4]Time: 0.235 (0.235) Data: 0.162 (0.162) Loss: 1.4713 (1.4713)\n",
      "error:  3.98753564068155e-06 step  11\n",
      "cost:  2.2441222029642454\n",
      "opt took 0.00min,   11iters\n",
      "error:  9.542289735042786e-07 step  11\n",
      "cost:  2.1070903332777213\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.987556224056444e-05 step  11\n",
      "cost:  2.0218483652488635\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  35.85\n",
      "Saving..\n",
      "best accuracy: 35.85\n",
      "\n",
      "Epoch: 10\n",
      "VGG\n",
      "error:  0.0019730282495235008 step  71\n",
      "cost:  5.540552773164764\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [10][0/4]Time: 0.235 (0.235) Data: 0.168 (0.168) Loss: 1.9598 (1.9598)\n",
      "error:  0.001404144615079428 step  51\n",
      "cost:  4.082914614248076\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.0027546260666614364 step  51\n",
      "cost:  3.6297927005073527\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.0009066005907933938 step  71\n",
      "cost:  3.6708964726887676\n",
      "opt took 0.00min,   71iters\n",
      "10-NN,s=0.1: TOP1:  36.85\n",
      "Saving..\n",
      "best accuracy: 36.85\n",
      "\n",
      "Epoch: 11\n",
      "VGG\n",
      "error:  0.007368355011807659 step  41\n",
      "cost:  3.569938513359565\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [11][0/4]Time: 0.224 (0.224) Data: 0.158 (0.158) Loss: 2.0367 (2.0367)\n",
      "error:  0.0007051116843682648 step  41\n",
      "cost:  3.0266139069185427\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.00024420690213367013 step  31\n",
      "cost:  2.547024768583102\n",
      "opt took 0.00min,   31iters\n",
      "error:  4.963402924984539e-05 step  31\n",
      "cost:  2.6993978026247327\n",
      "opt took 0.00min,   31iters\n",
      "10-NN,s=0.1: TOP1:  38.05\n",
      "Saving..\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 12\n",
      "VGG\n",
      "error:  0.00048500196293121967 step  51\n",
      "cost:  2.7423822597984944\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [12][0/4]Time: 0.259 (0.259) Data: 0.187 (0.187) Loss: 1.8747 (1.8747)\n",
      "error:  0.0005315260102221053 step  61\n",
      "cost:  2.738797159071049\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.008171545828084326 step  71\n",
      "cost:  2.605644208584692\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.0027696190355043226 step  41\n",
      "cost:  2.511155536944532\n",
      "opt took 0.00min,   41iters\n",
      "10-NN,s=0.1: TOP1:  33.5\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 13\n",
      "VGG\n",
      "error:  0.005476409968617202 step  51\n",
      "cost:  3.3820484694536774\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [13][0/4]Time: 0.226 (0.226) Data: 0.160 (0.160) Loss: 1.8546 (1.8546)\n",
      "error:  0.00721112906961896 step  51\n",
      "cost:  3.331239263396163\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.0014755981420130349 step  51\n",
      "cost:  2.99697338487307\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.00468178016188725 step  51\n",
      "cost:  2.671236040034353\n",
      "opt took 0.00min,   51iters\n",
      "10-NN,s=0.1: TOP1:  34.45\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 14\n",
      "VGG\n",
      "error:  0.0028569159009753653 step  51\n",
      "cost:  3.0317438767127163\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [14][0/4]Time: 0.229 (0.229) Data: 0.163 (0.163) Loss: 1.8983 (1.8983)\n",
      "error:  0.007228238564485623 step  61\n",
      "cost:  2.8914346003108484\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.003931056047418502 step  91\n",
      "cost:  2.773043931513156\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.007815104738085998 step  61\n",
      "cost:  2.575912732102776\n",
      "opt took 0.00min,   61iters\n",
      "10-NN,s=0.1: TOP1:  31.95\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 15\n",
      "VGG\n",
      "error:  0.005408145665363517 step  141\n",
      "cost:  2.949881532089203\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [15][0/4]Time: 0.237 (0.237) Data: 0.171 (0.171) Loss: 1.8849 (1.8849)\n",
      "error:  0.0035669112935089453 step  91\n",
      "cost:  2.7901627573517693\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.0019922936752417852 step  71\n",
      "cost:  2.6700929671532614\n",
      "opt took 0.00min,   71iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.007531093639683184 step  71\n",
      "cost:  2.6236246615226886\n",
      "opt took 0.00min,   71iters\n",
      "10-NN,s=0.1: TOP1:  32.95\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 16\n",
      "VGG\n",
      "error:  0.003694374658156252 step  91\n",
      "cost:  2.7564363831031\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [16][0/4]Time: 0.231 (0.231) Data: 0.166 (0.166) Loss: 1.9574 (1.9574)\n",
      "error:  0.006803512193455319 step  111\n",
      "cost:  2.667986983456185\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.009200507534560809 step  151\n",
      "cost:  2.62516183261115\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.00584825028928837 step  121\n",
      "cost:  2.403013245752802\n",
      "opt took 0.00min,  121iters\n",
      "10-NN,s=0.1: TOP1:  33.95\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 17\n",
      "VGG\n",
      "error:  0.004846558107547194 step  151\n",
      "cost:  2.8843215276785985\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [17][0/4]Time: 0.238 (0.238) Data: 0.173 (0.173) Loss: 2.0180 (2.0180)\n",
      "error:  0.006288331616299936 step  191\n",
      "cost:  2.9969393624112333\n",
      "opt took 0.00min,  191iters\n",
      "error:  0.008300182627599462 step  161\n",
      "cost:  2.6989434233462624\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.006877004970149203 step  131\n",
      "cost:  2.4955623473226374\n",
      "opt took 0.00min,  131iters\n",
      "10-NN,s=0.1: TOP1:  35.1\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 18\n",
      "VGG\n",
      "error:  0.0058400463144464565 step  221\n",
      "cost:  3.2381311492238574\n",
      "opt took 0.00min,  221iters\n",
      "Epoch: [18][0/4]Time: 0.245 (0.245) Data: 0.180 (0.180) Loss: 2.1379 (2.1379)\n",
      "error:  0.007341272069901095 step  141\n",
      "cost:  2.741916527206261\n",
      "opt took 0.00min,  141iters\n",
      "error:  0.005505703839067233 step  131\n",
      "cost:  2.53862094120228\n",
      "opt took 0.00min,  131iters\n",
      "error:  0.00628177106360539 step  171\n",
      "cost:  2.5875641527633686\n",
      "opt took 0.00min,  171iters\n",
      "10-NN,s=0.1: TOP1:  34.35\n",
      "best accuracy: 38.05\n",
      "\n",
      "Epoch: 19\n",
      "VGG\n",
      "error:  0.005620812372974737 step  181\n",
      "cost:  2.665884091830192\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [19][0/4]Time: 0.246 (0.246) Data: 0.180 (0.180) Loss: 2.1349 (2.1349)\n",
      "error:  0.004970782219796299 step  111\n",
      "cost:  2.5541661566671836\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.00567680697344608 step  171\n",
      "cost:  2.4636238549258516\n",
      "opt took 0.00min,  171iters\n",
      "error:  0.007247705676881155 step  261\n",
      "cost:  2.5108896036833617\n",
      "opt took 0.00min,  261iters\n",
      "10-NN,s=0.1: TOP1:  35.65\n",
      "best accuracy: 38.05\n",
      "doing PCA with 4 components ..done\n",
      "10-NN,s=0.1: TOP1:  33.35\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "best_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    \n",
    "    acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim)\n",
    "    accuracies.append(acc)\n",
    "    feature_return_switch(model, False)\n",
    "#     writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "        best_accuracies.append(best_acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = my_kNN(model, K=[50, 10], sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "#         for num_nn in [50, 10]:\n",
    "#             for sig in [0.1, 0.5]:\n",
    "#                 writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "#                 i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "end = time.time()\n",
    "\n",
    "# checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "# model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc(accuracies, model_name, dataset_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-labeling\n",
      "-------------------------\n",
      "Model:      VGG\n",
      "Dataset:    LSST\n",
      "Epochs:     20\n",
      "lr:         0.003\n",
      "Batch size: 500\n",
      "Heads:      10\n",
      "Time:       29.579330444335938\n",
      "Best acc:   38.05 %\n"
     ]
    }
   ],
   "source": [
    "print (\"Self-labeling\")\n",
    "print (\"-------------------------\")\n",
    "print (\"Model:     \", model_name)\n",
    "print (\"Dataset:   \", dataset_name)\n",
    "print (\"Epochs:    \", epochs)\n",
    "print (\"lr:        \", lr)\n",
    "print (\"Batch size:\", batch_size)\n",
    "print (\"Heads:     \", hc)\n",
    "print (\"Time:      \", end-start)\n",
    "print (\"Best acc:  \", best_acc*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
