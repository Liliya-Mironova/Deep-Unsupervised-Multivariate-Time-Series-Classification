{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"VGG\"\n",
    "# magic_dim = 2048\n",
    "\n",
    "model_name = \"ResNet\"\n",
    "magic_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"PenDigits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"/root/data/Multivariate_ts\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 30   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './resnet1d_exp' # experiments results dir\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 500\n",
    "lr=0.003     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 10\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sktime.utils.load_data import load_from_tsfile_to_dataframe\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_torch(X):\n",
    "    X = X.applymap(np.array)\n",
    "    dimensions_lst = []\n",
    "\n",
    "    for dim in X.columns:\n",
    "        dimensions_lst.append(np.dstack(list(X[dim].values))[0])\n",
    "\n",
    "    dimensions_lst = np.array(dimensions_lst)\n",
    "    X = torch.from_numpy(np.array(dimensions_lst, dtype=np.float64))\n",
    "    X = X.transpose(0, 2)\n",
    "    X = X.transpose(1, 2)\n",
    "    X = F.normalize(X, dim=1)\n",
    "    return X.float()\n",
    "\n",
    "def answers_to_torch(y):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    y = torch.from_numpy(np.array(y, dtype=np.int32))\n",
    "    y = y.long()\n",
    "    return y\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TRAIN.ts')\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TEST.ts')\n",
    "\n",
    "X_train = features_to_torch(X_train)\n",
    "X_test = features_to_torch(X_test)\n",
    "\n",
    "y_train = answers_to_torch(y_train)\n",
    "y_test = answers_to_torch(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_steps: 8\n",
      "train samples_num: 7494\n",
      "dims_num: 2\n",
      "num_classes: 10\n"
     ]
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "time_steps = X_train.shape[2]\n",
    "dims_num = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print('time_steps:', time_steps)\n",
    "print('train samples_num:', N)\n",
    "print('dims_num:', dims_num)\n",
    "print('num_classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters (AlexNet in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10                 # number of heads\n",
    "ncl=num_classes       # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# # (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "# CFG = {\n",
    "#     'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "#     'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['resnetv1','resnetv1_18']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, in_channel=3, width=1, num_classes=[1000]):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        self.base = int(16 * width)\n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(in_channel, 16, kernel_size=3, padding=1, bias=False), # [100, 16, 36]\n",
    "                            nn.BatchNorm1d(16),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            self._make_layer(block, self.base, layers[0]),                   # [100, 16, 36]\n",
    "                            self._make_layer(block, self.base * 2, layers[1]),               # [100, 32, 36]\n",
    "                            self._make_layer(block, self.base * 4, layers[2]),               # [100, 64, 36]\n",
    "                            self._make_layer(block, self.base * 8, layers[3]),               # [100, 128, 36]\n",
    "                            nn.AvgPool1d(2),                                                 # [100, 128, 18]\n",
    "        ])\n",
    "    \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnetv1_18(num_classes=[1000]):\n",
    "    \"\"\"Encoder for instance discrimination and MoCo\"\"\"\n",
    "    return resnet18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        \n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(dims_num, 64, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "\n",
    "#                             nn.Flatten(),\n",
    "#                             nn.Linear(in_features=512 * (time_steps // 2**5), out_features=fc_hidden_dim, bias=True),\n",
    "\n",
    "#                             nn.ReLU(inplace=True),\n",
    "#                             nn.Dropout(p=0.5, inplace=False),\n",
    "#                             nn.Linear(in_features=fc_hidden_dim, out_features=fc_hidden_dim, bias=True),\n",
    "#                             nn.ReLU(inplace=True),\n",
    "#                             nn.Dropout(p=0.5, inplace=False),\n",
    "#                             nn.Linear(in_features=fc_hidden_dim, out_features=num_classes, bias=True),\n",
    "#                             nn.Softmax()\n",
    "        ])\n",
    "        \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())  # [50, 10, 400] -> [50, 512, 12]\n",
    "        out = out.view(out.size(0), -1) # [50, magic_dim]\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "                print (out.size())\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "        \n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((N, ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((N, magic_dim)) # knn_dim\n",
    "    \n",
    "    for batch_idx, (data, _, _selected) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data.float())\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy() # p: [20, magic_dim]\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(model_name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N // batch_size + batch_idx\n",
    "        if niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h], selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "#         if True:\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N // batch_size, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "#             writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*N/batch_size)\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(acc, model_name, dataset_name):\n",
    "    step = 3\n",
    "    x = np.arange(1, epochs//step)\n",
    "    acc = acc[::step]\n",
    "    plt.plot(x*step, acc[1:])\n",
    "    plt.xlabel(\"# epoch\")\n",
    "    plt.ylabel(\"Accuracy, %\")\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Train accuracy, self-labeling, {model_name}, {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet created\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"ResNet\":\n",
    "    model = resnet18(num_classes=numc, in_channel=dims_num)\n",
    "else:\n",
    "    model = VGG(num_classes=numc)\n",
    "print (model_name, \"created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [40.0, 31.0, 30.92, 30.84, 30.77, 30.69, 30.61, 30.53, 30.46, 30.38, 30.3, 30.22, 30.15, 30.07, 29.99, 29.91, 29.83, 29.76, 29.68, 29.6, 29.52, 29.45, 29.37, 29.29, 29.21, 29.14, 29.06, 28.98, 28.9, 28.82, 28.75, 28.67, 28.59, 28.51, 28.44, 28.36, 28.28, 28.2, 28.13, 28.05, 27.97, 27.89, 27.81, 27.74, 27.66, 27.58, 27.5, 27.43, 27.35, 27.27, 27.19, 27.12, 27.04, 26.96, 26.88, 26.8, 26.73, 26.65, 26.57, 26.49, 26.42, 26.34, 26.26, 26.18, 26.11, 26.03, 25.95, 25.87, 25.79, 25.72, 25.64, 25.56, 25.48, 25.41, 25.33, 25.25, 25.17, 25.1, 25.02, 24.94, 24.86, 24.78, 24.71, 24.63, 24.55, 24.47, 24.4, 24.32, 24.24, 24.16, 24.09, 24.01, 23.93, 23.85, 23.77, 23.7, 23.62, 23.54, 23.46, 23.39, 23.31, 23.23, 23.15, 23.08, 23.0, 22.92, 22.84, 22.76, 22.69, 22.61, 22.53, 22.45, 22.38, 22.3, 22.22, 22.14, 22.07, 21.99, 21.91, 21.83, 21.75, 21.68, 21.6, 21.52, 21.44, 21.37, 21.29, 21.21, 21.13, 21.06, 20.98, 20.9, 20.82, 20.74, 20.67, 20.59, 20.51, 20.43, 20.36, 20.28, 20.2, 20.12, 20.05, 19.97, 19.89, 19.81, 19.73, 19.66, 19.58, 19.5, 19.42, 19.35, 19.27, 19.19, 19.11, 19.04, 18.96, 18.88, 18.8, 18.72, 18.65, 18.57, 18.49, 18.41, 18.34, 18.26, 18.18, 18.1, 18.03, 17.95, 17.87, 17.79, 17.71, 17.64, 17.56, 17.48, 17.4, 17.33, 17.25, 17.17, 17.09, 17.02, 16.94, 16.86, 16.78, 16.7, 16.63, 16.55, 16.47, 16.39, 16.32, 16.24, 16.16, 16.08, 16.01, 15.93, 15.85, 15.77, 15.69, 15.62, 15.54, 15.46, 15.38, 15.31, 15.23, 15.15, 15.07, 15.0, 14.92, 14.84, 14.76, 14.68, 14.61, 14.53, 14.45, 14.37, 14.3, 14.22, 14.14, 14.06, 13.99, 13.91, 13.83, 13.75, 13.67, 13.6, 13.52, 13.44, 13.36, 13.29, 13.21, 13.13, 13.05, 12.97, 12.9, 12.82, 12.74, 12.66, 12.59, 12.51, 12.43, 12.35, 12.28, 12.2, 12.12, 12.04, 11.96, 11.89, 11.81, 11.73, 11.65, 11.58, 11.5, 11.42, 11.34, 11.27, 11.19, 11.11, 11.03, 10.95, 10.88, 10.8, 10.72, 10.64, 10.57, 10.49, 10.41, 10.33, 10.26, 10.18, 10.1, 10.02, 9.94, 9.87, 9.79, 9.71, 9.63, 9.56, 9.48, 9.4, 9.32, 9.25, 9.17, 9.09, 9.01, 8.93, 8.86, 8.78, 8.7, 8.62, 8.55, 8.47, 8.39, 8.31, 8.24, 8.16, 8.08, 8.0, 7.92, 7.85, 7.77, 7.69, 7.61, 7.54, 7.46, 7.38, 7.3, 7.23, 7.15, 7.07, 6.99, 6.91, 6.84, 6.76, 6.68, 6.6, 6.53, 6.45, 6.37, 6.29, 6.22, 6.14, 6.06, 5.98, 5.9, 5.83, 5.75, 5.67, 5.59, 5.52, 5.44, 5.36, 5.28, 5.21, 5.13, 5.05, 4.97, 4.89, 4.82, 4.74, 4.66, 4.58, 4.51, 4.43, 4.35, 4.27, 4.2, 4.12, 4.04, 3.96, 3.88, 3.81, 3.73, 3.65, 3.57, 3.5, 3.42, 3.34, 3.26, 3.19, 3.11, 3.03, 2.95, 2.87, 2.8, 2.72, 2.64, 2.56, 2.49, 2.41, 2.33, 2.25, 2.18, 2.1, 2.02, 1.94, 1.86, 1.79, 1.71, 1.63, 1.55, 1.48, 1.4, 1.32, 1.24, 1.17, 1.09, 1.01, 0.93, 0.85, 0.78, 0.7, 0.62, 0.54, 0.47, 0.39, 0.31, 0.23, 0.16, 0.08, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'./runs/{dataset_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(net, K, sigma=0.1, dim=128, use_pca=False):\n",
    "    net.eval()\n",
    "    # this part is ugly but made to be backwards-compatible. there was a change in cifar dataset's structure.\n",
    "    trainLabels = y_train\n",
    "    LEN = N\n",
    "    C = trainLabels.max() + 1\n",
    "\n",
    "    trainFeatures = torch.zeros((magic_dim, LEN))  # , device='cuda:0') # dim\n",
    "    normalize = Normalize()\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=False)):\n",
    "        batchSize = batch_size\n",
    "        inputs = inputs.cuda()\n",
    "        features = net(inputs.float())\n",
    "        if not use_pca:\n",
    "            features = normalize(features)\n",
    "        trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu()\n",
    "        \n",
    "    if use_pca:\n",
    "        comps = 4\n",
    "        print('doing PCA with %s components'%comps, end=' ')\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=comps, whiten=False)\n",
    "        trainFeatures = pca.fit_transform(trainFeatures.numpy().T)\n",
    "        trainFeatures = torch.Tensor(trainFeatures)\n",
    "        trainFeatures = normalize(trainFeatures).t()\n",
    "        print('..done')\n",
    "    def eval_k_s(K_,sigma_):\n",
    "        total = 0\n",
    "        top1 = 0.\n",
    "        top5 = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            retrieval_one_hot = torch.zeros(K_, C)# .cuda()\n",
    "            for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=False)):\n",
    "                targets = targets # .cuda(async=True) # or without async for py3.7\n",
    "                inputs = inputs.cuda()\n",
    "                batchSize = batch_size\n",
    "                features = net(inputs)\n",
    "                if use_pca:\n",
    "                    features = pca.transform(features.cpu().numpy())\n",
    "                    features = torch.Tensor(features).cuda()\n",
    "                features = normalize(features).cpu()\n",
    "\n",
    "                dist = torch.mm(features, trainFeatures)\n",
    "\n",
    "                yd, yi = dist.topk(K_, dim=1, largest=True, sorted=True)\n",
    "                candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "                retrieval = torch.gather(candidates, 1, yi).long()\n",
    "\n",
    "                retrieval_one_hot.resize_(batchSize * K_, C).zero_()\n",
    "                retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1.)\n",
    "                \n",
    "                yd_transform = yd.clone().div_(sigma_).exp_()\n",
    "                probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C),\n",
    "                                            yd_transform.view(batchSize, -1, 1)),\n",
    "                                  1)\n",
    "                _, predictions = probs.sort(1, True)\n",
    "\n",
    "                # Find which predictions match the target\n",
    "                correct = predictions.eq(targets.data.view(-1, 1))\n",
    "\n",
    "                top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "                top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        print(f\"{K_}-NN,s={sigma_}: TOP1: \", top1 * 100. / total)\n",
    "        return top1 / total\n",
    "\n",
    "    if isinstance(K, list):\n",
    "        res = []\n",
    "        for K_ in K:\n",
    "            for sigma_ in sigma:\n",
    "                res.append(eval_k_s(K_, sigma_))\n",
    "        return res\n",
    "    else:\n",
    "        res = eval_k_s(K, sigma)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "ResNet\n",
      "error:  0.0015402576511577282 step  31\n",
      "cost:  1.8896547803967036\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [0][0/14]Time: 0.230 (0.230) Data: 0.173 (0.173) Loss: 2.4584 (2.4584)\n",
      "error:  0.0016104083665109403 step  61\n",
      "cost:  1.8365955556674802\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.008056123587746766 step  91\n",
      "cost:  1.5750405927300593\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.006358236716571208 step  121\n",
      "cost:  1.4240073279621797\n",
      "opt took 0.00min,  121iters\n",
      "error:  0.007512972465985812 step  151\n",
      "cost:  1.3470586206896116\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.006630689788855171 step  221\n",
      "cost:  1.221907366958299\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.008118809904080981 step  231\n",
      "cost:  1.1065262640548637\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.00803001570873818 step  281\n",
      "cost:  0.9172640080038702\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008560528916061738 step  331\n",
      "cost:  0.847633897199281\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [0][10/14]Time: 0.206 (0.176) Data: 0.172 (0.136) Loss: 2.2402 (2.3529)\n",
      "error:  0.009834549403922477 step  381\n",
      "cost:  0.7826741094725173\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.007863656457766321 step  331\n",
      "cost:  0.7342429186133603\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.0075536748775894 step  321\n",
      "cost:  0.6837932924545757\n",
      "opt took 0.00min,  321iters\n",
      "10-NN,s=0.1: TOP1:  88.1\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 4 components ..done\n",
      "50-NN,s=0.1: TOP1:  62.333333333333336\n",
      "50-NN,s=0.5: TOP1:  62.03333333333333\n",
      "10-NN,s=0.1: TOP1:  64.4\n",
      "10-NN,s=0.5: TOP1:  64.4\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 1\n",
      "ResNet\n",
      "error:  0.0008952548659058035 step  21\n",
      "cost:  2.0269849576486574\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [1][0/14]Time: 0.215 (0.215) Data: 0.179 (0.179) Loss: 2.1396 (2.1396)\n",
      "error:  0.009229424291837263 step  41\n",
      "cost:  1.8563369872660238\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.0033673646161569426 step  81\n",
      "cost:  1.69940585352858\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.00755642954408664 step  121\n",
      "cost:  1.491863774705742\n",
      "opt took 0.00min,  121iters\n",
      "error:  0.00737782880484017 step  181\n",
      "cost:  1.2708750354168816\n",
      "opt took 0.00min,  181iters\n",
      "error:  0.006502458769774733 step  201\n",
      "cost:  1.067825159324043\n",
      "opt took 0.00min,  201iters\n",
      "error:  0.007947822669177262 step  321\n",
      "cost:  0.9069333923847798\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.008419152412964204 step  351\n",
      "cost:  0.7044685240909648\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.008201266245853933 step  391\n",
      "cost:  0.6432773218317848\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [1][10/14]Time: 0.207 (0.176) Data: 0.173 (0.137) Loss: 1.9932 (2.0586)\n",
      "error:  0.007961242735726293 step  451\n",
      "cost:  0.5927759444467012\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.008708723829558007 step  491\n",
      "cost:  0.5422823799789096\n",
      "opt took 0.00min,  491iters\n",
      "error:  0.009636202475134525 step  761\n",
      "cost:  0.5053478584834008\n",
      "opt took 0.00min,  761iters\n",
      "10-NN,s=0.1: TOP1:  87.46666666666667\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 2\n",
      "ResNet\n",
      "error:  9.633687841092975e-05 step  21\n",
      "cost:  2.0240727757605863\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [2][0/14]Time: 0.151 (0.151) Data: 0.116 (0.116) Loss: 1.9416 (1.9416)\n",
      "error:  0.00025829785720876686 step  31\n",
      "cost:  1.9498625952001902\n",
      "opt took 0.00min,   31iters\n",
      "error:  0.001965778225810899 step  61\n",
      "cost:  1.787340908439686\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.004444589342827876 step  131\n",
      "cost:  1.562157552140985\n",
      "opt took 0.00min,  131iters\n",
      "error:  0.006962193532394356 step  231\n",
      "cost:  1.3228785158730703\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.008420880688661758 step  301\n",
      "cost:  1.0816362889082296\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.00928836173081482 step  451\n",
      "cost:  0.8872264569020983\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.008941162797360125 step  411\n",
      "cost:  0.643244350706246\n",
      "opt took 0.00min,  411iters\n",
      "error:  0.009524566486347363 step  371\n",
      "cost:  0.5743168752234171\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.00907288958023389 step  321\n",
      "cost:  0.5294878405098123\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [2][10/14]Time: 0.212 (0.203) Data: 0.174 (0.167) Loss: 1.7973 (1.8533)\n",
      "error:  0.009410840711119861 step  371\n",
      "cost:  0.4908377591678683\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.009721599649305634 step  651\n",
      "cost:  0.46338772654035154\n",
      "opt took 0.00min,  651iters\n",
      "error:  0.009061456814131175 step  511\n",
      "cost:  0.441133765525118\n",
      "opt took 0.00min,  511iters\n",
      "10-NN,s=0.1: TOP1:  87.36666666666666\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 3\n",
      "ResNet\n",
      "error:  0.001370272588098409 step  21\n",
      "cost:  1.9628868557330594\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [3][0/14]Time: 0.191 (0.191) Data: 0.153 (0.153) Loss: 1.7152 (1.7152)\n",
      "error:  0.0023859832568454697 step  51\n",
      "cost:  1.8416145687085252\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.009261997205293748 step  161\n",
      "cost:  1.6134867214751512\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.009540388561552149 step  251\n",
      "cost:  1.3347684591018278\n",
      "opt took 0.00min,  251iters\n",
      "error:  0.007971925642672018 step  391\n",
      "cost:  1.087176703170812\n",
      "opt took 0.00min,  391iters\n",
      "error:  0.009191779861661975 step  811\n",
      "cost:  0.8800314398525086\n",
      "opt took 0.00min,  811iters\n",
      "error:  0.009421398077642085 step  1161\n",
      "cost:  0.6178907616524327\n",
      "opt took 0.00min, 1161iters\n",
      "error:  0.009657081205813545 step  991\n",
      "cost:  0.5449686353530175\n",
      "opt took 0.00min,  991iters\n",
      "error:  0.009915155726369251 step  941\n",
      "cost:  0.48966995973649735\n",
      "opt took 0.00min,  941iters\n",
      "error:  0.009552153520085871 step  981\n",
      "cost:  0.45356055370291276\n",
      "opt took 0.00min,  981iters\n",
      "Epoch: [3][10/14]Time: 0.294 (0.263) Data: 0.259 (0.222) Loss: 1.5729 (1.6296)\n",
      "error:  0.009330080092387116 step  991\n",
      "cost:  0.42677473144782596\n",
      "opt took 0.00min,  991iters\n",
      "error:  0.007874366604770255 step  291\n",
      "cost:  0.412913084918509\n",
      "opt took 0.00min,  291iters\n",
      "10-NN,s=0.1: TOP1:  87.03333333333333\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 4\n",
      "ResNet\n",
      "error:  5.148907928331781e-05 step  31\n",
      "cost:  1.927091174128978\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [4][0/14]Time: 0.168 (0.168) Data: 0.128 (0.128) Loss: 1.5205 (1.5205)\n",
      "error:  0.0036508212704484677 step  41\n",
      "cost:  1.8498368260143785\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.004966025046770461 step  51\n",
      "cost:  1.67072446279036\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.009616171366198745 step  161\n",
      "cost:  1.4216220272070543\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.00803917948166355 step  341\n",
      "cost:  1.16530447172919\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.008178801900146082 step  371\n",
      "cost:  0.9554788385845043\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.008331123633777748 step  501\n",
      "cost:  0.793878892151717\n",
      "opt took 0.00min,  501iters\n",
      "error:  0.009130456147576793 step  451\n",
      "cost:  0.6753070671750308\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.008167928518905798 step  341\n",
      "cost:  0.598401383534722\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.00988981809318934 step  311\n",
      "cost:  0.5463275686652797\n",
      "opt took 0.00min,  311iters\n",
      "error:  0.009314914672373553 step  371\n",
      "cost:  0.498651761072296\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [4][10/14]Time: 0.305 (0.242) Data: 0.253 (0.199) Loss: 1.3954 (1.4387)\n",
      "error:  0.007773516497396327 step  321\n",
      "cost:  0.45903476840338336\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.008198940664722265 step  371\n",
      "cost:  0.39687194288984134\n",
      "opt took 0.00min,  371iters\n",
      "10-NN,s=0.1: TOP1:  86.7\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 5\n",
      "ResNet\n",
      "error:  5.77267390198255e-05 step  31\n",
      "cost:  1.9157523281337798\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [5][0/14]Time: 0.154 (0.154) Data: 0.119 (0.119) Loss: 1.3385 (1.3385)\n",
      "error:  0.003952174469923464 step  41\n",
      "cost:  1.8351843520734958\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.005182235216522724 step  71\n",
      "cost:  1.6755641825499001\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.009049040068092817 step  121\n",
      "cost:  1.4605068376389274\n",
      "opt took 0.00min,  121iters\n",
      "error:  0.006883546058337187 step  201\n",
      "cost:  1.219232849496406\n",
      "opt took 0.00min,  201iters\n",
      "error:  0.0071133901610558326 step  261\n",
      "cost:  0.9867936892082085\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.009772357390255393 step  231\n",
      "cost:  0.7850542842409998\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.009588624866370243 step  401\n",
      "cost:  0.6343053290753181\n",
      "opt took 0.00min,  401iters\n",
      "error:  0.008286090626547837 step  481\n",
      "cost:  0.5318090953064174\n",
      "opt took 0.00min,  481iters\n",
      "error:  0.009340399928919996 step  341\n",
      "cost:  0.4639236470361635\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.008389697986925793 step  391\n",
      "cost:  0.4159598354507573\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [5][10/14]Time: 0.344 (0.247) Data: 0.280 (0.204) Loss: 1.1786 (1.2453)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009815548290130538 step  471\n",
      "cost:  0.3779006738770209\n",
      "opt took 0.00min,  471iters\n",
      "error:  0.009023440140412897 step  541\n",
      "cost:  0.36502972219669255\n",
      "opt took 0.00min,  541iters\n",
      "10-NN,s=0.1: TOP1:  87.23333333333333\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 6\n",
      "ResNet\n",
      "error:  0.00047410357041277607 step  31\n",
      "cost:  1.8751417984979326\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [6][0/14]Time: 0.157 (0.157) Data: 0.122 (0.122) Loss: 1.1424 (1.1424)\n",
      "error:  0.006137594072491148 step  41\n",
      "cost:  1.7617272236053385\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.0090668397724466 step  81\n",
      "cost:  1.540698915373771\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.008436420083275054 step  151\n",
      "cost:  1.2725807430819285\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.00670792516021157 step  221\n",
      "cost:  1.0157037708366765\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.009900704015983752 step  371\n",
      "cost:  0.7977301761715049\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.008618449976986908 step  421\n",
      "cost:  0.6456220908332306\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.008876948164273979 step  391\n",
      "cost:  0.5352617624509408\n",
      "opt took 0.00min,  391iters\n",
      "error:  0.009245033210097398 step  301\n",
      "cost:  0.4578994142546076\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.009779558450013326 step  341\n",
      "cost:  0.40681207742006814\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.008894743552087192 step  501\n",
      "cost:  0.37162306832302044\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [6][10/14]Time: 0.232 (0.210) Data: 0.197 (0.171) Loss: 1.0012 (1.0431)\n",
      "error:  0.008928836361894676 step  611\n",
      "cost:  0.33212422327259766\n",
      "opt took 0.00min,  611iters\n",
      "error:  0.008462335982444502 step  491\n",
      "cost:  0.31806135258037027\n",
      "opt took 0.00min,  491iters\n",
      "10-NN,s=0.1: TOP1:  86.93333333333334\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 7\n",
      "ResNet\n",
      "error:  0.00919305599060427 step  31\n",
      "cost:  1.8492624918534322\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [7][0/14]Time: 0.198 (0.198) Data: 0.145 (0.145) Loss: 0.9273 (0.9273)\n",
      "error:  0.005686641951234472 step  71\n",
      "cost:  1.735193170782677\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.006738133157305648 step  261\n",
      "cost:  1.4982977483703277\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.007085191295958304 step  341\n",
      "cost:  1.2002183928272045\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009661416724663918 step  431\n",
      "cost:  0.9151319933278511\n",
      "opt took 0.00min,  431iters\n",
      "error:  0.008811962301563891 step  701\n",
      "cost:  0.696466105305784\n",
      "opt took 0.00min,  701iters\n",
      "error:  0.00992536034649727 step  931\n",
      "cost:  0.5502930911866275\n",
      "opt took 0.00min,  931iters\n",
      "error:  0.009767697790637353 step  681\n",
      "cost:  0.4589846831802442\n",
      "opt took 0.00min,  681iters\n",
      "error:  0.009016246305822095 step  881\n",
      "cost:  0.39267718818491804\n",
      "opt took 0.00min,  881iters\n",
      "error:  0.009390047494059028 step  891\n",
      "cost:  0.3514671163695625\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [7][10/14]Time: 0.034 (0.263) Data: 0.000 (0.221) Loss: 0.7851 (0.8382)\n",
      "error:  0.009890132926719364 step  891\n",
      "cost:  0.31775482981709025\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.009971196752534772 step  601\n",
      "cost:  0.305209957811084\n",
      "opt took 0.01min,  601iters\n",
      "error:  0.008685804904577044 step  451\n",
      "cost:  0.2899611466034351\n",
      "opt took 0.00min,  451iters\n",
      "10-NN,s=0.1: TOP1:  86.83333333333333\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 8\n",
      "ResNet\n",
      "error:  0.00012574599282233478 step  31\n",
      "cost:  1.8864673496527282\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [8][0/14]Time: 0.151 (0.151) Data: 0.112 (0.112) Loss: 0.7242 (0.7242)\n",
      "error:  0.00035082542012920825 step  41\n",
      "cost:  1.7882933080639907\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.004633881827851849 step  111\n",
      "cost:  1.596572392357732\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.008703915907249948 step  221\n",
      "cost:  1.3216570254578546\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.008942092805463764 step  341\n",
      "cost:  1.0456682410144005\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.00933905309745775 step  491\n",
      "cost:  0.8257100171582745\n",
      "opt took 0.00min,  491iters\n",
      "error:  0.009954068269525496 step  491\n",
      "cost:  0.6734724187242346\n",
      "opt took 0.00min,  491iters\n",
      "error:  0.009767079089117336 step  631\n",
      "cost:  0.567628117713195\n",
      "opt took 0.00min,  631iters\n",
      "error:  0.009381293122793899 step  701\n",
      "cost:  0.48426164038521063\n",
      "opt took 0.00min,  701iters\n",
      "error:  0.009602325077571194 step  911\n",
      "cost:  0.385848198938324\n",
      "opt took 0.00min,  911iters\n",
      "Epoch: [8][10/14]Time: 0.269 (0.260) Data: 0.234 (0.220) Loss: 0.5931 (0.6330)\n",
      "error:  0.009559661788184237 step  1141\n",
      "cost:  0.3592136347433658\n",
      "opt took 0.00min, 1141iters\n",
      "error:  0.009487909550696116 step  1221\n",
      "cost:  0.3512202354229872\n",
      "opt took 0.00min, 1221iters\n",
      "error:  0.009272830803182197 step  991\n",
      "cost:  0.3357321947645975\n",
      "opt took 0.00min,  991iters\n",
      "10-NN,s=0.1: TOP1:  86.7\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 9\n",
      "ResNet\n",
      "error:  0.001957602222631527 step  31\n",
      "cost:  1.818181821594829\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [9][0/14]Time: 0.167 (0.167) Data: 0.130 (0.130) Loss: 0.5409 (0.5409)\n",
      "error:  0.0036541593363664804 step  41\n",
      "cost:  1.7274832712373833\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.0067045904773196385 step  81\n",
      "cost:  1.520480286209013\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.006596884218982524 step  141\n",
      "cost:  1.2463548978190186\n",
      "opt took 0.00min,  141iters\n",
      "error:  0.008994977175424967 step  231\n",
      "cost:  0.991126473142094\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.007384441126161434 step  361\n",
      "cost:  0.7923067235418418\n",
      "opt took 0.00min,  361iters\n",
      "error:  0.00950726462606355 step  731\n",
      "cost:  0.6389811705417602\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.009842109732040805 step  921\n",
      "cost:  0.5173880234949486\n",
      "opt took 0.00min,  921iters\n",
      "error:  0.009740487831154665 step  1701\n",
      "cost:  0.40055891407422206\n",
      "opt took 0.01min, 1701iters\n",
      "error:  0.009560235408159978 step  1671\n",
      "cost:  0.3774903154867123\n",
      "opt took 0.01min, 1671iters\n",
      "Epoch: [9][10/14]Time: 0.527 (0.284) Data: 0.492 (0.242) Loss: 0.3965 (0.4579)\n",
      "error:  0.009720940931251087 step  1751\n",
      "cost:  0.3557472072566226\n",
      "opt took 0.01min, 1751iters\n",
      "error:  0.009721174631314167 step  1411\n",
      "cost:  0.34351196773392506\n",
      "opt took 0.00min, 1411iters\n",
      "error:  0.009702286310928843 step  2211\n",
      "cost:  0.3236987984229096\n",
      "opt took 0.01min, 2211iters\n",
      "10-NN,s=0.1: TOP1:  86.9\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 10\n",
      "ResNet\n",
      "error:  0.009838299212621915 step  1231\n",
      "cost:  0.28355560120410633\n",
      "opt took 0.00min, 1231iters\n",
      "Epoch: [10][0/14]Time: 0.391 (0.391) Data: 0.356 (0.356) Loss: 0.4207 (0.4207)\n",
      "error:  0.009919497853532744 step  1051\n",
      "cost:  0.2781171661066336\n",
      "opt took 0.00min, 1051iters\n",
      "error:  0.009425999353464554 step  1231\n",
      "cost:  0.2646557788740579\n",
      "opt took 0.00min, 1231iters\n",
      "error:  0.009409755296319045 step  911\n",
      "cost:  0.26018334643209134\n",
      "opt took 0.00min,  911iters\n",
      "error:  0.009610640832782957 step  1621\n",
      "cost:  0.27901471901029196\n",
      "opt took 0.01min, 1621iters\n",
      "error:  0.009945613929507546 step  1061\n",
      "cost:  0.294413548991787\n",
      "opt took 0.00min, 1061iters\n",
      "error:  0.009338889301971465 step  1201\n",
      "cost:  0.3119575583399155\n",
      "opt took 0.01min, 1201iters\n",
      "error:  0.0087931117089326 step  511\n",
      "cost:  0.3023743160749333\n",
      "opt took 0.00min,  511iters\n",
      "error:  0.008916182257675764 step  671\n",
      "cost:  0.293668027741663\n",
      "opt took 0.00min,  671iters\n",
      "error:  0.009103352466149794 step  441\n",
      "cost:  0.2815471068647822\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [10][10/14]Time: 0.249 (0.326) Data: 0.213 (0.286) Loss: 0.3536 (0.3820)\n",
      "error:  0.009072502461787257 step  831\n",
      "cost:  0.27689434545722025\n",
      "opt took 0.00min,  831iters\n",
      "error:  0.008328405004805806 step  301\n",
      "cost:  0.2737529557077211\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.008658540771556256 step  311\n",
      "cost:  0.27680097667866377\n",
      "opt took 0.00min,  311iters\n",
      "10-NN,s=0.1: TOP1:  87.06666666666666\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 11\n",
      "ResNet\n",
      "error:  0.009873416552078518 step  1321\n",
      "cost:  0.29886822217488035\n",
      "opt took 0.01min, 1321iters\n",
      "Epoch: [11][0/14]Time: 0.487 (0.487) Data: 0.442 (0.442) Loss: 0.4125 (0.4125)\n",
      "error:  0.009150508582329908 step  491\n",
      "cost:  0.2767608446413031\n",
      "opt took 0.00min,  491iters\n",
      "error:  0.009243530572691783 step  981\n",
      "cost:  0.2696179765177675\n",
      "opt took 0.00min,  981iters\n",
      "error:  0.00987788272781065 step  1191\n",
      "cost:  0.27357190010983196\n",
      "opt took 0.01min, 1191iters\n",
      "error:  0.009828989446825642 step  1161\n",
      "cost:  0.26097926533194776\n",
      "opt took 0.00min, 1161iters\n",
      "error:  0.009983767561892343 step  1481\n",
      "cost:  0.2609449501784369\n",
      "opt took 0.01min, 1481iters\n",
      "error:  0.009269910331828113 step  931\n",
      "cost:  0.31781232624776634\n",
      "opt took 0.00min,  931iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009334939234804662 step  461\n",
      "cost:  0.3005855998022735\n",
      "opt took 0.00min,  461iters\n",
      "error:  0.009651428851832455 step  1091\n",
      "cost:  0.28262392589933294\n",
      "opt took 0.00min, 1091iters\n",
      "error:  0.009651774189521678 step  1621\n",
      "cost:  0.2845826032418224\n",
      "opt took 0.00min, 1621iters\n",
      "Epoch: [11][10/14]Time: 0.486 (0.384) Data: 0.451 (0.341) Loss: 0.3752 (0.3801)\n",
      "error:  0.00919404986774941 step  1121\n",
      "cost:  0.2614444169892552\n",
      "opt took 0.00min, 1121iters\n",
      "error:  0.009753590645148358 step  2561\n",
      "cost:  0.25033998037236876\n",
      "opt took 0.01min, 2561iters\n",
      "10-NN,s=0.1: TOP1:  87.13333333333334\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 12\n",
      "ResNet\n",
      "error:  0.009843093245213708 step  3511\n",
      "cost:  0.282603310760049\n",
      "opt took 0.01min, 3511iters\n",
      "Epoch: [12][0/14]Time: 0.938 (0.938) Data: 0.903 (0.903) Loss: 0.3909 (0.3909)\n",
      "error:  0.009695907462070052 step  2651\n",
      "cost:  0.276437821383517\n",
      "opt took 0.01min, 2651iters\n",
      "error:  0.00998208823634028 step  2051\n",
      "cost:  0.27010424684484075\n",
      "opt took 0.01min, 2051iters\n",
      "error:  0.009569488674890714 step  1911\n",
      "cost:  0.2658749947508043\n",
      "opt took 0.01min, 1911iters\n",
      "error:  0.009860852319025337 step  2041\n",
      "cost:  0.2608086751750981\n",
      "opt took 0.01min, 2041iters\n",
      "error:  0.009589786125728805 step  441\n",
      "cost:  0.24935833554300327\n",
      "opt took 0.00min,  441iters\n",
      "error:  0.009087590393531575 step  581\n",
      "cost:  0.24686983790215972\n",
      "opt took 0.00min,  581iters\n",
      "error:  0.009979836342519754 step  2221\n",
      "cost:  0.25507574870159716\n",
      "opt took 0.01min, 2221iters\n",
      "error:  0.009965854146799513 step  4641\n",
      "cost:  0.27773914290617574\n",
      "opt took 0.02min, 4641iters\n",
      "error:  0.009813134136979484 step  2021\n",
      "cost:  0.2949021002005043\n",
      "opt took 0.01min, 2021iters\n",
      "error:  0.009811799809432475 step  1031\n",
      "cost:  0.30655965489897546\n",
      "opt took 0.00min, 1031iters\n",
      "Epoch: [12][10/14]Time: 0.308 (0.575) Data: 0.273 (0.538) Loss: 0.3947 (0.3568)\n",
      "error:  0.0092534208643823 step  891\n",
      "cost:  0.2871341563562062\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.009863741085527566 step  861\n",
      "cost:  0.24697532214638085\n",
      "opt took 0.00min,  861iters\n",
      "10-NN,s=0.1: TOP1:  86.8\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 13\n",
      "ResNet\n",
      "error:  0.009460755349891659 step  911\n",
      "cost:  0.2676894715706271\n",
      "opt took 0.00min,  911iters\n",
      "Epoch: [13][0/14]Time: 0.293 (0.293) Data: 0.258 (0.258) Loss: 0.3935 (0.3935)\n",
      "error:  0.00988763919872937 step  1371\n",
      "cost:  0.25730422634415784\n",
      "opt took 0.00min, 1371iters\n",
      "error:  0.009526509983070586 step  1381\n",
      "cost:  0.2535402764018715\n",
      "opt took 0.00min, 1381iters\n",
      "error:  0.009898891511622598 step  4411\n",
      "cost:  0.24478176967312834\n",
      "opt took 0.01min, 4411iters\n",
      "error:  0.009960177070591714 step  1901\n",
      "cost:  0.24065197702215252\n",
      "opt took 0.00min, 1901iters\n",
      "error:  0.009234147191809927 step  401\n",
      "cost:  0.24241954659248316\n",
      "opt took 0.00min,  401iters\n",
      "error: ng head 3  0.00840198674271675 step  281\n",
      "cost:  0.24735819613048335\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.009594602813550845 step  731\n",
      "cost:  0.2529449025756926\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.0093245881577243 step  721\n",
      "cost:  0.25960366186587935\n",
      "opt took 0.01min,  721iters\n",
      "error:  0.009570298273273381 step  841\n",
      "cost:  0.2716169933079947\n",
      "opt took 0.00min,  841iters\n",
      "error:  0.009905685312859625 step  691\n",
      "cost:  0.29000476901641425\n",
      "opt took 0.00min,  691iters\n",
      "Epoch: [13][10/14]Time: 0.269 (0.405) Data: 0.234 (0.367) Loss: 0.3536 (0.3503)\n",
      "error:  0.00993440998835804 step  2591\n",
      "cost:  0.2901506831139491\n",
      "opt took 0.01min, 2591iters\n",
      "error:  0.009935532114735746 step  2941\n",
      "cost:  0.2578513743128889\n",
      "opt took 0.01min, 2941iters\n",
      "10-NN,s=0.1: TOP1:  87.4\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 14\n",
      "ResNet\n",
      "error:  0.009192637222754874 step  881\n",
      "cost:  0.2952070673878278\n",
      "opt took 0.00min,  881iters\n",
      "Epoch: [14][0/14]Time: 0.282 (0.282) Data: 0.246 (0.246) Loss: 0.3627 (0.3627)\n",
      "error:  0.009949676816763287 step  1041\n",
      "cost:  0.2857244054035362\n",
      "opt took 0.00min, 1041iters\n",
      "error:  0.00908067218081543 step  981\n",
      "cost:  0.2799454461780575\n",
      "opt took 0.01min,  981iters\n",
      "error:  0.009960834903319782 step  1271\n",
      "cost:  0.27812060020154494\n",
      "opt took 0.00min, 1271iters\n",
      "error:  0.00966680703707401 step  1021\n",
      "cost:  0.2768919748921833\n",
      "opt took 0.00min, 1021iters\n",
      "error:  0.00929614410796853 step  971\n",
      "cost:  0.277464679487964\n",
      "opt took 0.00min,  971iters\n",
      "error:  0.009952127961769808 step  671\n",
      "cost:  0.2869142839087663\n",
      "opt took 0.00min,  671iters\n",
      "error:  0.00856899372838904 step  201\n",
      "cost:  0.2810160117091748\n",
      "opt took 0.00min,  201iters\n",
      "error:  0.00968613117128414 step  311\n",
      "cost:  0.27563283930103044\n",
      "opt took 0.00min,  311iters\n",
      "error:  0.009906195693802355 step  1081\n",
      "cost:  0.27649244409409385\n",
      "opt took 0.00min, 1081iters\n",
      "error:  0.00927543733046765 step  1231\n",
      "cost:  0.27366743280247263\n",
      "opt took 0.01min, 1231iters\n",
      "Epoch: [14][10/14]Time: 0.701 (0.350) Data: 0.647 (0.311) Loss: 0.3078 (0.3270)\n",
      "error:  0.009336760979020142 step  1181\n",
      "cost:  0.26987183949288485\n",
      "opt took 0.00min, 1181iters\n",
      "error:  0.009414574386173324 step  1311\n",
      "cost:  0.26295784851549203\n",
      "opt took 0.00min, 1311iters\n",
      "10-NN,s=0.1: TOP1:  86.9\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 15\n",
      "ResNet\n",
      "error:  0.009618676898109801 step  1061\n",
      "cost:  0.2518989229112896\n",
      "opt took 0.00min, 1061iters\n",
      "Epoch: [15][0/14]Time: 0.327 (0.327) Data: 0.292 (0.292) Loss: 0.3259 (0.3259)\n",
      "error:  0.00993566328919937 step  581\n",
      "cost:  0.2546338218752426\n",
      "opt took 0.00min,  581iters\n",
      "error:  0.009207790012554451 step  461\n",
      "cost:  0.2518985577564912\n",
      "opt took 0.00min,  461iters\n",
      "error:  0.009689925555947165 step  721\n",
      "cost:  0.2423374578205441\n",
      "opt took 0.00min,  721iters\n",
      "error:  0.009442597337717906 step  721\n",
      "cost:  0.2372570787313219\n",
      "opt took 0.00min,  721iters\n",
      "error:  0.0097945199061521 step  1211\n",
      "cost:  0.22946896997942753\n",
      "opt took 0.00min, 1211iters\n",
      "error:  0.009272236525999733 step  871\n",
      "cost:  0.2382867204639362\n",
      "opt took 0.00min,  871iters\n",
      "error:  0.009902940548154548 step  1161\n",
      "cost:  0.24839052988233648\n",
      "opt took 0.00min, 1161iters\n",
      "error:  0.009948988152293503 step  1641\n",
      "cost:  0.24969183656562927\n",
      "opt took 0.00min, 1641iters\n",
      "error:  0.009888730941946644 step  1471\n",
      "cost:  0.2560536385407264\n",
      "opt took 0.00min, 1471iters\n",
      "Epoch: [15][10/14]Time: 0.035 (0.300) Data: 0.000 (0.262) Loss: 0.3222 (0.3392)\n",
      "error:  0.009901049592798916 step  481\n",
      "cost:  0.25627993977662983\n",
      "opt took 0.00min,  481iters\n",
      "error:  0.009633463776217877 step  1491\n",
      "cost:  0.2624468126223196\n",
      "opt took 0.01min, 1491iters\n",
      "error:  0.009249436249906262 step  901\n",
      "cost:  0.2794500229283939\n",
      "opt took 0.00min,  901iters\n",
      "10-NN,s=0.1: TOP1:  87.2\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 16\n",
      "ResNet\n",
      "error:  0.0099323827765182 step  1191\n",
      "cost:  0.2591390871777905\n",
      "opt took 0.00min, 1191iters\n",
      "Epoch: [16][0/14]Time: 0.359 (0.359) Data: 0.325 (0.325) Loss: 0.3432 (0.3432)\n",
      "error:  0.009600899056151024 step  1301\n",
      "cost:  0.24803844025220137\n",
      "opt took 0.00min, 1301iters\n",
      "error:  0.00942621390895848 step  971\n",
      "cost:  0.24780449454382505\n",
      "opt took 0.00min,  971iters\n",
      "error:  0.009401438935221496 step  1261\n",
      "cost:  0.24906004032894677\n",
      "opt took 0.00min, 1261iters\n",
      "error:  0.009729492543451013 step  1401\n",
      "cost:  0.24823123611891068\n",
      "opt took 0.01min, 1401iters\n",
      "error:  0.009904893918344482 step  1701\n",
      "cost:  0.24454545189283783\n",
      "opt took 0.01min, 1701iters\n",
      "error:  0.009758177866127205 step  1051\n",
      "cost:  0.24497264917209566\n",
      "opt took 0.00min, 1051iters\n",
      "error:  0.009226528956349678 step  761\n",
      "cost:  0.25756501938452214\n",
      "opt took 0.00min,  761iters\n",
      "error:  0.008075015550680131 step  171\n",
      "cost:  0.26101122204903376\n",
      "opt took 0.00min,  171iters\n",
      "error:  0.007866019573514182 step  181\n",
      "cost:  0.2565382982709172\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [16][10/14]Time: 0.267 (0.359) Data: 0.230 (0.319) Loss: 0.2993 (0.3255)\n",
      "error:  0.009797785060568054 step  261\n",
      "cost:  0.26070859114485\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.00953835715221718 step  451\n",
      "cost:  0.26735485072299486\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.009760877600632267 step  921\n",
      "cost:  0.2727285948295524\n",
      "opt took 0.00min,  921iters\n",
      "10-NN,s=0.1: TOP1:  87.76666666666667\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 17\n",
      "ResNet\n",
      "error:  0.009899781845078337 step  1031\n",
      "cost:  0.28249619604290044\n",
      "opt took 0.00min, 1031iters\n",
      "Epoch: [17][0/14]Time: 0.332 (0.332) Data: 0.297 (0.297) Loss: 0.3710 (0.3710)\n",
      "error:  0.009108802278162353 step  621\n",
      "cost:  0.26429965536868033\n",
      "opt took 0.00min,  621iters\n",
      "error:  0.008846949317895092 step  391\n",
      "cost:  0.24206028187536366\n",
      "opt took 0.00min,  391iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009769697577486691 step  2051\n",
      "cost:  0.23497085153254632\n",
      "opt took 0.01min, 2051iters\n",
      "error:  0.009930377549683289 step  2451\n",
      "cost:  0.24214350494624615\n",
      "opt took 0.01min, 2451iters\n",
      "error:  0.0095363931954775 step  1511\n",
      "cost:  0.24340782032248998\n",
      "opt took 0.00min, 1511iters\n",
      "error:  0.00980062981266483 step  2171\n",
      "cost:  0.24649737215426676\n",
      "opt took 0.01min, 2171iters\n",
      "error:  0.00960111775500061 step  2041\n",
      "cost:  0.25065192854174495\n",
      "opt took 0.01min, 2041iters\n",
      "error:  0.009296552035512051 step  431\n",
      "cost:  0.27331257523283997\n",
      "opt took 0.00min,  431iters\n",
      "error:  0.006611453195326966 step  201\n",
      "cost:  0.2631664878021427\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [17][10/14]Time: 0.206 (0.452) Data: 0.160 (0.411) Loss: 0.3267 (0.3314)\n",
      "error:  0.007251284542859282 step  201\n",
      "cost:  0.24841016354670808\n",
      "opt took 0.00min,  201iters\n",
      "error:  0.007921198184410172 step  221\n",
      "cost:  0.2403153199611405\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.009608685195056199 step  361\n",
      "cost:  0.2409122055740641\n",
      "opt took 0.00min,  361iters\n",
      "10-NN,s=0.1: TOP1:  87.56666666666666\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 18\n",
      "ResNet\n",
      "error:  0.009352568727570065 step  1531\n",
      "cost:  0.2905602747293786\n",
      "opt took 0.01min, 1531iters\n",
      "Epoch: [18][0/14]Time: 0.542 (0.542) Data: 0.507 (0.507) Loss: 0.3374 (0.3374)\n",
      "error:  0.009582218289408528 step  1611\n",
      "cost:  0.2900493546390074\n",
      "opt took 0.01min, 1611iters\n",
      "error:  0.009720115797525741 step  1591\n",
      "cost:  0.2774643267472143\n",
      "opt took 0.00min, 1591iters\n",
      "error:  0.009528285167926587 step  1621\n",
      "cost:  0.2602047543380694\n",
      "opt took 0.00min, 1621iters\n",
      "error:  0.009961822567923218 step  711\n",
      "cost:  0.25285069968797047\n",
      "opt took 0.01min,  711iters\n",
      "error:  0.009447641315228128 step  611\n",
      "cost:  0.25247048355636537\n",
      "opt took 0.00min,  611iters\n",
      "error:  0.009933417913740361 step  591\n",
      "cost:  0.2526836322429751\n",
      "opt took 0.00min,  591iters\n",
      "error:  0.008803127111306486 step  381\n",
      "cost:  0.2549181464307733\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.008344773883545376 step  181\n",
      "cost:  0.28129716969993557\n",
      "opt took 0.00min,  181iters\n",
      "error:  0.008950309211032237 step  271\n",
      "cost:  0.2805030481110605\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [18][10/14]Time: 0.196 (0.376) Data: 0.161 (0.334) Loss: 0.3702 (0.3223)\n",
      "error:  0.008894585796546672 step  421\n",
      "cost:  0.26463643353879346\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.009777675834550359 step  531\n",
      "cost:  0.24691170051600989\n",
      "opt took 0.00min,  531iters\n",
      "error:  0.00925115396865206 step  481\n",
      "cost:  0.24218854907615647\n",
      "opt took 0.00min,  481iters\n",
      "10-NN,s=0.1: TOP1:  87.03333333333333\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 19\n",
      "ResNet\n",
      "error:  0.009012974552925712 step  291\n",
      "cost:  0.2908581757032929\n",
      "opt took 0.00min,  291iters\n",
      "Epoch: [19][0/14]Time: 0.185 (0.185) Data: 0.148 (0.148) Loss: 0.3215 (0.3215)\n",
      "error:  0.009925690049688352 step  291\n",
      "cost:  0.27190252975402845\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.009725231362956221 step  521\n",
      "cost:  0.25159747556648376\n",
      "opt took 0.00min,  521iters\n",
      "error:  0.009795834737558962 step  2161\n",
      "cost:  0.24547712328066187\n",
      "opt took 0.01min, 2161iters\n",
      "error:  0.009812821253161963 step  2401\n",
      "cost:  0.23805962037442807\n",
      "opt took 0.01min, 2401iters\n",
      "error:  0.009968439622635605 step  3001\n",
      "cost:  0.23659211653210777\n",
      "opt took 0.01min, 3001iters\n",
      "error:  0.009980405956043437 step  1131\n",
      "cost:  0.24191018831730352\n",
      "opt took 0.01min, 1131iters\n",
      "error:  0.009087407827173322 step  641\n",
      "cost:  0.2768125819532012\n",
      "opt took 0.00min,  641iters\n",
      "error:  0.009144777267443116 step  451\n",
      "cost:  0.28679968131899386\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.008456040827459765 step  231\n",
      "cost:  0.27090275861065044\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [19][10/14]Time: 0.228 (0.390) Data: 0.193 (0.349) Loss: 0.3289 (0.3128)\n",
      "error:  0.008377015705367863 step  411\n",
      "cost:  0.24788290847766872\n",
      "opt took 0.00min,  411iters\n",
      "error:  0.009566916011741866 step  681\n",
      "cost:  0.23293105193591557\n",
      "opt took 0.00min,  681iters\n",
      "error:  0.009849723881233619 step  4451\n",
      "cost:  0.2324010238897806\n",
      "opt took 0.02min, 4451iters\n",
      "10-NN,s=0.1: TOP1:  87.46666666666667\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 20\n",
      "ResNet\n",
      "error:  0.00842446426626009 step  381\n",
      "cost:  0.23012339195400555\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [20][0/14]Time: 0.252 (0.252) Data: 0.214 (0.214) Loss: 0.3129 (0.3129)\n",
      "error:  0.009415411375181204 step  841\n",
      "cost:  0.22160463294165145\n",
      "opt took 0.00min,  841iters\n",
      "error:  0.00990375864945503 step  1011\n",
      "cost:  0.21557335791449062\n",
      "opt took 0.00min, 1011iters\n",
      "error:  0.009889048136301759 step  351\n",
      "cost:  0.22364384349686692\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.008736556096546222 step  301\n",
      "cost:  0.24014083641279363\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.009107936252072513 step  421\n",
      "cost:  0.2418139041735507\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.0092477758989572 step  611\n",
      "cost:  0.22805527220273422\n",
      "opt took 0.00min,  611iters\n",
      "error:  0.009847060592971135 step  1111\n",
      "cost:  0.23637722606075728\n",
      "opt took 0.01min, 1111iters\n",
      "error:  0.009459161054130028 step  1081\n",
      "cost:  0.24499761361718878\n",
      "opt took 0.00min, 1081iters\n",
      "error:  0.009089046352083185 step  751\n",
      "cost:  0.24589170723483744\n",
      "opt took 0.00min,  751iters\n",
      "Epoch: [20][10/14]Time: 0.368 (0.291) Data: 0.318 (0.249) Loss: 0.3230 (0.3065)\n",
      "error:  0.009570837835732204 step  661\n",
      "cost:  0.23632521723890326\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.008143872500404603 step  241\n",
      "cost:  0.23372281164590136\n",
      "opt took 0.00min,  241iters\n",
      "10-NN,s=0.1: TOP1:  87.1\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 21\n",
      "ResNet\n",
      "error:  0.009927561125136819 step  471\n",
      "cost:  0.2595243206439792\n",
      "opt took 0.00min,  471iters\n",
      "Epoch: [21][0/14]Time: 0.218 (0.218) Data: 0.183 (0.183) Loss: 0.3043 (0.3043)\n",
      "error:  0.009530704212312502 step  351\n",
      "cost:  0.23644689443510886\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.009892561704081593 step  831\n",
      "cost:  0.22706311024154244\n",
      "opt took 0.00min,  831iters\n",
      "error:  0.009740748642902308 step  871\n",
      "cost:  0.22521544257345089\n",
      "opt took 0.00min,  871iters\n",
      "error:  0.009821895406296854 step  791\n",
      "cost:  0.23242673462764357\n",
      "opt took 0.00min,  791iters\n",
      "error:  0.009827074778685385 step  2041\n",
      "cost:  0.24351227269399933\n",
      "opt took 0.01min, 2041iters\n",
      "error:  0.009732978430669559 step  581\n",
      "cost:  0.26095925048355234\n",
      "opt took 0.00min,  581iters\n",
      "error:  0.00989404094796531 step  841\n",
      "cost:  0.25054209405138017\n",
      "opt took 0.01min,  841iters\n",
      "error:  0.009931041029137999 step  3491\n",
      "cost:  0.24615211059573336\n",
      "opt took 0.02min, 3491iters\n",
      "error:  0.009259547083806385 step  401\n",
      "cost:  0.22713047492786073\n",
      "opt took 0.00min,  401iters\n",
      "error:  0.00967895242584682 step  361\n",
      "cost:  0.2294679279451819\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [21][10/14]Time: 0.300 (0.404) Data: 0.249 (0.366) Loss: 0.2907 (0.3033)\n",
      "error:  0.008474707531003878 step  281\n",
      "cost:  0.23898800870274206\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008184300969392044 step  261\n",
      "cost:  0.28095035810758423\n",
      "opt took 0.00min,  261iters\n",
      "10-NN,s=0.1: TOP1:  87.5\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 22\n",
      "ResNet\n",
      "error:  0.009999470948930611 step  1861\n",
      "cost:  0.24966765584384124\n",
      "opt took 0.01min, 1861iters\n",
      "Epoch: [22][0/14]Time: 0.574 (0.574) Data: 0.540 (0.540) Loss: 0.3242 (0.3242)\n",
      "error:  0.009889868830348703 step  661\n",
      "cost:  0.24581362416452368\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.009826641030295846 step  1821\n",
      "cost:  0.23823309291619518\n",
      "opt took 0.01min, 1821iters\n",
      "error:  0.009977038658247506 step  1641\n",
      "cost:  0.2383086188693211\n",
      "opt took 0.01min, 1641iters\n",
      "error:  0.009593307416472463 step  1311\n",
      "cost:  0.23886378199896613\n",
      "opt took 0.00min, 1311iters\n",
      "error:  0.008871168357777015 step  261\n",
      "cost:  0.23964296750778324\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.00875072305172564 step  511\n",
      "cost:  0.24259245873697652\n",
      "opt took 0.00min,  511iters\n",
      "error:  0.008994316914718858 step  501\n",
      "cost:  0.24762652187417591\n",
      "opt took 0.00min,  501iters\n",
      "error:  0.008387137361510089 step  441\n",
      "cost:  0.24878340869395657\n",
      "opt took 0.00min,  441iters\n",
      "error:  0.00838145977780902 step  371\n",
      "cost:  0.23208455105030748\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.009035558889259554 step  461\n",
      "cost:  0.23022881662056402\n",
      "opt took 0.00min,  461iters\n",
      "Epoch: [22][10/14]Time: 0.324 (0.397) Data: 0.277 (0.359) Loss: 0.2858 (0.3062)\n",
      "error:  0.009497011008548939 step  341\n",
      "cost:  0.23975433185449033\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009783091491498808 step  931\n",
      "cost:  0.24352339104138926\n",
      "opt took 0.00min,  931iters\n",
      "10-NN,s=0.1: TOP1:  87.36666666666666\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 23\n",
      "ResNet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.00937385277760705 step  781\n",
      "cost:  0.24025037775269265\n",
      "opt took 0.00min,  781iters\n",
      "Epoch: [23][0/14]Time: 0.313 (0.313) Data: 0.278 (0.278) Loss: 0.3044 (0.3044)\n",
      "error: ng head 3  0.008554640595128338 step  341\n",
      "cost:  0.22648681738341586\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009107851437501924 step  711\n",
      "cost:  0.21758890011607987\n",
      "opt took 0.00min,  711iters\n",
      "error:  0.008582865802251649 step  491\n",
      "cost:  0.21560757959935137\n",
      "opt took 0.00min,  491iters\n",
      "error:  0.009753639331395725 step  631\n",
      "cost:  0.22164299925168443\n",
      "opt took 0.00min,  631iters\n",
      "error:  0.009263129717764329 step  491\n",
      "cost:  0.23184888452510155\n",
      "opt took 0.00min,  491iters\n",
      "error:  0.009477243328378404 step  281\n",
      "cost:  0.2451827372521696\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008205255083984397 step  421\n",
      "cost:  0.24540416352735894\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.009511147845525758 step  541\n",
      "cost:  0.2371298310554669\n",
      "opt took 0.00min,  541iters\n",
      "error:  0.00985816671428108 step  931\n",
      "cost:  0.23730323617991747\n",
      "opt took 0.00min,  931iters\n",
      "Epoch: [23][10/14]Time: 0.035 (0.281) Data: 0.001 (0.241) Loss: 0.2846 (0.3038)\n",
      "error:  0.009446808315194954 step  621\n",
      "cost:  0.24006276881009225\n",
      "opt took 0.00min,  621iters\n",
      "error:  0.009184458557629172 step  811\n",
      "cost:  0.229133566907052\n",
      "opt took 0.00min,  811iters\n",
      "error:  0.009791565522008105 step  291\n",
      "cost:  0.22772220476264754\n",
      "opt took 0.00min,  291iters\n",
      "10-NN,s=0.1: TOP1:  87.5\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 24\n",
      "ResNet\n",
      "error:  0.009650511314331811 step  301\n",
      "cost:  0.24503507732385252\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [24][0/14]Time: 0.192 (0.192) Data: 0.157 (0.157) Loss: 0.3129 (0.3129)\n",
      "error:  0.009446925678858298 step  441\n",
      "cost:  0.24117853975232786\n",
      "opt took 0.00min,  441iters\n",
      "error:  0.009030163689357495 step  461\n",
      "cost:  0.234802955662212\n",
      "opt took 0.00min,  461iters\n",
      "error:  0.009893431391971741 step  911\n",
      "cost:  0.23511340391314622\n",
      "opt took 0.00min,  911iters\n",
      "error:  0.009616744412088485 step  1221\n",
      "cost:  0.2409742674803903\n",
      "opt took 0.00min, 1221iters\n",
      "error:  0.009515717612426977 step  1411\n",
      "cost:  0.2516411000479236\n",
      "opt took 0.01min, 1411iters\n",
      "error:  0.009191103778576015 step  591\n",
      "cost:  0.2605951383669575\n",
      "opt took 0.00min,  591iters\n",
      "error:  0.008769012982626223 step  281\n",
      "cost:  0.25738703831970144\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.009130313571482307 step  831\n",
      "cost:  0.24811074754774998\n",
      "opt took 0.00min,  831iters\n",
      "error:  0.009712774031611748 step  2111\n",
      "cost:  0.24564379114408275\n",
      "opt took 0.01min, 2111iters\n",
      "Epoch: [24][10/14]Time: 0.058 (0.314) Data: 0.000 (0.276) Loss: 0.3194 (0.3021)\n",
      "error:  0.009868420656617416 step  2231\n",
      "cost:  0.2589278512656501\n",
      "opt took 0.01min, 2231iters\n",
      "error:  0.009899102951939076 step  2101\n",
      "cost:  0.2599470745345963\n",
      "opt took 0.01min, 2101iters\n",
      "error:  0.008073369210574999 step  351\n",
      "cost:  0.25172874633470577\n",
      "opt took 0.00min,  351iters\n",
      "10-NN,s=0.1: TOP1:  87.46666666666667\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 25\n",
      "ResNet\n",
      "error:  0.00854030388673599 step  511\n",
      "cost:  0.2606015530698977\n",
      "opt took 0.00min,  511iters\n",
      "Epoch: [25][0/14]Time: 0.203 (0.203) Data: 0.175 (0.175) Loss: 0.3207 (0.3207)\n",
      "error:  0.009373730700658278 step  761\n",
      "cost:  0.244164120383511\n",
      "opt took 0.01min,  761iters\n",
      "error:  0.009741095858153526 step  1341\n",
      "cost:  0.22567962182168072\n",
      "opt took 0.01min, 1341iters\n",
      "error:  0.009916265660028345 step  2161\n",
      "cost:  0.2172396680200861\n",
      "opt took 0.01min, 2161iters\n",
      "error:  0.00957331471337497 step  1221\n",
      "cost:  0.21759503990207527\n",
      "opt took 0.00min, 1221iters\n",
      "error:  0.009717178472848031 step  941\n",
      "cost:  0.22516265802953345\n",
      "opt took 0.00min,  941iters\n",
      "error:  0.007276125030681313 step  271\n",
      "cost:  0.24294908621139674\n",
      "opt took 0.00min,  271iters\n",
      "error:  0.009940204773752126 step  321\n",
      "cost:  0.2617258253036354\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.007511426470552696 step  271\n",
      "cost:  0.2579270897735862\n",
      "opt took 0.00min,  271iters\n",
      "error:  0.009216808207500593 step  411\n",
      "cost:  0.2381066925116973\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [25][10/14]Time: 0.243 (0.321) Data: 0.204 (0.284) Loss: 0.3079 (0.3104)\n",
      "error:  0.007694199810968416 step  251\n",
      "cost:  0.23807383363287063\n",
      "opt took 0.00min,  251iters\n",
      "error:  0.009908466447819508 step  241\n",
      "cost:  0.24025668192395616\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.009019117925885611 step  601\n",
      "cost:  0.2515816683978699\n",
      "opt took 0.00min,  601iters\n",
      "10-NN,s=0.1: TOP1:  87.23333333333333\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 26\n",
      "ResNet\n",
      "error:  0.00966256977375779 step  1281\n",
      "cost:  0.23085095181247248\n",
      "opt took 0.00min, 1281iters\n",
      "Epoch: [26][0/14]Time: 0.321 (0.321) Data: 0.281 (0.281) Loss: 0.3366 (0.3366)\n",
      "error:  0.00973439190800851 step  1411\n",
      "cost:  0.21999917322079784\n",
      "opt took 0.01min, 1411iters\n",
      "error:  0.009480060952384628 step  571\n",
      "cost:  0.21278409198720896\n",
      "opt took 0.00min,  571iters\n",
      "error:  0.007716778440645999 step  281\n",
      "cost:  0.2195180007854646\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008468713587603816 step  441\n",
      "cost:  0.2261587378548458\n",
      "opt took 0.00min,  441iters\n",
      "error:  0.008916160409193075 step  481\n",
      "cost:  0.2327843846451842\n",
      "opt took 0.00min,  481iters\n",
      "error:  0.009506621654667091 step  601\n",
      "cost:  0.24597393382066862\n",
      "opt took 0.00min,  601iters\n",
      "error:  0.008163804522852036 step  391\n",
      "cost:  0.27394671890977024\n",
      "opt took 0.00min,  391iters\n",
      "error:  0.006837425395861851 step  171\n",
      "cost:  0.2754788619604786\n",
      "opt took 0.00min,  171iters\n",
      "error:  0.008181751111164703 step  331\n",
      "cost:  0.2541557541791302\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [26][10/14]Time: 0.229 (0.259) Data: 0.192 (0.221) Loss: 0.3181 (0.3215)\n",
      "error:  0.009424719570433115 step  831\n",
      "cost:  0.23747885135856095\n",
      "opt took 0.00min,  831iters\n",
      "error:  0.009977731062968731 step  891\n",
      "cost:  0.24240421089322528\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.009923286579149204 step  3991\n",
      "cost:  0.23752769953113254\n",
      "opt took 0.01min, 3991iters\n",
      "10-NN,s=0.1: TOP1:  87.33333333333333\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 27\n",
      "ResNet\n",
      "error:  0.008287511923441682 step  411\n",
      "cost:  0.23130770351114785\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [27][0/14]Time: 0.209 (0.209) Data: 0.174 (0.174) Loss: 0.3047 (0.3047)\n",
      "error:  0.008555583879806594 step  301\n",
      "cost:  0.22742356514996354\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.009446185736384782 step  401\n",
      "cost:  0.22928231558539997\n",
      "opt took 0.00min,  401iters\n",
      "error:  0.009091913716296918 step  821\n",
      "cost:  0.2296427370272118\n",
      "opt took 0.00min,  821iters\n",
      "error:  0.009967596135009416 step  1121\n",
      "cost:  0.23183360993395177\n",
      "opt took 0.00min, 1121iters\n",
      "error:  0.008965023392158211 step  761\n",
      "cost:  0.23950629320610456\n",
      "opt took 0.00min,  761iters\n",
      "error:  0.009724721980517659 step  921\n",
      "cost:  0.2430953725826422\n",
      "opt took 0.00min,  921iters\n",
      "error:  0.008725550577287966 step  221\n",
      "cost:  0.2252929911758903\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.009882762810253243 step  201\n",
      "cost:  0.22248116383280606\n",
      "opt took 0.00min,  201iters\n",
      "error:  0.008276218003267277 step  171\n",
      "cost:  0.22437226176510314\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [27][10/14]Time: 0.232 (0.276) Data: 0.193 (0.233) Loss: 0.3023 (0.3024)\n",
      "error:  0.009038078686969042 step  261\n",
      "cost:  0.22561478825961886\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.007580101422514196 step  241\n",
      "cost:  0.235238326620549\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.007785408473811706 step  251\n",
      "cost:  0.23203638239197708\n",
      "opt took 0.00min,  251iters\n",
      "10-NN,s=0.1: TOP1:  87.8\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 28\n",
      "ResNet\n",
      "error:  0.00927955759588317 step  291\n",
      "cost:  0.24856778467968918\n",
      "opt took 0.00min,  291iters\n",
      "Epoch: [28][0/14]Time: 0.227 (0.227) Data: 0.188 (0.188) Loss: 0.2678 (0.2678)\n",
      "error:  0.009352399237801068 step  731\n",
      "cost:  0.2525204119711465\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.00925873537472821 step  641\n",
      "cost:  0.25573657647495546\n",
      "opt took 0.00min,  641iters\n",
      "error:  0.009568426747516234 step  671\n",
      "cost:  0.25583527721845756\n",
      "opt took 0.00min,  671iters\n",
      "error:  0.008659886367551772 step  441\n",
      "cost:  0.244072556134426\n",
      "opt took 0.00min,  441iters\n",
      "error:  0.008834237032184444 step  291\n",
      "cost:  0.23310430260035325\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.009636862212865194 step  431\n",
      "cost:  0.226649649085913\n",
      "opt took 0.00min,  431iters\n",
      "error:  0.008458563332379598 step  401\n",
      "cost:  0.23280952963328225\n",
      "opt took 0.00min,  401iters\n",
      "error:  0.009938446507693155 step  331\n",
      "cost:  0.23628271513403296\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.009282215495097557 step  701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  0.23728395099435723\n",
      "opt took 0.00min,  701iters\n",
      "Epoch: [28][10/14]Time: 0.251 (0.253) Data: 0.214 (0.212) Loss: 0.2653 (0.2804)\n",
      "error:  0.008978889410409918 step  621\n",
      "cost:  0.23624904967090807\n",
      "opt took 0.00min,  621iters\n",
      "error:  0.009033281661146142 step  721\n",
      "cost:  0.24077084501835844\n",
      "opt took 0.00min,  721iters\n",
      "10-NN,s=0.1: TOP1:  86.96666666666667\n",
      "best accuracy: 88.10\n",
      "\n",
      "Epoch: 29\n",
      "ResNet\n",
      "error:  0.008426057317733604 step  341\n",
      "cost:  0.24854625125527768\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [29][0/14]Time: 0.246 (0.246) Data: 0.210 (0.210) Loss: 0.2758 (0.2758)\n",
      "error:  0.007665200918131387 step  271\n",
      "cost:  0.2631346163158891\n",
      "opt took 0.00min,  271iters\n",
      "error:  0.008676117360015834 step  481\n",
      "cost:  0.2745491310705156\n",
      "opt took 0.00min,  481iters\n",
      "error:  0.00986286404382175 step  471\n",
      "cost:  0.26087774192904345\n",
      "opt took 0.00min,  471iters\n",
      "error:  g head 9 0.008917355798661819 step  281\n",
      "cost:  0.23588946771514616\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008107707334780612 step  341\n",
      "cost:  0.22437110049287207\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009237007170254441 step  371\n",
      "cost:  0.22058779130103373\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.009764391299519004 step  561\n",
      "cost:  0.21812349846407833\n",
      "opt took 0.00min,  561iters\n",
      "error:  0.008685057557094256 step  271\n",
      "cost:  0.22170753851898162\n",
      "opt took 0.00min,  271iters\n",
      "error:  0.009241791771369412 step  551\n",
      "cost:  0.230499711201905\n",
      "opt took 0.00min,  551iters\n",
      "error:  0.009762845954397026 step  1631\n",
      "cost:  0.23302332709098322\n",
      "opt took 0.01min, 1631iters\n",
      "Epoch: [29][10/14]Time: 0.539 (0.270) Data: 0.503 (0.233) Loss: 0.2774 (0.2841)\n",
      "error:  0.008941239038271154 step  421\n",
      "cost:  0.24250479210374237\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.008433512127784093 step  281\n",
      "cost:  0.2448307174090882\n",
      "opt took 0.00min,  281iters\n",
      "10-NN,s=0.1: TOP1:  87.3\n",
      "best accuracy: 88.10\n",
      "doing PCA with 4 components ..done\n",
      "10-NN,s=0.1: TOP1:  65.1\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "best_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    \n",
    "    acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim)\n",
    "    accuracies.append(acc)\n",
    "    feature_return_switch(model, False)\n",
    "#     writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "        best_accuracies.append(best_acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = my_kNN(model, K=[50, 10], sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "#         for num_nn in [50, 10]:\n",
    "#             for sig in [0.1, 0.5]:\n",
    "#                 writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "#                 i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "end = time.time()\n",
    "\n",
    "# checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "# model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc(accuracies, model_name, dataset_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-labeling\n",
      "-------------------------\n",
      "Model:      ResNet\n",
      "Dataset:    PenDigits\n",
      "Epochs:     30\n",
      "lr:         0.003\n",
      "Batch size: 500\n",
      "Heads:      10\n",
      "Time:       137.36616945266724\n",
      "Best acc:   88.1\n"
     ]
    }
   ],
   "source": [
    "print (\"Self-labeling\")\n",
    "print (\"-------------------------\")\n",
    "print (\"Model:     \", model_name)\n",
    "print (\"Dataset:   \", dataset_name)\n",
    "print (\"Epochs:    \", epochs)\n",
    "print (\"lr:        \", lr)\n",
    "print (\"Batch size:\", batch_size)\n",
    "print (\"Heads:     \", hc)\n",
    "print (\"Time:      \", end-start)\n",
    "print (\"Best acc:  \", best_acc*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
