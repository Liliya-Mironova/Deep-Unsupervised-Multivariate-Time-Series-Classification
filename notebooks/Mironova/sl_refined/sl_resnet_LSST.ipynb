{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"VGG\"\n",
    "# magic_dim = 9216\n",
    "\n",
    "model_name = \"ResNet\"\n",
    "magic_dim = 2304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"LSST\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"/root/data/Multivariate_ts\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 20   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './resnet1d_exp' # experiments results dir\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 100\n",
    "lr=0.003     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 10\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sktime.utils.load_data import load_from_tsfile_to_dataframe\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_torch(X):\n",
    "    X = X.applymap(np.array)\n",
    "    dimensions_lst = []\n",
    "\n",
    "    for dim in X.columns:\n",
    "        dimensions_lst.append(np.dstack(list(X[dim].values))[0])\n",
    "\n",
    "    dimensions_lst = np.array(dimensions_lst)\n",
    "    X = torch.from_numpy(np.array(dimensions_lst, dtype=np.float64))\n",
    "    X = X.transpose(0, 2)\n",
    "    X = X.transpose(1, 2)\n",
    "    X = F.normalize(X, dim=1)\n",
    "    return X.float()\n",
    "\n",
    "def answers_to_torch(y):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    y = torch.from_numpy(np.array(y, dtype=np.int32))\n",
    "    y = y.long()\n",
    "    return y\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TRAIN.ts')\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TEST.ts')\n",
    "\n",
    "X_train = features_to_torch(X_train)\n",
    "X_test = features_to_torch(X_test)\n",
    "\n",
    "y_train = answers_to_torch(y_train)\n",
    "y_test = answers_to_torch(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_steps: 36\n",
      "train samples_num: 2459\n",
      "dims_num: 6\n",
      "num_classes: 14\n"
     ]
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "time_steps = X_train.shape[2]\n",
    "dims_num = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print('time_steps:', time_steps)\n",
    "print('train samples_num:', N)\n",
    "print('dims_num:', dims_num)\n",
    "print('num_classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters (AlexNet in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10                 # number of heads\n",
    "ncl=num_classes       # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# # (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "# CFG = {\n",
    "#     'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "#     'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['resnetv1','resnetv1_18']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, in_channel=3, width=1, num_classes=[1000]):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        self.base = int(16 * width)\n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(in_channel, 16, kernel_size=3, padding=1, bias=False), # [100, 16, 36]\n",
    "                            nn.BatchNorm1d(16),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            self._make_layer(block, self.base, layers[0]),                   # [100, 16, 36]\n",
    "                            self._make_layer(block, self.base * 2, layers[1]),               # [100, 32, 36]\n",
    "                            self._make_layer(block, self.base * 4, layers[2]),               # [100, 64, 36]\n",
    "                            self._make_layer(block, self.base * 8, layers[3]),               # [100, 128, 36]\n",
    "                            nn.AvgPool1d(2),                                                 # [100, 128, 18]\n",
    "        ])\n",
    "    \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnetv1_18(num_classes=[1000]):\n",
    "    \"\"\"Encoder for instance discrimination and MoCo\"\"\"\n",
    "    return resnet18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        \n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(dims_num, 64, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "\n",
    "#                             nn.Flatten(),\n",
    "#                             nn.Linear(in_features=512 * (time_steps // 2**5), out_features=fc_hidden_dim, bias=True),\n",
    "\n",
    "#                             nn.ReLU(inplace=True),\n",
    "#                             nn.Dropout(p=0.5, inplace=False),\n",
    "#                             nn.Linear(in_features=fc_hidden_dim, out_features=fc_hidden_dim, bias=True),\n",
    "#                             nn.ReLU(inplace=True),\n",
    "#                             nn.Dropout(p=0.5, inplace=False),\n",
    "#                             nn.Linear(in_features=fc_hidden_dim, out_features=num_classes, bias=True),\n",
    "#                             nn.Softmax()\n",
    "        ])\n",
    "        \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())  # [50, 10, 400] -> [50, 512, 12]\n",
    "        out = out.view(out.size(0), -1) # [50, magic_dim]\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "                print (out.size())\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "        \n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((N, ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((N, magic_dim)) # knn_dim\n",
    "    \n",
    "    for batch_idx, (data, _, _selected) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data.float())\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy() # p: [20, magic_dim]\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(acc, model_name, dataset_name):\n",
    "    step = 3\n",
    "    x = np.arange(1, epochs//step + 1)\n",
    "    acc = acc[::step]\n",
    "    plt.plot(x*step, acc[1:])\n",
    "    plt.xlabel(\"# epoch\")\n",
    "    plt.ylabel(\"Accuracy, %\")\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Train accuracy: self-labeling, {model_name}, {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(model_name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N // batch_size + batch_idx\n",
    "        if niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h], selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "#         if True:\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N // batch_size, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "#             writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*N/batch_size)\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet created\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"ResNet\":\n",
    "    model = resnet18(num_classes=numc, in_channel=dims_num)\n",
    "else:\n",
    "    model = VGG(num_classes=numc)\n",
    "print (model_name, \"created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [30.0, 21.0, 20.95, 20.89, 20.84, 20.79, 20.74, 20.68, 20.63, 20.58, 20.53, 20.47, 20.42, 20.37, 20.32, 20.26, 20.21, 20.16, 20.11, 20.05, 20.0, 19.95, 19.89, 19.84, 19.79, 19.74, 19.68, 19.63, 19.58, 19.53, 19.47, 19.42, 19.37, 19.32, 19.26, 19.21, 19.16, 19.11, 19.05, 19.0, 18.95, 18.89, 18.84, 18.79, 18.74, 18.68, 18.63, 18.58, 18.53, 18.47, 18.42, 18.37, 18.32, 18.26, 18.21, 18.16, 18.11, 18.05, 18.0, 17.95, 17.89, 17.84, 17.79, 17.74, 17.68, 17.63, 17.58, 17.53, 17.47, 17.42, 17.37, 17.32, 17.26, 17.21, 17.16, 17.11, 17.05, 17.0, 16.95, 16.89, 16.84, 16.79, 16.74, 16.68, 16.63, 16.58, 16.53, 16.47, 16.42, 16.37, 16.32, 16.26, 16.21, 16.16, 16.11, 16.05, 16.0, 15.95, 15.89, 15.84, 15.79, 15.74, 15.68, 15.63, 15.58, 15.53, 15.47, 15.42, 15.37, 15.32, 15.26, 15.21, 15.16, 15.11, 15.05, 15.0, 14.95, 14.89, 14.84, 14.79, 14.74, 14.68, 14.63, 14.58, 14.53, 14.47, 14.42, 14.37, 14.32, 14.26, 14.21, 14.16, 14.11, 14.05, 14.0, 13.95, 13.89, 13.84, 13.79, 13.74, 13.68, 13.63, 13.58, 13.53, 13.47, 13.42, 13.37, 13.32, 13.26, 13.21, 13.16, 13.11, 13.05, 13.0, 12.95, 12.89, 12.84, 12.79, 12.74, 12.68, 12.63, 12.58, 12.53, 12.47, 12.42, 12.37, 12.32, 12.26, 12.21, 12.16, 12.11, 12.05, 12.0, 11.95, 11.89, 11.84, 11.79, 11.74, 11.68, 11.63, 11.58, 11.53, 11.47, 11.42, 11.37, 11.32, 11.26, 11.21, 11.16, 11.11, 11.05, 11.0, 10.95, 10.89, 10.84, 10.79, 10.74, 10.68, 10.63, 10.58, 10.53, 10.47, 10.42, 10.37, 10.32, 10.26, 10.21, 10.16, 10.11, 10.05, 10.0, 9.95, 9.89, 9.84, 9.79, 9.74, 9.68, 9.63, 9.58, 9.53, 9.47, 9.42, 9.37, 9.32, 9.26, 9.21, 9.16, 9.11, 9.05, 9.0, 8.95, 8.89, 8.84, 8.79, 8.74, 8.68, 8.63, 8.58, 8.53, 8.47, 8.42, 8.37, 8.32, 8.26, 8.21, 8.16, 8.11, 8.05, 8.0, 7.95, 7.89, 7.84, 7.79, 7.74, 7.68, 7.63, 7.58, 7.53, 7.47, 7.42, 7.37, 7.32, 7.26, 7.21, 7.16, 7.11, 7.05, 7.0, 6.95, 6.89, 6.84, 6.79, 6.74, 6.68, 6.63, 6.58, 6.53, 6.47, 6.42, 6.37, 6.32, 6.26, 6.21, 6.16, 6.11, 6.05, 6.0, 5.95, 5.89, 5.84, 5.79, 5.74, 5.68, 5.63, 5.58, 5.53, 5.47, 5.42, 5.37, 5.32, 5.26, 5.21, 5.16, 5.11, 5.05, 5.0, 4.95, 4.89, 4.84, 4.79, 4.74, 4.68, 4.63, 4.58, 4.53, 4.47, 4.42, 4.37, 4.32, 4.26, 4.21, 4.16, 4.11, 4.05, 4.0, 3.95, 3.89, 3.84, 3.79, 3.74, 3.68, 3.63, 3.58, 3.53, 3.47, 3.42, 3.37, 3.32, 3.26, 3.21, 3.16, 3.11, 3.05, 3.0, 2.95, 2.89, 2.84, 2.79, 2.74, 2.68, 2.63, 2.58, 2.53, 2.47, 2.42, 2.37, 2.32, 2.26, 2.21, 2.16, 2.11, 2.05, 2.0, 1.95, 1.89, 1.84, 1.79, 1.74, 1.68, 1.63, 1.58, 1.53, 1.47, 1.42, 1.37, 1.32, 1.26, 1.21, 1.16, 1.11, 1.05, 1.0, 0.95, 0.89, 0.84, 0.79, 0.74, 0.68, 0.63, 0.58, 0.53, 0.47, 0.42, 0.37, 0.32, 0.26, 0.21, 0.16, 0.11, 0.05, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'./runs/{dataset_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(net, K, sigma=0.1, dim=128, use_pca=False):\n",
    "    net.eval()\n",
    "    # this part is ugly but made to be backwards-compatible. there was a change in cifar dataset's structure.\n",
    "    trainLabels = y_train\n",
    "    LEN = N\n",
    "    C = trainLabels.max() + 1\n",
    "\n",
    "    trainFeatures = torch.zeros((magic_dim, LEN))  # , device='cuda:0') # dim\n",
    "    normalize = Normalize()\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=False)):\n",
    "        batchSize = batch_size\n",
    "        inputs = inputs.cuda()\n",
    "        features = net(inputs.float())\n",
    "        if not use_pca:\n",
    "            features = normalize(features)\n",
    "        trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu()\n",
    "        \n",
    "    if use_pca:\n",
    "        comps = 4\n",
    "        print('doing PCA with %s components'%comps, end=' ')\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=comps, whiten=False)\n",
    "        trainFeatures = pca.fit_transform(trainFeatures.numpy().T)\n",
    "        trainFeatures = torch.Tensor(trainFeatures)\n",
    "        trainFeatures = normalize(trainFeatures).t()\n",
    "        print('..done')\n",
    "    def eval_k_s(K_,sigma_):\n",
    "        total = 0\n",
    "        top1 = 0.\n",
    "        top5 = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            retrieval_one_hot = torch.zeros(K_, C)# .cuda()\n",
    "            for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=False)):\n",
    "                targets = targets # .cuda(async=True) # or without async for py3.7\n",
    "                inputs = inputs.cuda()\n",
    "                batchSize = batch_size\n",
    "                features = net(inputs)\n",
    "                if use_pca:\n",
    "                    features = pca.transform(features.cpu().numpy())\n",
    "                    features = torch.Tensor(features).cuda()\n",
    "                features = normalize(features).cpu()\n",
    "\n",
    "                dist = torch.mm(features, trainFeatures)\n",
    "\n",
    "                yd, yi = dist.topk(K_, dim=1, largest=True, sorted=True)\n",
    "                candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "                retrieval = torch.gather(candidates, 1, yi).long()\n",
    "\n",
    "                retrieval_one_hot.resize_(batchSize * K_, C).zero_()\n",
    "                retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1.)\n",
    "                \n",
    "                yd_transform = yd.clone().div_(sigma_).exp_()\n",
    "                probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C),\n",
    "                                            yd_transform.view(batchSize, -1, 1)),\n",
    "                                  1)\n",
    "                _, predictions = probs.sort(1, True)\n",
    "\n",
    "                # Find which predictions match the target\n",
    "                correct = predictions.eq(targets.data.view(-1, 1))\n",
    "\n",
    "                top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "                top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        print(f\"{K_}-NN,s={sigma_}: TOP1: \", top1 * 100. / total)\n",
    "        return top1 / total\n",
    "\n",
    "    if isinstance(K, list):\n",
    "        res = []\n",
    "        for K_ in K:\n",
    "            for sigma_ in sigma:\n",
    "                res.append(eval_k_s(K_, sigma_))\n",
    "        return res\n",
    "    else:\n",
    "        res = eval_k_s(K, sigma)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "ResNet\n",
      "error:  0.0008437758467613188 step  31\n",
      "cost:  2.1598333479398892\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [0][0/24]Time: 0.226 (0.226) Data: 0.175 (0.175) Loss: 2.7502 (2.7502)\n",
      "error:  0.004737286195829471 step  91\n",
      "cost:  5.2851966821958465\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.006471255248513774 step  81\n",
      "cost:  3.373770886480058\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.004445682548518071 step  61\n",
      "cost:  2.469353470421497\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.002148648513817908 step  81\n",
      "cost:  2.366910392152002\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.009543004796728427 step  91\n",
      "cost:  2.376417531715943\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.009378331898404868 step  91\n",
      "cost:  2.3347647324626264\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.005546316022491782 step  121\n",
      "cost:  2.263336923968059\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [0][10/24]Time: 0.278 (0.188) Data: 0.235 (0.147) Loss: 3.0240 (4.0294)\n",
      "error:  0.008538072335483315 step  101\n",
      "cost:  2.2453240001510597\n",
      "opt took 0.00min,  101iters\n",
      "error:  0.007279118370323134 step  81\n",
      "cost:  2.1705176616773816\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.0036590735457407986 step  101\n",
      "cost:  2.12380559763175\n",
      "opt took 0.00min,  101iters\n",
      "error:  0.0058835078955921105 step  71\n",
      "cost:  2.111269239489921\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.006404883107347659 step  61\n",
      "cost:  2.12824356534988\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.006281596097504671 step  61\n",
      "cost:  2.107842393295652\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.0026632858865091036 step  71\n",
      "cost:  2.0883970538810095\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.003393345081689869 step  71\n",
      "cost:  2.0192919098893225\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [0][20/24]Time: 0.225 (0.201) Data: 0.187 (0.158) Loss: 2.6725 (3.4454)\n",
      "error:  0.0027681403976922425 step  81\n",
      "cost:  1.993874410396372\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.004695678344804777 step  111\n",
      "cost:  1.8642781503720112\n",
      "opt took 0.00min,  111iters\n",
      "10-NN,s=0.1: TOP1:  27.125\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 4 components ..done\n",
      "50-NN,s=0.1: TOP1:  30.291666666666668\n",
      "50-NN,s=0.5: TOP1:  30.625\n",
      "10-NN,s=0.1: TOP1:  25.083333333333332\n",
      "10-NN,s=0.5: TOP1:  24.916666666666668\n",
      "best accuracy: 27.12\n",
      "\n",
      "Epoch: 1\n",
      "ResNet\n",
      "error:  0.0005913407314837027 step  21\n",
      "cost:  2.4729166230204145\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [1][0/24]Time: 0.201 (0.201) Data: 0.159 (0.159) Loss: 2.5859 (2.5859)\n",
      "error:  6.715285421998729e-05 step  31\n",
      "cost:  2.4158793601778368\n",
      "opt took 0.00min,   31iters\n",
      "error:  0.007114684954883765 step  41\n",
      "cost:  2.321143616459682\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.002377973942929379 step  71\n",
      "cost:  2.232339040384297\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.00851071793189817 step  71\n",
      "cost:  2.2153972332860756\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.00249337823250273 step  91\n",
      "cost:  2.157120930331986\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.0073959226476900675 step  131\n",
      "cost:  1.9997488768396599\n",
      "opt took 0.00min,  131iters\n",
      "error:  0.005839120209467419 step  161\n",
      "cost:  1.9039591377369618\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.006840754714356967 step  201\n",
      "cost:  1.8316317076029813\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [1][10/24]Time: 0.257 (0.200) Data: 0.214 (0.155) Loss: 2.5006 (2.5183)\n",
      "error:  0.006234553201913129 step  221\n",
      "cost:  1.768847080075736\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.006621024116713059 step  221\n",
      "cost:  1.6656368305812976\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.008391735532556455 step  241\n",
      "cost:  1.6171734839669722\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.009266591339043173 step  261\n",
      "cost:  1.5681766553223202\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.009049652323475654 step  281\n",
      "cost:  1.5045198929338068\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008159163828222238 step  281\n",
      "cost:  1.4967702646882797\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008776854557659863 step  321\n",
      "cost:  1.4360317285599478\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [1][20/24]Time: 0.056 (0.196) Data: 0.001 (0.154) Loss: 2.4017 (2.4782)\n",
      "error:  0.009673338969387824 step  321\n",
      "cost:  1.3848191774999183\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.008023253746940684 step  341\n",
      "cost:  1.356285361238806\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.007970167713638898 step  341\n",
      "cost:  1.3357790975553028\n",
      "opt took 0.00min,  341iters\n",
      "10-NN,s=0.1: TOP1:  26.208333333333332\n",
      "best accuracy: 27.12\n",
      "\n",
      "Epoch: 2\n",
      "ResNet\n",
      "error:  2.8965600923358537e-05 step  31\n",
      "cost:  2.337437637354711\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [2][0/24]Time: 0.228 (0.228) Data: 0.182 (0.182) Loss: 2.3188 (2.3188)\n",
      "error:  0.0006006112491773186 step  31\n",
      "cost:  2.305427573822726\n",
      "opt took 0.00min,   31iters\n",
      "error:  0.0002753440279640307 step  41\n",
      "cost:  2.2377365545807857\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.0018196844650713029 step  51\n",
      "cost:  2.1401169924641033\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.00436729030192351 step  101\n",
      "cost:  1.8945077654531253\n",
      "opt took 0.00min,  101iters\n",
      "error:  0.004598466814037416 step  131\n",
      "cost:  1.7655137379830155\n",
      "opt took 0.00min,  131iters\n",
      "error:  0.00897302169422265 step  151\n",
      "cost:  1.6286776529058062\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.008485655280841775 step  171\n",
      "cost:  1.509533938497037\n",
      "opt took 0.00min,  171iters\n",
      "error:  0.008844676625644698 step  231\n",
      "cost:  1.3398160432480555\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [2][10/24]Time: 0.290 (0.220) Data: 0.245 (0.176) Loss: 2.1938 (2.2547)\n",
      "error:  0.00973210758029286 step  231\n",
      "cost:  1.2544967334458075\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.008461819584447472 step  271\n",
      "cost:  1.2220449014180317\n",
      "opt took 0.00min,  271iters\n",
      "error:  0.008829751103784256 step  351\n",
      "cost:  1.0838666958400192\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.009529520976291561 step  361\n",
      "cost:  1.0214641990858309\n",
      "opt took 0.00min,  361iters\n",
      "error:  0.007441918758569699 step  341\n",
      "cost:  0.9956439806772461\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.008321088053949044 step  411\n",
      "cost:  0.9175259837908658\n",
      "opt took 0.00min,  411iters\n",
      "error:  0.009171384703107854 step  451\n",
      "cost:  0.9031049443183782\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.008051937828697175 step  391\n",
      "cost:  0.8961777512516066\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [2][20/24]Time: 0.243 (0.220) Data: 0.207 (0.178) Loss: 2.1989 (2.2271)\n",
      "error:  0.00984431771958294 step  341\n",
      "cost:  0.8739465380619391\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.008903980844034987 step  411\n",
      "cost:  0.8727209630059422\n",
      "opt took 0.00min,  411iters\n",
      "10-NN,s=0.1: TOP1:  26.416666666666668\n",
      "best accuracy: 27.12\n",
      "\n",
      "Epoch: 3\n",
      "ResNet\n",
      "error:  0.0004946804558558782 step  31\n",
      "cost:  2.210262272372159\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [3][0/24]Time: 0.207 (0.207) Data: 0.165 (0.165) Loss: 2.0840 (2.0840)\n",
      "error:  0.004390839310805972 step  31\n",
      "cost:  2.1661865684931065\n",
      "opt took 0.00min,   31iters\n",
      "error:  0.002389941965108 step  61\n",
      "cost:  1.9659490534470376\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.0045173357440551065 step  91\n",
      "cost:  1.8552574173701168\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.009897460493073829 step  111\n",
      "cost:  1.7228552939005948\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.008181929275736133 step  161\n",
      "cost:  1.5953737561302086\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.007007172127436734 step  241\n",
      "cost:  1.3721238798391164\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.006489764418622368 step  231\n",
      "cost:  1.2900666821212072\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.009233753262443911 step  261\n",
      "cost:  1.2108019499892768\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [3][10/24]Time: 0.257 (0.199) Data: 0.220 (0.159) Loss: 1.9906 (2.0164)\n",
      "error:  0.009251953617731123 step  281\n",
      "cost:  1.0714414686891711\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.007909109236060807 step  331\n",
      "cost:  1.0068011258142568\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.009847697187641558 step  301\n",
      "cost:  0.9476611133172829\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.006896531135235917 step  231\n",
      "cost:  0.8645078478753657\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.006876914841880355 step  231\n",
      "cost:  0.8392327569802834\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.008502711334427149 step  271\n",
      "cost:  0.8303593303867369\n",
      "opt took 0.00min,  271iters\n",
      "error:  0.009681949492542863 step  251\n",
      "cost:  0.8211031823792141\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [3][20/24]Time: 0.037 (0.192) Data: 0.001 (0.153) Loss: 1.9939 (1.9908)\n",
      "error:  0.008918163231858012 step  351\n",
      "cost:  0.7816885318358758\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.007951633227643695 step  351\n",
      "cost:  0.7735350603948676\n",
      "opt took 0.00min,  351iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009468899210750559 step  351\n",
      "cost:  0.7683324895280482\n",
      "opt took 0.00min,  351iters\n",
      "10-NN,s=0.1: TOP1:  26.583333333333332\n",
      "best accuracy: 27.12\n",
      "\n",
      "Epoch: 4\n",
      "ResNet\n",
      "error:  0.0018266175162542364 step  41\n",
      "cost:  2.102340930797898\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [4][0/24]Time: 0.224 (0.224) Data: 0.184 (0.184) Loss: 1.8263 (1.8263)\n",
      "error:  0.0009014806568430345 step  51\n",
      "cost:  2.064924177803476\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.005149999979509379 step  61\n",
      "cost:  1.9949805993997098\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.006279194808053723 step  81\n",
      "cost:  1.9041734031958613\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.006414832838723683 step  91\n",
      "cost:  1.7020717417737439\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.009794083107558804 step  121\n",
      "cost:  1.578502684012514\n",
      "opt took 0.00min,  121iters\n",
      "error:  0.006304714417449286 step  161\n",
      "cost:  1.4613476291621068\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.008189960406716934 step  201\n",
      "cost:  1.2715576268372766\n",
      "opt took 0.00min,  201iters\n",
      "error:  0.0083035388876328 step  201\n",
      "cost:  1.2166585425369763\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [4][10/24]Time: 0.248 (0.205) Data: 0.191 (0.162) Loss: 1.7640 (1.7912)\n",
      "error:  0.007595792689817693 step  201\n",
      "cost:  1.1558787144436922\n",
      "opt took 0.00min,  201iters\n",
      "error:  0.007286591586950997 step  241\n",
      "cost:  1.0374597579278386\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.007287144970855142 step  241\n",
      "cost:  0.9961624542880404\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.006726764869923851 step  231\n",
      "cost:  0.9400837390622927\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.00741416416741536 step  251\n",
      "cost:  0.8863787504427709\n",
      "opt took 0.00min,  251iters\n",
      "error:  0.007281413883420962 step  281\n",
      "cost:  0.7776137040569179\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.007795825135966661 step  311\n",
      "cost:  0.7242322137906985\n",
      "opt took 0.00min,  311iters\n",
      "error:  0.008113179345039412 step  341\n",
      "cost:  0.6816908846894753\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [4][20/24]Time: 0.265 (0.217) Data: 0.222 (0.174) Loss: 1.7201 (1.7631)\n",
      "error:  0.008166081595164854 step  301\n",
      "cost:  0.6350977802774894\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.008268799322750264 step  311\n",
      "cost:  0.6125110825254378\n",
      "opt took 0.00min,  311iters\n",
      "10-NN,s=0.1: TOP1:  27.041666666666668\n",
      "best accuracy: 27.12\n",
      "\n",
      "Epoch: 5\n",
      "ResNet\n",
      "error:  0.0013207351283620605 step  41\n",
      "cost:  2.0077265926290675\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [5][0/24]Time: 0.302 (0.302) Data: 0.252 (0.252) Loss: 1.4925 (1.4925)\n",
      "error:  0.0008672648782072079 step  51\n",
      "cost:  1.9677382812709132\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.0021443465975701237 step  71\n",
      "cost:  1.8147527804596713\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.008518326723137348 step  81\n",
      "cost:  1.6995928799237647\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.005692084521939145 step  101\n",
      "cost:  1.570138499268566\n",
      "opt took 0.00min,  101iters\n",
      "error:  0.007479884894748601 step  151\n",
      "cost:  1.3125150347372632\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.008158060256419963 step  181\n",
      "cost:  1.2065233431394113\n",
      "opt took 0.00min,  181iters\n",
      "error:  0.007518669607448403 step  201\n",
      "cost:  1.1451621046313125\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [5][10/24]Time: 0.052 (0.197) Data: 0.001 (0.153) Loss: 1.4630 (1.5169)\n",
      "error:  0.007532538894017837 step  241\n",
      "cost:  1.0538240278819568\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.008795386858033383 step  241\n",
      "cost:  0.9956335860670934\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.008805717485118603 step  281\n",
      "cost:  0.9148590440325668\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.00855548667588768 step  291\n",
      "cost:  0.8391162175475245\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.00918211136784619 step  291\n",
      "cost:  0.6979447751156709\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.008177823286896313 step  331\n",
      "cost:  0.6700309110436343\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.007526827161126959 step  251\n",
      "cost:  0.6503555908988736\n",
      "opt took 0.00min,  251iters\n",
      "error:  0.008779438427928277 step  361\n",
      "cost:  0.6534144068466653\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [5][20/24]Time: 0.338 (0.213) Data: 0.302 (0.170) Loss: 1.4451 (1.5054)\n",
      "error:  0.008472257524380189 step  371\n",
      "cost:  0.6659019111310478\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.008519637978457006 step  361\n",
      "cost:  0.6652794382965063\n",
      "opt took 0.00min,  361iters\n",
      "error:  0.00916656068196886 step  371\n",
      "cost:  0.652322106741871\n",
      "opt took 0.00min,  371iters\n",
      "10-NN,s=0.1: TOP1:  27.041666666666668\n",
      "best accuracy: 27.12\n",
      "\n",
      "Epoch: 6\n",
      "ResNet\n",
      "error:  0.0012965847986088352 step  51\n",
      "cost:  1.977265652305824\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [6][0/24]Time: 0.225 (0.225) Data: 0.183 (0.183) Loss: 1.3209 (1.3209)\n",
      "error:  0.0035631836971287134 step  51\n",
      "cost:  1.941723571626268\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.008876747800529161 step  41\n",
      "cost:  1.875917858980829\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.0028732134822025968 step  71\n",
      "cost:  1.6914740259277072\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.00576053729349979 step  81\n",
      "cost:  1.589628336237445\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.007932937961399933 step  111\n",
      "cost:  1.4845211042244357\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.007296541655270161 step  151\n",
      "cost:  1.324402622517424\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.008675730648102231 step  191\n",
      "cost:  1.2321627853677277\n",
      "opt took 0.00min,  191iters\n",
      "error:  0.008587231235849813 step  201\n",
      "cost:  1.1392889587068518\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [6][10/24]Time: 0.285 (0.216) Data: 0.242 (0.171) Loss: 1.2711 (1.3218)\n",
      "error:  0.006459651997480775 step  221\n",
      "cost:  1.0267691208802499\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.009069966513865158 step  251\n",
      "cost:  0.8551143052169539\n",
      "opt took 0.00min,  251iters\n",
      "error:  0.007956869854823023 step  301\n",
      "cost:  0.8092837609176282\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.009996365072274749 step  321\n",
      "cost:  0.7769592087259833\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.009636958442365007 step  351\n",
      "cost:  0.70812196513096\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.008675518157062134 step  381\n",
      "cost:  0.6868486881778023\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.009904198588746582 step  391\n",
      "cost:  0.6697491390748175\n",
      "opt took 0.00min,  391iters\n",
      "error:  0.009592853272400004 step  381\n",
      "cost:  0.6563565470537394\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [6][20/24]Time: 0.286 (0.223) Data: 0.249 (0.180) Loss: 1.2587 (1.2900)\n",
      "error:  0.008266215933504628 step  291\n",
      "cost:  0.6271369557957848\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.009433102581670183 step  311\n",
      "cost:  0.6031322719324194\n",
      "opt took 0.00min,  311iters\n",
      "10-NN,s=0.1: TOP1:  28.291666666666668\n",
      "Saving..\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 7\n",
      "ResNet\n",
      "error:  0.0022615947270081538 step  41\n",
      "cost:  1.856543558308297\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [7][0/24]Time: 0.199 (0.199) Data: 0.157 (0.157) Loss: 1.1312 (1.1312)\n",
      "error:  0.00877904315086886 step  41\n",
      "cost:  1.8219669590028609\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.0034536130948086274 step  51\n",
      "cost:  1.7614389369522732\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.006532918241954899 step  61\n",
      "cost:  1.6826831317256956\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.009865416724137877 step  91\n",
      "cost:  1.4738084437693881\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.009143725877816089 step  111\n",
      "cost:  1.3461074407594846\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.008336973071021547 step  141\n",
      "cost:  1.234845445865386\n",
      "opt took 0.00min,  141iters\n",
      "error:  0.008059174857692364 step  171\n",
      "cost:  1.1244384200590223\n",
      "opt took 0.00min,  171iters\n",
      "error:  0.007360287619737393 step  201\n",
      "cost:  0.9813808171295398\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [7][10/24]Time: 0.256 (0.207) Data: 0.205 (0.164) Loss: 1.1341 (1.0794)\n",
      "error:  0.00755023621182993 step  221\n",
      "cost:  0.9238617068833694\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.009108668539809961 step  221\n",
      "cost:  0.8627198646162003\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.008984604090391368 step  271\n",
      "cost:  0.7748237881981762\n",
      "opt took 0.00min,  271iters\n",
      "error:  0.009436304075527868 step  301\n",
      "cost:  0.724358678701964\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.008487431460876937 step  351\n",
      "cost:  0.6692618515121886\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.00842094600511345 step  341\n",
      "cost:  0.6255674324826884\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009000781057162177 step  381\n",
      "cost:  0.5690155182461367\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.008883281570696622 step  441\n",
      "cost:  0.5466629312655108\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [7][20/24]Time: 0.418 (0.218) Data: 0.351 (0.175) Loss: 1.0290 (1.0624)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.008112175119054932 step  441\n",
      "cost:  0.5312604954122998\n",
      "opt took 0.00min,  441iters\n",
      "error:  0.009027474190967721 step  411\n",
      "cost:  0.49966410034888964\n",
      "opt took 0.00min,  411iters\n",
      "10-NN,s=0.1: TOP1:  26.708333333333332\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 8\n",
      "ResNet\n",
      "error:  0.002181568246664045 step  51\n",
      "cost:  1.732910799710601\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][0/24]Time: 0.211 (0.211) Data: 0.174 (0.174) Loss: 0.8908 (0.8908)\n",
      "error:  0.005441660859568431 step  51\n",
      "cost:  1.7092118890620092\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.003326431014027076 step  71\n",
      "cost:  1.5705898597585408\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.004066187191877901 step  71\n",
      "cost:  1.4746487760101985\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.0044424399972675666 step  101\n",
      "cost:  1.358575504203919\n",
      "opt took 0.00min,  101iters\n",
      "error:  0.007188137592363697 step  131\n",
      "cost:  1.2515256383070135\n",
      "opt took 0.00min,  131iters\n",
      "error:  0.006012363839164481 step  191\n",
      "cost:  1.0741421866405816\n",
      "opt took 0.00min,  191iters\n",
      "error:  0.007473250877367388 step  191\n",
      "cost:  0.999876757370564\n",
      "opt took 0.00min,  191iters\n",
      "error:  0.008206739108952643 step  201\n",
      "cost:  0.9176552291211112\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [8][10/24]Time: 0.286 (0.196) Data: 0.249 (0.159) Loss: 0.8087 (0.8577)\n",
      "error:  0.008948728190934063 step  221\n",
      "cost:  0.7951789312344194\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.009913266319405922 step  231\n",
      "cost:  0.7669950564213933\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.009577650426748963 step  241\n",
      "cost:  0.7339598372440231\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.008105765885708327 step  281\n",
      "cost:  0.6933176479150471\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008475878480738386 step  281\n",
      "cost:  0.6181275553344056\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.007692592983212454 step  331\n",
      "cost:  0.5905927128965056\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.009371544672588383 step  361\n",
      "cost:  0.5541067312311294\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [8][20/24]Time: 0.037 (0.191) Data: 0.001 (0.153) Loss: 0.8034 (0.8470)\n",
      "error:  0.008624127307796492 step  341\n",
      "cost:  0.48127378052350334\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.008579594096873477 step  301\n",
      "cost:  0.4512669199918145\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.008314263405248101 step  411\n",
      "cost:  0.4318480325746586\n",
      "opt took 0.00min,  411iters\n",
      "10-NN,s=0.1: TOP1:  26.5\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 9\n",
      "ResNet\n",
      "error:  0.003474677725616049 step  61\n",
      "cost:  1.6153611878766883\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [9][0/24]Time: 0.201 (0.201) Data: 0.164 (0.164) Loss: 0.6321 (0.6321)\n",
      "error:  0.002483748439587652 step  61\n",
      "cost:  1.5952507733473917\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.006604481907633963 step  61\n",
      "cost:  1.559422947526443\n",
      "opt took 0.00min,   61iters\n",
      "error:  0.004599896931777314 step  71\n",
      "cost:  1.503507384450378\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.0064647310131429325 step  91\n",
      "cost:  1.3604650087131829\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.006559818429731634 step  111\n",
      "cost:  1.2759233148059135\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.005207039070155073 step  131\n",
      "cost:  1.1869249817873098\n",
      "opt took 0.00min,  131iters\n",
      "error:  0.00708321731924888 step  161\n",
      "cost:  1.0064718168515479\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.007602063899136424 step  171\n",
      "cost:  0.9222585761399222\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [9][10/24]Time: 0.259 (0.203) Data: 0.213 (0.160) Loss: 0.5294 (0.6051)\n",
      "error:  0.009222885983095286 step  181\n",
      "cost:  0.8411409989465717\n",
      "opt took 0.00min,  181iters\n",
      "error:  0.006709678821709808 step  221\n",
      "cost:  0.7838640494938037\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.008808160706745172 step  321\n",
      "cost:  0.6915822568978134\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.007282556939716245 step  281\n",
      "cost:  0.6652911688901252\n",
      "opt took 0.00min,  281iters\n",
      "error:  0.008871778875758074 step  341\n",
      "cost:  0.6398255287671151\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009096356020087404 step  321\n",
      "cost:  0.5604700583718867\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.008189549095010351 step  341\n",
      "cost:  0.5199732427478908\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.008643509000838323 step  371\n",
      "cost:  0.5022633746931535\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [9][20/24]Time: 0.403 (0.217) Data: 0.363 (0.174) Loss: 0.8104 (0.6017)\n",
      "error:  0.008946031305174684 step  301\n",
      "cost:  0.47288909018598546\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.008367459659036514 step  391\n",
      "cost:  0.47857824869088117\n",
      "opt took 0.00min,  391iters\n",
      "10-NN,s=0.1: TOP1:  27.041666666666668\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 10\n",
      "ResNet\n",
      "error:  0.009312238720327382 step  841\n",
      "cost:  0.72063515104869\n",
      "opt took 0.00min,  841iters\n",
      "Epoch: [10][0/24]Time: 0.315 (0.315) Data: 0.275 (0.275) Loss: 0.5281 (0.5281)\n",
      "error:  0.009489105770149497 step  1351\n",
      "cost:  0.7031466858425056\n",
      "opt took 0.00min, 1351iters\n",
      "error:  0.009728317126526886 step  801\n",
      "cost:  0.5981694161085847\n",
      "opt took 0.00min,  801iters\n",
      "error:  0.009947033672266126 step  731\n",
      "cost:  0.5158557172632643\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.008502790441102803 step  571\n",
      "cost:  0.4612940775220702\n",
      "opt took 0.00min,  571iters\n",
      "error:  0.008980071676508827 step  771\n",
      "cost:  0.44231625378388545\n",
      "opt took 0.01min,  771iters\n",
      "error:  0.009938347195174613 step  601\n",
      "cost:  0.4400155352405592\n",
      "opt took 0.00min,  601iters\n",
      "error:  0.009083076831800985 step  711\n",
      "cost:  0.43830739460483326\n",
      "opt took 0.00min,  711iters\n",
      "error:  0.009345453848257357 step  641\n",
      "cost:  0.4234450234658818\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [10][10/24]Time: 0.339 (0.302) Data: 0.302 (0.261) Loss: 0.6196 (0.5693)\n",
      "error:  0.00976739663611792 step  501\n",
      "cost:  0.4036201559973851\n",
      "opt took 0.00min,  501iters\n",
      "error:  0.009339180743970554 step  751\n",
      "cost:  0.3951402467013778\n",
      "opt took 0.00min,  751iters\n",
      "error:  0.00940411209569847 step  731\n",
      "cost:  0.3900113942284072\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.009380037952512965 step  1041\n",
      "cost:  0.34623585375155236\n",
      "opt took 0.00min, 1041iters\n",
      "error:  0.009930175406093089 step  771\n",
      "cost:  0.34530749212309364\n",
      "opt took 0.00min,  771iters\n",
      "error:  0.009398998782858548 step  661\n",
      "cost:  0.34414716345194674\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.009834671769977765 step  1051\n",
      "cost:  0.33241597423178654\n",
      "opt took 0.01min, 1051iters\n",
      "Epoch: [10][20/24]Time: 0.549 (0.276) Data: 0.498 (0.235) Loss: 0.5294 (0.5607)\n",
      "error:  0.009310624963031477 step  891\n",
      "cost:  0.32884281842436475\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.009515467482558604 step  901\n",
      "cost:  0.3210093904889106\n",
      "opt took 0.00min,  901iters\n",
      "error:  0.009881649544111859 step  911\n",
      "cost:  0.3188359128454433\n",
      "opt took 0.00min,  911iters\n",
      "10-NN,s=0.1: TOP1:  26.0\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 11\n",
      "ResNet\n",
      "error:  0.009497525631704029 step  891\n",
      "cost:  0.4663792622705032\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [11][0/24]Time: 0.313 (0.313) Data: 0.276 (0.276) Loss: 0.5167 (0.5167)\n",
      "error:  0.009983484617853478 step  941\n",
      "cost:  0.4517118024491635\n",
      "opt took 0.00min,  941iters\n",
      "error:  0.009447970820868457 step  1091\n",
      "cost:  0.4415198958287722\n",
      "opt took 0.00min, 1091iters\n",
      "error:  0.009546242096810542 step  821\n",
      "cost:  0.44004393973033595\n",
      "opt took 0.00min,  821iters\n",
      "error:  0.009409509271984029 step  721\n",
      "cost:  0.43928262847475374\n",
      "opt took 0.00min,  721iters\n",
      "error:  0.009259224714643799 step  851\n",
      "cost:  0.41912963908232054\n",
      "opt took 0.00min,  851iters\n",
      "error:  0.008796514746621797 step  621\n",
      "cost:  0.3993975254623982\n",
      "opt took 0.00min,  621iters\n",
      "error:  0.009720677029630531 step  891\n",
      "cost:  0.40488357468738145\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.009427234979124788 step  921\n",
      "cost:  0.41067044994567947\n",
      "opt took 0.00min,  921iters\n",
      "Epoch: [11][10/24]Time: 0.451 (0.301) Data: 0.414 (0.262) Loss: 0.4964 (0.5021)\n",
      "error:  0.00968638579375769 step  1161\n",
      "cost:  0.42907726746692626\n",
      "opt took 0.00min, 1161iters\n",
      "error:  0.009567096749955262 step  1011\n",
      "cost:  0.4004358855513503\n",
      "opt took 0.00min, 1011iters\n",
      "error:  0.009303342099616918 step  991\n",
      "cost:  0.3896460612762955\n",
      "opt took 0.00min,  991iters\n",
      "error:  0.009758277845855257 step  981\n",
      "cost:  0.3755866345615156\n",
      "opt took 0.00min,  981iters\n",
      "error:  0.009638738870270558 step  1031\n",
      "cost:  0.3519410209862545\n",
      "opt took 0.00min, 1031iters\n",
      "error:  0.009565803147163487 step  1191\n",
      "cost:  0.34401477214139364\n",
      "opt took 0.00min, 1191iters\n",
      "error:  0.00990230559197014 step  821\n",
      "cost:  0.33424434878903597\n",
      "opt took 0.00min,  821iters\n",
      "error:  0.009816787689721052 step  901\n",
      "cost:  0.32116562932321613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt took 0.00min,  901iters\n",
      "Epoch: [11][20/24]Time: 0.357 (0.317) Data: 0.321 (0.275) Loss: 0.4550 (0.4994)\n",
      "error:  0.009892851922097323 step  1371\n",
      "cost:  0.3129875665532508\n",
      "opt took 0.00min, 1371iters\n",
      "error:  0.009306813513456857 step  1061\n",
      "cost:  0.31535350868208856\n",
      "opt took 0.00min, 1061iters\n",
      "10-NN,s=0.1: TOP1:  26.666666666666668\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 12\n",
      "ResNet\n",
      "error:  0.009409924844783757 step  1041\n",
      "cost:  0.2693842284578818\n",
      "opt took 0.00min, 1041iters\n",
      "Epoch: [12][0/24]Time: 0.296 (0.296) Data: 0.261 (0.261) Loss: 0.4779 (0.4779)\n",
      "error:  0.00957835944323382 step  1281\n",
      "cost:  0.25692686367739576\n",
      "opt took 0.00min, 1281iters\n",
      "error:  0.009879238874894725 step  1211\n",
      "cost:  0.25334370252682725\n",
      "opt took 0.00min, 1211iters\n",
      "error:  0.009825215805516097 step  1181\n",
      "cost:  0.25678227949132504\n",
      "opt took 0.00min, 1181iters\n",
      "error:  0.009997111736450237 step  1021\n",
      "cost:  0.25777133231819604\n",
      "opt took 0.00min, 1021iters\n",
      "error:  0.009254918704273751 step  811\n",
      "cost:  0.25016381719828507\n",
      "opt took 0.01min,  811iters\n",
      "error:  0.00940297026551784 step  581\n",
      "cost:  0.2563509239523499\n",
      "opt took 0.00min,  581iters\n",
      "error:  0.009925160696503399 step  781\n",
      "cost:  0.25934640964357725\n",
      "opt took 0.00min,  781iters\n",
      "error:  0.009808998620182141 step  1021\n",
      "cost:  0.2514037374249617\n",
      "opt took 0.00min, 1021iters\n",
      "Epoch: [12][10/24]Time: 0.316 (0.305) Data: 0.281 (0.268) Loss: 0.3919 (0.4576)\n",
      "error:  0.009143720728954863 step  831\n",
      "cost:  0.25511535109351485\n",
      "opt took 0.00min,  831iters\n",
      "error:  0.009935686896476614 step  1401\n",
      "cost:  0.25878394207116107\n",
      "opt took 0.01min, 1401iters\n",
      "error:  0.009908016960996968 step  971\n",
      "cost:  0.27915763774689895\n",
      "opt took 0.00min,  971iters\n",
      "error:  0.009800063861029717 step  901\n",
      "cost:  0.2756150803680201\n",
      "opt took 0.00min,  901iters\n",
      "error:  0.0096519247501331 step  791\n",
      "cost:  0.2611111713727184\n",
      "opt took 0.00min,  791iters\n",
      "error:  0.00929101220866635 step  811\n",
      "cost:  0.2502568283510082\n",
      "opt took 0.00min,  811iters\n",
      "error:  0.00941309809001678 step  681\n",
      "cost:  0.2465175159476659\n",
      "opt took 0.00min,  681iters\n",
      "error:  0.009852239331632817 step  631\n",
      "cost:  0.25445492678406584\n",
      "opt took 0.00min,  631iters\n",
      "Epoch: [12][20/24]Time: 0.338 (0.300) Data: 0.301 (0.262) Loss: 0.6093 (0.4892)\n",
      "error:  0.009263225075778703 step  901\n",
      "cost:  0.2584824366850723\n",
      "opt took 0.00min,  901iters\n",
      "error:  0.0095740616592358 step  951\n",
      "cost:  0.26372725066447694\n",
      "opt took 0.00min,  951iters\n",
      "10-NN,s=0.1: TOP1:  27.083333333333332\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 13\n",
      "ResNet\n",
      "error:  0.009220067341347282 step  491\n",
      "cost:  0.2761930781917012\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [13][0/24]Time: 0.358 (0.358) Data: 0.301 (0.301) Loss: 0.5207 (0.5207)\n",
      "error:  0.009169222601959293 step  621\n",
      "cost:  0.26669395902726273\n",
      "opt took 0.00min,  621iters\n",
      "error:  0.009432778791580554 step  1121\n",
      "cost:  0.2512059276561959\n",
      "opt took 0.00min, 1121iters\n",
      "error:  0.00994995608301108 step  901\n",
      "cost:  0.25830702620372115\n",
      "opt took 0.01min,  901iters\n",
      "error:  0.009491530102203827 step  1141\n",
      "cost:  0.25409197777214737\n",
      "opt took 0.01min, 1141iters\n",
      "error:  0.009606741556202003 step  1161\n",
      "cost:  0.24407974369288912\n",
      "opt took 0.00min, 1161iters\n",
      "error:  0.009535851412291074 step  861\n",
      "cost:  0.24413586534642862\n",
      "opt took 0.00min,  861iters\n",
      "error:  0.009975244147280793 step  831\n",
      "cost:  0.23767917025291477\n",
      "opt took 0.00min,  831iters\n",
      "error:  0.009611846942108926 step  821\n",
      "cost:  0.23247738534155282\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [13][10/24]Time: 0.368 (0.324) Data: 0.328 (0.282) Loss: 0.5138 (0.4267)\n",
      "error:  0.00993315418076135 step  761\n",
      "cost:  0.23008910936334723\n",
      "opt took 0.00min,  761iters\n",
      "error:  0.00991165771106739 step  711\n",
      "cost:  0.2353810478711358\n",
      "opt took 0.00min,  711iters\n",
      "error:  0.009722425164685577 step  1181\n",
      "cost:  0.23778417463786908\n",
      "opt took 0.00min, 1181iters\n",
      "error:  0.0097395303460176 step  991\n",
      "cost:  0.23460887235946745\n",
      "opt took 0.01min,  991iters\n",
      "error:  0.009963311145974596 step  1061\n",
      "cost:  0.23683901233761626\n",
      "opt took 0.00min, 1061iters\n",
      "error:  0.009259659172114687 step  801\n",
      "cost:  0.2484072730951733\n",
      "opt took 0.00min,  801iters\n",
      "error:  0.009924289000949882 step  821\n",
      "cost:  0.24387445648558087\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [13][20/24]Time: 0.044 (0.308) Data: 0.001 (0.266) Loss: 0.5546 (0.4276)\n",
      "error:  0.00980202755164361 step  711\n",
      "cost:  0.2340261032581654\n",
      "opt took 0.00min,  711iters\n",
      "error:  0.00900367144325176 step  691\n",
      "cost:  0.23308524450040596\n",
      "opt took 0.00min,  691iters\n",
      "error:  0.00959233402301829 step  971\n",
      "cost:  0.23904334494719107\n",
      "opt took 0.00min,  971iters\n",
      "10-NN,s=0.1: TOP1:  26.583333333333332\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 14\n",
      "ResNet\n",
      "error:  0.009805723608277583 step  1151\n",
      "cost:  0.2588925165265071\n",
      "opt took 0.00min, 1151iters\n",
      "Epoch: [14][0/24]Time: 0.403 (0.403) Data: 0.342 (0.342) Loss: 0.4287 (0.4287)\n",
      "error:  0.009348225331988091 step  951\n",
      "cost:  0.2911741098961725\n",
      "opt took 0.00min,  951iters\n",
      "error:  0.009693094424709892 step  1111\n",
      "cost:  0.2901161986911158\n",
      "opt took 0.00min, 1111iters\n",
      "error:  0.009624412738625021 step  971\n",
      "cost:  0.2740110650744774\n",
      "opt took 0.01min,  971iters\n",
      "error:  0.009547067926635955 step  731\n",
      "cost:  0.23463706055506078\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.009810944311142156 step  971\n",
      "cost:  0.2348950330440017\n",
      "opt took 0.01min,  971iters\n",
      "error:  0.00969455151956966 step  1751\n",
      "cost:  0.23193073130147457\n",
      "opt took 0.00min, 1751iters\n",
      "error:  0.009658906541186174 step  971\n",
      "cost:  0.2354571415441537\n",
      "opt took 0.00min,  971iters\n",
      "error:  0.009909198858167656 step  1151\n",
      "cost:  0.2337484238555136\n",
      "opt took 0.00min, 1151iters\n",
      "Epoch: [14][10/24]Time: 0.407 (0.352) Data: 0.370 (0.310) Loss: 0.3866 (0.4508)\n",
      "error:  0.00931221993863951 step  1111\n",
      "cost:  0.21570901798156483\n",
      "opt took 0.00min, 1111iters\n",
      "error:  0.009617387511721143 step  1381\n",
      "cost:  0.20962680591063004\n",
      "opt took 0.00min, 1381iters\n",
      "error:  0.00923464450448419 step  791\n",
      "cost:  0.21186971145764663\n",
      "opt took 0.00min,  791iters\n",
      "error:  0.009948664422130493 step  1331\n",
      "cost:  0.21188083439938207\n",
      "opt took 0.01min, 1331iters\n",
      "error:  0.009851242761603207 step  1101\n",
      "cost:  0.21597869599435077\n",
      "opt took 0.00min, 1101iters\n",
      "error:  0.009860555253516479 step  1001\n",
      "cost:  0.22615102908835127\n",
      "opt took 0.00min, 1001iters\n",
      "error:  0.009497998037307331 step  781\n",
      "cost:  0.2193887411827686\n",
      "opt took 0.00min,  781iters\n",
      "error:  0.009830117970315633 step  1181\n",
      "cost:  0.2164193826728411\n",
      "opt took 0.00min, 1181iters\n",
      "Epoch: [14][20/24]Time: 0.422 (0.355) Data: 0.385 (0.315) Loss: 0.4152 (0.4345)\n",
      "error:  0.009428843889807026 step  911\n",
      "cost:  0.2190743369883686\n",
      "opt took 0.00min,  911iters\n",
      "error:  0.009800035918391958 step  1031\n",
      "cost:  0.23628807150024667\n",
      "opt took 0.00min, 1031iters\n",
      "10-NN,s=0.1: TOP1:  27.083333333333332\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 15\n",
      "ResNet\n",
      "error:  0.009366075464431356 step  1031\n",
      "cost:  0.26945795493013275\n",
      "opt took 0.00min, 1031iters\n",
      "Epoch: [15][0/24]Time: 0.384 (0.384) Data: 0.342 (0.342) Loss: 0.3909 (0.3909)\n",
      "error:  0.00994470853972329 step  901\n",
      "cost:  0.25024465503630167\n",
      "opt took 0.00min,  901iters\n",
      "error:  0.009089749583195417 step  901\n",
      "cost:  0.23409534823002914\n",
      "opt took 0.00min,  901iters\n",
      "error:  0.009416824633051268 step  1181\n",
      "cost:  0.23874771861688618\n",
      "opt took 0.00min, 1181iters\n",
      "error:  0.009879331065680064 step  1421\n",
      "cost:  0.233186701898576\n",
      "opt took 0.01min, 1421iters\n",
      "error:  0.00995257978941455 step  961\n",
      "cost:  0.22617369757671835\n",
      "opt took 0.00min,  961iters\n",
      "error:  0.00964898251885582 step  1221\n",
      "cost:  0.22589865418994937\n",
      "opt took 0.00min, 1221iters\n",
      "error:  0.00934873785790602 step  661\n",
      "cost:  0.22396634711269744\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.009455517507762856 step  691\n",
      "cost:  0.22455531655567665\n",
      "opt took 0.00min,  691iters\n",
      "Epoch: [15][10/24]Time: 0.442 (0.339) Data: 0.386 (0.295) Loss: 0.4739 (0.4046)\n",
      "error:  0.009251723999414874 step  741\n",
      "cost:  0.22347080058438457\n",
      "opt took 0.00min,  741iters\n",
      "error:  0.009873684532182181 step  711\n",
      "cost:  0.22749284344149495\n",
      "opt took 0.00min,  711iters\n",
      "error:  0.00999285356056423 step  971\n",
      "cost:  0.2271506986335288\n",
      "opt took 0.00min,  971iters\n",
      "error:  0.009501076407861908 step  851\n",
      "cost:  0.21877586711871813\n",
      "opt took 0.00min,  851iters\n",
      "error:  0.009798764263042381 step  731\n",
      "cost:  0.21523569857817426\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.009654787354289485 step  651\n",
      "cost:  0.21927079450920187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt took 0.00min,  651iters\n",
      "error:  0.009431747574353389 step  1001\n",
      "cost:  0.22415401771403531\n",
      "opt took 0.00min, 1001iters\n",
      "Epoch: [15][20/24]Time: 0.043 (0.292) Data: 0.001 (0.248) Loss: 0.3152 (0.3924)\n",
      "error:  0.009871797819748451 step  1181\n",
      "cost:  0.23861017922212527\n",
      "opt took 0.00min, 1181iters\n",
      "error:  0.009275611998946331 step  1021\n",
      "cost:  0.23568532639291068\n",
      "opt took 0.00min, 1021iters\n",
      "error:  0.009957514593748051 step  1391\n",
      "cost:  0.21808550948535707\n",
      "opt took 0.00min, 1391iters\n",
      "10-NN,s=0.1: TOP1:  27.208333333333332\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 16\n",
      "ResNet\n",
      "error:  0.009441940003744986 step  861\n",
      "cost:  0.25366854055033256\n",
      "opt took 0.00min,  861iters\n",
      "Epoch: [16][0/24]Time: 0.313 (0.313) Data: 0.278 (0.278) Loss: 0.3229 (0.3229)\n",
      "error:  0.009601628106278182 step  1001\n",
      "cost:  0.26195926959045684\n",
      "opt took 0.00min, 1001iters\n",
      "error:  0.009565621767391397 step  1101\n",
      "cost:  0.2633623717300334\n",
      "opt took 0.00min, 1101iters\n",
      "error:  0.009969641240469507 step  841\n",
      "cost:  0.2595692020639436\n",
      "opt took 0.00min,  841iters\n",
      "error:  0.008942584433038747 step  771\n",
      "cost:  0.2527225051053958\n",
      "opt took 0.00min,  771iters\n",
      "error:  0.0096640312814128 step  891\n",
      "cost:  0.24566983301826367\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.009552999927982198 step  721\n",
      "cost:  0.24635686513207736\n",
      "opt took 0.00min,  721iters\n",
      "error:  0.00972348783466137 step  661\n",
      "cost:  0.26668283940895265\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.009883331269369688 step  1321\n",
      "cost:  0.2686050752666273\n",
      "opt took 0.00min, 1321iters\n",
      "Epoch: [16][10/24]Time: 0.503 (0.345) Data: 0.437 (0.299) Loss: 0.3590 (0.3595)\n",
      "error:  0.009603799793655665 step  1091\n",
      "cost:  0.25571881115802886\n",
      "opt took 0.00min, 1091iters\n",
      "error:  0.009561109863279738 step  641\n",
      "cost:  0.22965747758261068\n",
      "opt took 0.00min,  641iters\n",
      "error:  0.009630247042116458 step  1261\n",
      "cost:  0.22298940884756085\n",
      "opt took 0.00min, 1261iters\n",
      "error:  0.00939710154234541 step  1031\n",
      "cost:  0.225800020482546\n",
      "opt took 0.00min, 1031iters\n",
      "error:  0.009507638883415437 step  1161\n",
      "cost:  0.2337638112763452\n",
      "opt took 0.01min, 1161iters\n",
      "error:  0.009768938335543376 step  1201\n",
      "cost:  0.24826178458236262\n",
      "opt took 0.00min, 1201iters\n",
      "error:  0.009848853335753471 step  1011\n",
      "cost:  0.23896344483803858\n",
      "opt took 0.00min, 1011iters\n",
      "error:  0.009831045820334672 step  1091\n",
      "cost:  0.24241212489559794\n",
      "opt took 0.00min, 1091iters\n",
      "Epoch: [16][20/24]Time: 0.369 (0.340) Data: 0.316 (0.296) Loss: 0.3649 (0.3681)\n",
      "error:  0.009918773265139902 step  1061\n",
      "cost:  0.24968845159507236\n",
      "opt took 0.01min, 1061iters\n",
      "error:  0.009291185261198653 step  861\n",
      "cost:  0.23388539128974076\n",
      "opt took 0.00min,  861iters\n",
      "10-NN,s=0.1: TOP1:  26.458333333333332\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 17\n",
      "ResNet\n",
      "error:  0.009675477104939656 step  961\n",
      "cost:  0.2619602913461681\n",
      "opt took 0.00min,  961iters\n",
      "Epoch: [17][0/24]Time: 0.292 (0.292) Data: 0.256 (0.256) Loss: 0.2863 (0.2863)\n",
      "error:  0.009604100757809264 step  1331\n",
      "cost:  0.2561885972028699\n",
      "opt took 0.00min, 1331iters\n",
      "error:  0.009430806301808925 step  1181\n",
      "cost:  0.24070246566986503\n",
      "opt took 0.00min, 1181iters\n",
      "error:  0.009448439701286615 step  1031\n",
      "cost:  0.23520007638846954\n",
      "opt took 0.00min, 1031iters\n",
      "error:  0.009968868646280216 step  1161\n",
      "cost:  0.23068354287805243\n",
      "opt took 0.00min, 1161iters\n",
      "error:  0.009561420444041513 step  881\n",
      "cost:  0.2279282970709326\n",
      "opt took 0.00min,  881iters\n",
      "error:  0.008778592660551698 step  601\n",
      "cost:  0.22165543610115476\n",
      "opt took 0.00min,  601iters\n",
      "error:  0.009508193320861391 step  541\n",
      "cost:  0.21773283185799994\n",
      "opt took 0.00min,  541iters\n",
      "error:  0.009170894338575986 step  751\n",
      "cost:  0.21475007383573774\n",
      "opt took 0.00min,  751iters\n",
      "Epoch: [17][10/24]Time: 0.274 (0.288) Data: 0.232 (0.245) Loss: 0.3320 (0.3489)\n",
      "error:  0.009985490451083656 step  1391\n",
      "cost:  0.21393079394246547\n",
      "opt took 0.00min, 1391iters\n",
      "error:  0.00929954024654489 step  881\n",
      "cost:  0.21732538108889168\n",
      "opt took 0.00min,  881iters\n",
      "error:  0.009726579216002618 step  981\n",
      "cost:  0.21951180462951222\n",
      "opt took 0.00min,  981iters\n",
      "error:  0.009718847820168519 step  1001\n",
      "cost:  0.22960454246779577\n",
      "opt took 0.00min, 1001iters\n",
      "error:  0.009351445769885336 step  941\n",
      "cost:  0.22608675337477604\n",
      "opt took 0.00min,  941iters\n",
      "error:  0.009682960286669595 step  991\n",
      "cost:  0.22418703164358106\n",
      "opt took 0.00min,  991iters\n",
      "error:  0.009413328795302878 step  801\n",
      "cost:  0.2249906918972422\n",
      "opt took 0.00min,  801iters\n",
      "error:  0.009658027012772297 step  881\n",
      "cost:  0.21715380203546109\n",
      "opt took 0.00min,  881iters\n",
      "Epoch: [17][20/24]Time: 0.362 (0.295) Data: 0.325 (0.254) Loss: 0.3777 (0.3507)\n",
      "error:  0.008784524841163877 step  651\n",
      "cost:  0.21368163318838396\n",
      "opt took 0.00min,  651iters\n",
      "error:  0.009147393293832518 step  791\n",
      "cost:  0.20068657966294304\n",
      "opt took 0.00min,  791iters\n",
      "10-NN,s=0.1: TOP1:  26.458333333333332\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 18\n",
      "ResNet\n",
      "error:  0.009786712654205942 step  1021\n",
      "cost:  0.19795303674050116\n",
      "opt took 0.00min, 1021iters\n",
      "Epoch: [18][0/24]Time: 0.388 (0.388) Data: 0.352 (0.352) Loss: 0.3295 (0.3295)\n",
      "error:  0.009918172297672112 step  1261\n",
      "cost:  0.20307379870817735\n",
      "opt took 0.00min, 1261iters\n",
      "error:  0.009750663529776715 step  1621\n",
      "cost:  0.21266131032133154\n",
      "opt took 0.01min, 1621iters\n",
      "error:  0.009861840977306247 step  891\n",
      "cost:  0.21826231351123812\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.00998058929055412 step  2541\n",
      "cost:  0.22563848950530713\n",
      "opt took 0.01min, 2541iters\n",
      "error:  0.009415105637695653 step  921\n",
      "cost:  0.22669510610642535\n",
      "opt took 0.00min,  921iters\n",
      "error:  0.009337323116900165 step  551\n",
      "cost:  0.21714734229096583\n",
      "opt took 0.00min,  551iters\n",
      "error:  0.009874367488397873 step  731\n",
      "cost:  0.2195137297040944\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.009997511438334605 step  951\n",
      "cost:  0.223953009813403\n",
      "opt took 0.00min,  951iters\n",
      "Epoch: [18][10/24]Time: 0.369 (0.327) Data: 0.327 (0.290) Loss: 0.3707 (0.3705)\n",
      "error:  0.009710722767382718 step  1491\n",
      "cost:  0.22244417084135007\n",
      "opt took 0.00min, 1491iters\n",
      "error:  0.00959514658840055 step  1431\n",
      "cost:  0.2231422017524993\n",
      "opt took 0.00min, 1431iters\n",
      "error:  0.009746189862146815 step  1041\n",
      "cost:  0.2265836484693861\n",
      "opt took 0.00min, 1041iters\n",
      "error:  0.009724997043230155 step  1521\n",
      "cost:  0.21411932111914425\n",
      "opt took 0.01min, 1521iters\n",
      "error:  0.009628956800391109 step  1201\n",
      "cost:  0.20310864908273502\n",
      "opt took 0.00min, 1201iters\n",
      "error:  0.009392088177155977 step  1111\n",
      "cost:  0.19649197760174478\n",
      "opt took 0.00min, 1111iters\n",
      "error:  0.0093455668428708 step  941\n",
      "cost:  0.19627890142554763\n",
      "opt took 0.00min,  941iters\n",
      "Epoch: [18][20/24]Time: 0.037 (0.319) Data: 0.001 (0.279) Loss: 0.4632 (0.3521)\n",
      "error:  0.009833404557886838 step  771\n",
      "cost:  0.20036381039228018\n",
      "opt took 0.00min,  771iters\n",
      "error:  0.009884311601519702 step  901\n",
      "cost:  0.20108511134369014\n",
      "opt took 0.00min,  901iters\n",
      "error:  0.009857476480415683 step  951\n",
      "cost:  0.19608690116926\n",
      "opt took 0.00min,  951iters\n",
      "10-NN,s=0.1: TOP1:  26.916666666666668\n",
      "best accuracy: 28.29\n",
      "\n",
      "Epoch: 19\n",
      "ResNet\n",
      "error:  0.009774017730298623 step  941\n",
      "cost:  0.2340251168010581\n",
      "opt took 0.00min,  941iters\n",
      "Epoch: [19][0/24]Time: 0.288 (0.288) Data: 0.251 (0.251) Loss: 0.3284 (0.3284)\n",
      "error:  0.00923946285420163 step  931\n",
      "cost:  0.22611334701181618\n",
      "opt took 0.00min,  931iters\n",
      "error:  0.00981375091220471 step  1001\n",
      "cost:  0.22470838947617297\n",
      "opt took 0.00min, 1001iters\n",
      "error:  0.00982388306284121 step  811\n",
      "cost:  0.2306354257890829\n",
      "opt took 0.00min,  811iters\n",
      "error:  0.009824188591155969 step  851\n",
      "cost:  0.2222454462919744\n",
      "opt took 0.00min,  851iters\n",
      "error:  0.009650543459814975 step  1401\n",
      "cost:  0.2237417362901488\n",
      "opt took 0.01min, 1401iters\n",
      "error:  0.00966907922021587 step  1221\n",
      "cost:  0.2209320787905222\n",
      "opt took 0.00min, 1221iters\n",
      "error:  0.009675311808760245 step  721\n",
      "cost:  0.22018139094960953\n",
      "opt took 0.00min,  721iters\n",
      "error:  0.009485758815361733 step  681\n",
      "cost:  0.221504523977686\n",
      "opt took 0.00min,  681iters\n",
      "Epoch: [19][10/24]Time: 0.365 (0.306) Data: 0.311 (0.265) Loss: 0.3203 (0.3068)\n",
      "error:  0.009623897674361115 step  1341\n",
      "cost:  0.22244441775980955\n",
      "opt took 0.00min, 1341iters\n",
      "error:  0.009163365222319197 step  731\n",
      "cost:  0.22325867773215186\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.009577734920951597 step  681\n",
      "cost:  0.22731259978286492\n",
      "opt took 0.00min,  681iters\n",
      "error:  0.009636215505340218 step  661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  0.22814843803416854\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.009115509989291892 step  661\n",
      "cost:  0.23150470373457716\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.009520844372905457 step  991\n",
      "cost:  0.21707059800582432\n",
      "opt took 0.00min,  991iters\n",
      "error:  0.00951731045907278 step  731\n",
      "cost:  0.21429273023350046\n",
      "opt took 0.00min,  731iters\n",
      "error:  0.009834049973409975 step  1131\n",
      "cost:  0.21113040589160803\n",
      "opt took 0.00min, 1131iters\n",
      "Epoch: [19][20/24]Time: 0.400 (0.302) Data: 0.358 (0.260) Loss: 0.3252 (0.3058)\n",
      "error:  0.009949058555042467 step  1391\n",
      "cost:  0.20946696039338\n",
      "opt took 0.00min, 1391iters\n",
      "error:  0.009673582228145294 step  1371\n",
      "cost:  0.20949084287009637\n",
      "opt took 0.01min, 1371iters\n",
      "10-NN,s=0.1: TOP1:  26.25\n",
      "best accuracy: 28.29\n",
      "doing PCA with 4 components ..done\n",
      "10-NN,s=0.1: TOP1:  27.583333333333332\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "best_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    \n",
    "    acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim)\n",
    "    accuracies.append(acc)\n",
    "    feature_return_switch(model, False)\n",
    "#     writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "        best_accuracies.append(best_acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = my_kNN(model, K=[50, 10], sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "#         for num_nn in [50, 10]:\n",
    "#             for sig in [0.1, 0.5]:\n",
    "#                 writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "#                 i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "end = time.time()\n",
    "\n",
    "# checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "# model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc(accuracies, model_name, dataset_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-labeling\n",
      "-------------------------\n",
      "Model:      ResNet\n",
      "Dataset:    LSST\n",
      "Epochs:     20\n",
      "lr:         0.003\n",
      "Batch size: 100\n",
      "Heads:      10\n",
      "Time:       134.2119288444519\n",
      "Best acc:   28.291666666666664 %\n"
     ]
    }
   ],
   "source": [
    "print (\"Self-labeling\")\n",
    "print (\"-------------------------\")\n",
    "print (\"Model:     \", model_name)\n",
    "print (\"Dataset:   \", dataset_name)\n",
    "print (\"Epochs:    \", epochs)\n",
    "print (\"lr:        \", lr)\n",
    "print (\"Batch size:\", batch_size)\n",
    "print (\"Heads:     \", hc)\n",
    "print (\"Time:      \", end-start)\n",
    "print (\"Best acc:  \", best_acc*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
