{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"VGG\"\n",
    "magic_dim = 2048\n",
    "\n",
    "# model_name = \"ResNet\"\n",
    "# magic_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"PenDigits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"/root/data/Multivariate_ts\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 20   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './resnet1d_exp' # experiments results dir\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 500\n",
    "lr=0.003     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 10\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sktime.utils.load_data import load_from_tsfile_to_dataframe\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_torch(X):\n",
    "    X = X.applymap(np.array)\n",
    "    dimensions_lst = []\n",
    "\n",
    "    for dim in X.columns:\n",
    "        dimensions_lst.append(np.dstack(list(X[dim].values))[0])\n",
    "\n",
    "    dimensions_lst = np.array(dimensions_lst)\n",
    "    X = torch.from_numpy(np.array(dimensions_lst, dtype=np.float64))\n",
    "    X = X.transpose(0, 2)\n",
    "    X = X.transpose(1, 2)\n",
    "    X = F.normalize(X, dim=1)\n",
    "    return X.float()\n",
    "\n",
    "def answers_to_torch(y):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    y = torch.from_numpy(np.array(y, dtype=np.int32))\n",
    "    y = y.long()\n",
    "    return y\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TRAIN.ts')\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(datadir + f'/{dataset_name}/{dataset_name}_TEST.ts')\n",
    "\n",
    "X_train = features_to_torch(X_train)\n",
    "X_test = features_to_torch(X_test)\n",
    "\n",
    "y_train = answers_to_torch(y_train)\n",
    "y_test = answers_to_torch(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_steps: 8\n",
      "train samples_num: 7494\n",
      "dims_num: 2\n",
      "num_classes: 10\n"
     ]
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "time_steps = X_train.shape[2]\n",
    "dims_num = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print('time_steps:', time_steps)\n",
    "print('train samples_num:', N)\n",
    "print('dims_num:', dims_num)\n",
    "print('num_classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters (AlexNet in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10                 # number of heads\n",
    "ncl=num_classes       # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# # (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "# CFG = {\n",
    "#     'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "#     'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['resnetv1','resnetv1_18']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, in_channel=3, width=1, num_classes=[1000]):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        self.base = int(16 * width)\n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(in_channel, 16, kernel_size=3, padding=1, bias=False), # [100, 16, 36]\n",
    "                            nn.BatchNorm1d(16),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            self._make_layer(block, self.base, layers[0]),                   # [100, 16, 36]\n",
    "                            self._make_layer(block, self.base * 2, layers[1]),               # [100, 32, 36]\n",
    "                            self._make_layer(block, self.base * 4, layers[2]),               # [100, 64, 36]\n",
    "                            self._make_layer(block, self.base * 8, layers[3]),               # [100, 128, 36]\n",
    "                            nn.AvgPool1d(2),                                                 # [100, 128, 18]\n",
    "        ])\n",
    "    \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnetv1_18(num_classes=[1000]):\n",
    "    \"\"\"Encoder for instance discrimination and MoCo\"\"\"\n",
    "    return resnet18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        \n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(dims_num, 64, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "#                             nn.MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.AvgPool1d(kernel_size=2)#, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "        ])\n",
    "        \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())  # [50, 10, 400] -> [50, 512, 12]\n",
    "        out = out.view(out.size(0), -1) # [50, magic_dim]\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "                print (out.size())\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "        \n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((N, ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((N, magic_dim)) # knn_dim\n",
    "    \n",
    "    for batch_idx, (data, _, _selected) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data.float())\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy() # p: [20, magic_dim]\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(acc, model_name, dataset_name):\n",
    "    step = 3\n",
    "    x = np.arange(1, epochs//step + 1)\n",
    "    acc = acc[::step]\n",
    "    plt.plot(x*step, acc[1:])\n",
    "    plt.xlabel(\"# epoch\")\n",
    "    plt.ylabel(\"Accuracy, %\")\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Train accuracy, self-labeling, {model_name}, {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(model_name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N // batch_size + batch_idx\n",
    "        if niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h], selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "#         if True:\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N // batch_size, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "#             writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*N/batch_size)\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG created\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"ResNet\":\n",
    "    model = resnet18(num_classes=numc, in_channel=dims_num)\n",
    "else:\n",
    "    model = VGG(num_classes=numc)\n",
    "print (model_name, \"created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [30.0, 21.0, 20.95, 20.89, 20.84, 20.79, 20.74, 20.68, 20.63, 20.58, 20.53, 20.47, 20.42, 20.37, 20.32, 20.26, 20.21, 20.16, 20.11, 20.05, 20.0, 19.95, 19.89, 19.84, 19.79, 19.74, 19.68, 19.63, 19.58, 19.53, 19.47, 19.42, 19.37, 19.32, 19.26, 19.21, 19.16, 19.11, 19.05, 19.0, 18.95, 18.89, 18.84, 18.79, 18.74, 18.68, 18.63, 18.58, 18.53, 18.47, 18.42, 18.37, 18.32, 18.26, 18.21, 18.16, 18.11, 18.05, 18.0, 17.95, 17.89, 17.84, 17.79, 17.74, 17.68, 17.63, 17.58, 17.53, 17.47, 17.42, 17.37, 17.32, 17.26, 17.21, 17.16, 17.11, 17.05, 17.0, 16.95, 16.89, 16.84, 16.79, 16.74, 16.68, 16.63, 16.58, 16.53, 16.47, 16.42, 16.37, 16.32, 16.26, 16.21, 16.16, 16.11, 16.05, 16.0, 15.95, 15.89, 15.84, 15.79, 15.74, 15.68, 15.63, 15.58, 15.53, 15.47, 15.42, 15.37, 15.32, 15.26, 15.21, 15.16, 15.11, 15.05, 15.0, 14.95, 14.89, 14.84, 14.79, 14.74, 14.68, 14.63, 14.58, 14.53, 14.47, 14.42, 14.37, 14.32, 14.26, 14.21, 14.16, 14.11, 14.05, 14.0, 13.95, 13.89, 13.84, 13.79, 13.74, 13.68, 13.63, 13.58, 13.53, 13.47, 13.42, 13.37, 13.32, 13.26, 13.21, 13.16, 13.11, 13.05, 13.0, 12.95, 12.89, 12.84, 12.79, 12.74, 12.68, 12.63, 12.58, 12.53, 12.47, 12.42, 12.37, 12.32, 12.26, 12.21, 12.16, 12.11, 12.05, 12.0, 11.95, 11.89, 11.84, 11.79, 11.74, 11.68, 11.63, 11.58, 11.53, 11.47, 11.42, 11.37, 11.32, 11.26, 11.21, 11.16, 11.11, 11.05, 11.0, 10.95, 10.89, 10.84, 10.79, 10.74, 10.68, 10.63, 10.58, 10.53, 10.47, 10.42, 10.37, 10.32, 10.26, 10.21, 10.16, 10.11, 10.05, 10.0, 9.95, 9.89, 9.84, 9.79, 9.74, 9.68, 9.63, 9.58, 9.53, 9.47, 9.42, 9.37, 9.32, 9.26, 9.21, 9.16, 9.11, 9.05, 9.0, 8.95, 8.89, 8.84, 8.79, 8.74, 8.68, 8.63, 8.58, 8.53, 8.47, 8.42, 8.37, 8.32, 8.26, 8.21, 8.16, 8.11, 8.05, 8.0, 7.95, 7.89, 7.84, 7.79, 7.74, 7.68, 7.63, 7.58, 7.53, 7.47, 7.42, 7.37, 7.32, 7.26, 7.21, 7.16, 7.11, 7.05, 7.0, 6.95, 6.89, 6.84, 6.79, 6.74, 6.68, 6.63, 6.58, 6.53, 6.47, 6.42, 6.37, 6.32, 6.26, 6.21, 6.16, 6.11, 6.05, 6.0, 5.95, 5.89, 5.84, 5.79, 5.74, 5.68, 5.63, 5.58, 5.53, 5.47, 5.42, 5.37, 5.32, 5.26, 5.21, 5.16, 5.11, 5.05, 5.0, 4.95, 4.89, 4.84, 4.79, 4.74, 4.68, 4.63, 4.58, 4.53, 4.47, 4.42, 4.37, 4.32, 4.26, 4.21, 4.16, 4.11, 4.05, 4.0, 3.95, 3.89, 3.84, 3.79, 3.74, 3.68, 3.63, 3.58, 3.53, 3.47, 3.42, 3.37, 3.32, 3.26, 3.21, 3.16, 3.11, 3.05, 3.0, 2.95, 2.89, 2.84, 2.79, 2.74, 2.68, 2.63, 2.58, 2.53, 2.47, 2.42, 2.37, 2.32, 2.26, 2.21, 2.16, 2.11, 2.05, 2.0, 1.95, 1.89, 1.84, 1.79, 1.74, 1.68, 1.63, 1.58, 1.53, 1.47, 1.42, 1.37, 1.32, 1.26, 1.21, 1.16, 1.11, 1.05, 1.0, 0.95, 0.89, 0.84, 0.79, 0.74, 0.68, 0.63, 0.58, 0.53, 0.47, 0.42, 0.37, 0.32, 0.26, 0.21, 0.16, 0.11, 0.05, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'./runs/{dataset_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(net, K, sigma=0.1, dim=128, use_pca=False):\n",
    "    net.eval()\n",
    "    # this part is ugly but made to be backwards-compatible. there was a change in cifar dataset's structure.\n",
    "    trainLabels = y_train\n",
    "    LEN = N\n",
    "    C = trainLabels.max() + 1\n",
    "\n",
    "    trainFeatures = torch.zeros((magic_dim, LEN))  # , device='cuda:0') # dim\n",
    "    normalize = Normalize()\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=False)):\n",
    "        batchSize = batch_size\n",
    "        inputs = inputs.cuda()\n",
    "        features = net(inputs.float())\n",
    "        if not use_pca:\n",
    "            features = normalize(features)\n",
    "        trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu()\n",
    "        \n",
    "    if use_pca:\n",
    "        comps = 4\n",
    "        print('doing PCA with %s components'%comps, end=' ')\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=comps, whiten=False)\n",
    "        trainFeatures = pca.fit_transform(trainFeatures.numpy().T)\n",
    "        trainFeatures = torch.Tensor(trainFeatures)\n",
    "        trainFeatures = normalize(trainFeatures).t()\n",
    "        print('..done')\n",
    "    def eval_k_s(K_,sigma_):\n",
    "        total = 0\n",
    "        top1 = 0.\n",
    "        top5 = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            retrieval_one_hot = torch.zeros(K_, C)# .cuda()\n",
    "            for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=False)):\n",
    "                targets = targets # .cuda(async=True) # or without async for py3.7\n",
    "                inputs = inputs.cuda()\n",
    "                batchSize = batch_size\n",
    "                features = net(inputs)\n",
    "                if use_pca:\n",
    "                    features = pca.transform(features.cpu().numpy())\n",
    "                    features = torch.Tensor(features).cuda()\n",
    "                features = normalize(features).cpu()\n",
    "\n",
    "                dist = torch.mm(features, trainFeatures)\n",
    "\n",
    "                yd, yi = dist.topk(K_, dim=1, largest=True, sorted=True)\n",
    "                candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "                retrieval = torch.gather(candidates, 1, yi).long()\n",
    "\n",
    "                retrieval_one_hot.resize_(batchSize * K_, C).zero_()\n",
    "                retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1.)\n",
    "                \n",
    "                yd_transform = yd.clone().div_(sigma_).exp_()\n",
    "                probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C),\n",
    "                                            yd_transform.view(batchSize, -1, 1)),\n",
    "                                  1)\n",
    "                _, predictions = probs.sort(1, True)\n",
    "\n",
    "                # Find which predictions match the target\n",
    "                correct = predictions.eq(targets.data.view(-1, 1))\n",
    "\n",
    "                top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "                top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        print(f\"{K_}-NN,s={sigma_}: TOP1: \", top1 * 100. / total)\n",
    "        return top1 / total\n",
    "\n",
    "    if isinstance(K, list):\n",
    "        res = []\n",
    "        for K_ in K:\n",
    "            for sigma_ in sigma:\n",
    "                res.append(eval_k_s(K_, sigma_))\n",
    "        return res\n",
    "    else:\n",
    "        res = eval_k_s(K, sigma)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "VGG\n",
      "error:  0.0 step  11\n",
      "cost:  2.3019222209142476\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [0][0/14]Time: 0.404 (0.404) Data: 0.340 (0.340) Loss: 2.3029 (2.3029)\n",
      "error:  1.912619188981779e-05 step  11\n",
      "cost:  2.074192596256805\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.0983215165415459e-13 step  11\n",
      "cost:  2.1862795434837095\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.9917401061775308e-13 step  11\n",
      "cost:  2.153244077276453\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.312106227644108e-13 step  11\n",
      "cost:  2.06806794128232\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.5138446052276322e-11 step  11\n",
      "cost:  1.9194275939533214\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.4250957636186001e-08 step  11\n",
      "cost:  1.781789075025515\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0006241261671526832 step  11\n",
      "cost:  1.8438475782788155\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.618987685387708e-06 step  11\n",
      "cost:  1.8210357383528541\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.3445392573752102e-06 step  11\n",
      "cost:  1.786900324503951\n",
      "opt took 0.00min,   11iters\n",
      "error:  3.6900291158215026e-06 step  11\n",
      "cost:  2.24885875023575\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [0][10/14]Time: 0.345 (0.383) Data: 0.297 (0.335) Loss: 2.3053 (2.2992)\n",
      "error:  7.989798445295904e-06 step  11\n",
      "cost:  1.8213385104588815\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.6651702564929316e-05 step  11\n",
      "cost:  1.7963903156568284\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.570455152430597e-05 step  11\n",
      "cost:  1.7632101866701113\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  31.4\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 4 components ..done\n",
      "50-NN,s=0.1: TOP1:  60.766666666666666\n",
      "50-NN,s=0.5: TOP1:  60.9\n",
      "10-NN,s=0.1: TOP1:  63.06666666666667\n",
      "10-NN,s=0.5: TOP1:  63.13333333333333\n",
      "best accuracy: 31.40\n",
      "\n",
      "Epoch: 1\n",
      "VGG\n",
      "error:  6.896705428971472e-13 step  11\n",
      "cost:  2.199787955756839\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [1][0/14]Time: 0.297 (0.297) Data: 0.257 (0.257) Loss: 2.2400 (2.2400)\n",
      "error:  3.8413716652030416e-13 step  11\n",
      "cost:  2.1047404186963585\n",
      "opt took 0.00min,   11iters\n",
      "error:  9.696687897076117e-13 step  11\n",
      "cost:  1.9525806861182924\n",
      "opt took 0.00min,   11iters\n",
      "error:  8.412159857584811e-13 step  11\n",
      "cost:  1.7329381893960927\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.2406742300186124e-12 step  11\n",
      "cost:  1.4266347075185595\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.6759705562540148e-11 step  11\n",
      "cost:  1.3995907898391888\n",
      "opt took 0.00min,   11iters\n",
      "error:  3.012157301363061e-09 step  11\n",
      "cost:  1.633954174452813\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.6915408518336505e-08 step  11\n",
      "cost:  1.6353952225060944\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.404274901390835e-08 step  11\n",
      "cost:  1.567089027940296\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.7453891382478446e-08 step  11\n",
      "cost:  1.4397198087333867\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.909901096805669e-08 step  11\n",
      "cost:  1.4057386180839904\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [1][10/14]Time: 0.316 (0.375) Data: 0.289 (0.327) Loss: 2.1448 (2.1863)\n",
      "error:  1.2166267082669435e-07 step  11\n",
      "cost:  1.2697956578226577\n",
      "opt took 0.00min,   11iters\n",
      "error:  5.461890837388239e-07 step  11\n",
      "cost:  1.159211381962611\n",
      "opt took 0.00min,   11iters\n",
      "error:  3.673579153984363e-06 step  11\n",
      "cost:  1.0803384928406854\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  15.4\n",
      "best accuracy: 31.40\n",
      "\n",
      "Epoch: 2\n",
      "VGG\n",
      "error:  1.1069334338031922e-10 step  11\n",
      "cost:  2.1713567160167866\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [2][0/14]Time: 0.362 (0.362) Data: 0.332 (0.332) Loss: 2.1022 (2.1022)\n",
      "error:  5.762672561360205e-10 step  11\n",
      "cost:  2.030898063603416\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.9200449835897757e-09 step  11\n",
      "cost:  1.804732363981755\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.8284911624563165e-09 step  11\n",
      "cost:  1.5103399618147433\n",
      "opt took 0.00min,   11iters\n",
      "error:  6.6366829809538785e-09 step  11\n",
      "cost:  1.2153060717664677\n",
      "opt took 0.00min,   11iters\n",
      "error:  3.1223969187976763e-08 step  11\n",
      "cost:  1.3242256923530142\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.0378831653001797e-06 step  11\n",
      "cost:  1.7029730825294247\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.7253877005685148e-05 step  11\n",
      "cost:  1.8517309011521834\n",
      "opt took 0.00min,   11iters\n",
      "error:  8.308611871732374e-06 step  11\n",
      "cost:  2.116726789665349\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.0478804375324557e-06 step  11\n",
      "cost:  1.9830763341051243\n",
      "opt took 0.00min,   11iters\n",
      "error:  5.8885797016294816e-08 step  11\n",
      "cost:  1.8138306517523264\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [2][10/14]Time: 0.438 (0.448) Data: 0.385 (0.396) Loss: 2.0425 (2.0541)\n",
      "error:  6.103866079243403e-09 step  11\n",
      "cost:  1.6502397516318708\n",
      "opt took 0.00min,   11iters\n",
      "error:  5.464058872028943e-09 step  11\n",
      "cost:  1.6065493268258355\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.4896133215813734e-08 step  11\n",
      "cost:  1.501334353260032\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  19.2\n",
      "best accuracy: 31.40\n",
      "\n",
      "Epoch: 3\n",
      "VGG\n",
      "error:  4.117475249643121e-10 step  11\n",
      "cost:  2.1654844583101513\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [3][0/14]Time: 0.341 (0.341) Data: 0.304 (0.304) Loss: 2.0031 (2.0031)\n",
      "error:  1.3420489164417404e-09 step  11\n",
      "cost:  2.035946511801334\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.8576833404893023e-09 step  11\n",
      "cost:  1.7972496385501562\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.1258428633809103e-09 step  11\n",
      "cost:  1.4327343444092548\n",
      "opt took 0.00min,   11iters\n",
      "error:  3.772037204807788e-08 step  11\n",
      "cost:  1.30668780649892\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.2345286286685742e-05 step  11\n",
      "cost:  1.5357449730995478\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.00012265334815719786 step  11\n",
      "cost:  1.6711748028729247\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.3977105984895886e-05 step  11\n",
      "cost:  1.5183519190306347\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.9301158296070042e-06 step  11\n",
      "cost:  1.3771115070384545\n",
      "opt took 0.00min,   11iters\n",
      "error:  2.516469935764931e-07 step  11\n",
      "cost:  1.4277209420969479\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.329064667210389e-07 step  11\n",
      "cost:  1.6457954663722096\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [3][10/14]Time: 0.423 (0.381) Data: 0.390 (0.338) Loss: 1.9015 (1.9121)\n",
      "error:  7.567367030780758e-07 step  11\n",
      "cost:  2.0246743817436537\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.1327325417395144e-07 step  11\n",
      "cost:  2.1338449849859376\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.9487127778106128e-07 step  11\n",
      "cost:  2.0197208453700672\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  67.0\n",
      "Saving..\n",
      "best accuracy: 67.00\n",
      "\n",
      "Epoch: 4\n",
      "VGG\n",
      "error:  1.1523559884096812e-11 step  11\n",
      "cost:  2.161713383325861\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [4][0/14]Time: 0.364 (0.364) Data: 0.317 (0.317) Loss: 1.9407 (1.9407)\n",
      "error:  1.2938239368764926e-09 step  11\n",
      "cost:  2.055438659061151\n",
      "opt took 0.00min,   11iters\n",
      "error:  5.7674036657573424e-09 step  11\n",
      "cost:  1.9337500733735613\n",
      "opt took 0.00min,   11iters\n",
      "error:  8.312522559172919e-09 step  11\n",
      "cost:  1.8692526539212764\n",
      "opt took 0.00min,   11iters\n",
      "error:  5.158119997794586e-08 step  11\n",
      "cost:  1.7485388203402283\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.3801063893925658e-06 step  11\n",
      "cost:  1.5827324514368584\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.7152512408746823e-05 step  11\n",
      "cost:  1.72382218494719\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.4526369631481266e-05 step  11\n",
      "cost:  1.8146466237640329\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0001417891154201767 step  11\n",
      "cost:  1.7788579339831088\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.00012473403524404514 step  11\n",
      "cost:  1.4305585321774081\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.00031039974877833387 step  11\n",
      "cost:  1.9690570757903096\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [4][10/14]Time: 0.350 (0.386) Data: 0.314 (0.347) Loss: 1.8103 (1.8468)\n",
      "error:  0.0006376092022833957 step  11\n",
      "cost:  1.7694165114215112\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0009709515081871567 step  11\n",
      "cost:  1.7184861391931463\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0011222131372788935 step  11\n",
      "cost:  1.838038576235972\n",
      "opt took 0.00min,   11iters\n",
      "10-NN,s=0.1: TOP1:  68.63333333333334\n",
      "Saving..\n",
      "best accuracy: 68.63\n",
      "\n",
      "Epoch: 5\n",
      "VGG\n",
      "error:  3.375300039465401e-12 step  11\n",
      "cost:  2.1716022861375026\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [5][0/14]Time: 0.476 (0.476) Data: 0.437 (0.437) Loss: 1.7892 (1.7892)\n",
      "error:  4.480160686881618e-11 step  11\n",
      "cost:  2.0828920230386547\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.1456882371448955e-10 step  11\n",
      "cost:  2.047490562698254\n",
      "opt took 0.00min,   11iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  4.273798981380139e-09 step  11\n",
      "cost:  2.0455521794008504\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.5574704859133703e-09 step  11\n",
      "cost:  1.9371008550506903\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.6243658640391345e-09 step  11\n",
      "cost:  1.893526176201787\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.8438059412595464e-08 step  11\n",
      "cost:  1.966612029981318\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.1142302507316515e-06 step  11\n",
      "cost:  1.9876427350906976\n",
      "opt took 0.00min,   11iters\n",
      "error:  4.247671806467679e-05 step  11\n",
      "cost:  2.07734578591362\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0015350459319442011 step  11\n",
      "cost:  2.2175420637058143\n",
      "opt took 0.00min,   11iters\n",
      "error:  3.197880849015178e-10 step  21\n",
      "cost:  2.36012669735202\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [5][10/14]Time: 0.331 (0.375) Data: 0.294 (0.335) Loss: 1.7506 (1.7523)\n",
      "error:  3.8955824732678224e-07 step  21\n",
      "cost:  2.3709351859108168\n",
      "opt took 0.00min,   21iters\n",
      "error:  1.6727959849305307e-05 step  21\n",
      "cost:  2.443151187049062\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.0032066363372228457 step  21\n",
      "cost:  2.5291554403379197\n",
      "opt took 0.00min,   21iters\n",
      "10-NN,s=0.1: TOP1:  66.26666666666667\n",
      "best accuracy: 68.63\n",
      "\n",
      "Epoch: 6\n",
      "VGG\n",
      "error:  1.7607026947530358e-12 step  11\n",
      "cost:  2.291559705124434\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [6][0/14]Time: 0.353 (0.353) Data: 0.325 (0.325) Loss: 1.7711 (1.7711)\n",
      "error:  1.3743228777229888e-11 step  11\n",
      "cost:  2.243169762009569\n",
      "opt took 0.00min,   11iters\n",
      "error:  3.120490754682237e-09 step  11\n",
      "cost:  2.2007364134034346\n",
      "opt took 0.00min,   11iters\n",
      "error:  9.135077574273787e-06 step  11\n",
      "cost:  2.1370615110904376\n",
      "opt took 0.00min,   11iters\n",
      "error:  0.0027783814037117827 step  11\n",
      "cost:  2.0726483936113937\n",
      "opt took 0.00min,   11iters\n",
      "error:  1.0897565702716605e-05 step  21\n",
      "cost:  2.0151474387665496\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.0008193675497204511 step  21\n",
      "cost:  1.9929506956305991\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.009484557215694855 step  21\n",
      "cost:  1.9252460544170462\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.008882344241144202 step  81\n",
      "cost:  2.021185102649749\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.001955682262941516 step  51\n",
      "cost:  2.1139941591840232\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.005243008688052719 step  131\n",
      "cost:  2.260168952549244\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [6][10/14]Time: 0.548 (0.378) Data: 0.502 (0.331) Loss: 1.6461 (1.6852)\n",
      "error:  0.0034278383939766455 step  81\n",
      "cost:  2.271482918196231\n",
      "opt took 0.00min,   81iters\n",
      "error:  0.002811880373267517 step  71\n",
      "cost:  2.0993025763762097\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.007903573070998315 step  91\n",
      "cost:  1.6910299790123893\n",
      "opt took 0.00min,   91iters\n",
      "10-NN,s=0.1: TOP1:  73.76666666666667\n",
      "Saving..\n",
      "best accuracy: 73.77\n",
      "\n",
      "Epoch: 7\n",
      "VGG\n",
      "error:  0.0064135964474654195 step  11\n",
      "cost:  2.2380651605606428\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [7][0/14]Time: 0.550 (0.550) Data: 0.515 (0.515) Loss: 1.5084 (1.5084)\n",
      "error:  0.0005338497629764349 step  21\n",
      "cost:  2.1845677181740615\n",
      "opt took 0.00min,   21iters\n",
      "error:  0.0009750103544384769 step  31\n",
      "cost:  2.073015559509877\n",
      "opt took 0.00min,   31iters\n",
      "error:  0.002724087840064171 step  51\n",
      "cost:  1.8933256482115925\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.006480467587324568 step  91\n",
      "cost:  1.7542929994303962\n",
      "opt took 0.00min,   91iters\n",
      "error:  0.0043629817597095055 step  111\n",
      "cost:  1.6558989251848815\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.004765778207385329 step  111\n",
      "cost:  1.5992412164654322\n",
      "opt took 0.00min,  111iters\n",
      "error:  0.0060509215815787165 step  121\n",
      "cost:  1.6480767449957239\n",
      "opt took 0.00min,  121iters\n",
      "error:  0.004739849126315088 step  151\n",
      "cost:  1.6524992003843328\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.004583520422371423 step  161\n",
      "cost:  1.6600523228067257\n",
      "opt took 0.00min,  161iters\n",
      "error:  0.006091691433964486 step  221\n",
      "cost:  1.5758932883925318\n",
      "opt took 0.00min,  221iters\n",
      "Epoch: [7][10/14]Time: 0.503 (0.417) Data: 0.465 (0.380) Loss: 1.2812 (1.3661)\n",
      "error:  0.009682649423857992 step  291\n",
      "cost:  1.4530981024377607\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.007213087786914096 step  261\n",
      "cost:  1.48012661337858\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.008717770503283151 step  681\n",
      "cost:  1.278028115987906\n",
      "opt took 0.00min,  681iters\n",
      "10-NN,s=0.1: TOP1:  76.4\n",
      "Saving..\n",
      "best accuracy: 76.40\n",
      "\n",
      "Epoch: 8\n",
      "VGG\n",
      "error:  0.0061844176958604224 step  21\n",
      "cost:  2.100724514206313\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [8][0/14]Time: 0.460 (0.460) Data: 0.374 (0.374) Loss: 1.3228 (1.3228)\n",
      "error:  0.008154871032135524 step  41\n",
      "cost:  1.9463336669120053\n",
      "opt took 0.00min,   41iters\n",
      "error:  0.007087698662746944 step  71\n",
      "cost:  1.7741297374518772\n",
      "opt took 0.00min,   71iters\n",
      "error:  0.004609855507249572 step  101\n",
      "cost:  1.6614110403058175\n",
      "opt took 0.00min,  101iters\n",
      "error:  0.009054053393678041 step  131\n",
      "cost:  1.5857854830282962\n",
      "opt took 0.00min,  131iters\n",
      "error:  0.005596938720830447 step  181\n",
      "cost:  1.540957323734324\n",
      "opt took 0.00min,  181iters\n",
      "error:  0.009430304125282452 step  211\n",
      "cost:  1.482549176993303\n",
      "opt took 0.00min,  211iters\n",
      "error:  0.00797146197420906 step  291\n",
      "cost:  1.4601017566091095\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.006702446765721071 step  291\n",
      "cost:  1.320765712057354\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.009106716016466287 step  301\n",
      "cost:  1.2991366872080798\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.009504936136050635 step  331\n",
      "cost:  1.2137328825576197\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [8][10/14]Time: 0.470 (0.466) Data: 0.439 (0.418) Loss: 1.0631 (1.1584)\n",
      "error:  0.00764390161229056 step  371\n",
      "cost:  1.175342542365589\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.008271632503972781 step  371\n",
      "cost:  1.0835990695111193\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.008996275492753059 step  261\n",
      "cost:  1.0775600780100545\n",
      "opt took 0.00min,  261iters\n",
      "10-NN,s=0.1: TOP1:  74.73333333333333\n",
      "best accuracy: 76.40\n",
      "\n",
      "Epoch: 9\n",
      "VGG\n",
      "error:  0.00016540032911505076 step  31\n",
      "cost:  2.0377471286991624\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [9][0/14]Time: 0.453 (0.453) Data: 0.406 (0.406) Loss: 0.9874 (0.9874)\n",
      "error:  0.0012969746944627714 step  51\n",
      "cost:  1.8736053407038968\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.004494793247074802 step  51\n",
      "cost:  1.7414493329885357\n",
      "opt took 0.00min,   51iters\n",
      "error:  0.005134505447838222 step  151\n",
      "cost:  1.6368273428732618\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.006945010600485801 step  231\n",
      "cost:  1.5553872682020073\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.0065785664528918986 step  221\n",
      "cost:  1.4289543430544844\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.009262439604057704 step  291\n",
      "cost:  1.2561777118593933\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.009496578280866252 step  291\n",
      "cost:  1.1430103476779954\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.008897414811368076 step  241\n",
      "cost:  1.066133050099269\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.007986054486245342 step  321\n",
      "cost:  0.9497163850163932\n",
      "opt took 0.00min,  321iters\n",
      "error:  0.006119681468405247 step  211\n",
      "cost:  0.9004780960880823\n",
      "opt took 0.00min,  211iters\n",
      "Epoch: [9][10/14]Time: 0.476 (0.456) Data: 0.391 (0.410) Loss: 0.8052 (0.8880)\n",
      "error:  0.006663784815502116 step  221\n",
      "cost:  0.8629586347446483\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.007610768355147957 step  371\n",
      "cost:  0.8280954254411873\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.008474321363017068 step  311\n",
      "cost:  0.7631767749656639\n",
      "opt took 0.00min,  311iters\n",
      "10-NN,s=0.1: TOP1:  75.23333333333333\n",
      "best accuracy: 76.40\n",
      "\n",
      "Epoch: 10\n",
      "VGG\n",
      "error:  0.008082788880439229 step  441\n",
      "cost:  5.9409748778552895\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [10][0/14]Time: 0.376 (0.376) Data: 0.349 (0.349) Loss: 1.3708 (1.3708)\n",
      "error:  0.009600426590124611 step  411\n",
      "cost:  5.2302728401136065\n",
      "opt took 0.00min,  411iters\n",
      "error:  0.009839946121039889 step  501\n",
      "cost:  4.3498225367333285\n",
      "opt took 0.00min,  501iters\n",
      "error:  0.007137941886795063 step  341\n",
      "cost:  3.5929196904344183\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009587175041421703 step  291\n",
      "cost:  3.0746995965180948\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.008956502958291201 step  461\n",
      "cost:  2.6862742534821535\n",
      "opt took 0.00min,  461iters\n",
      "error:  0.009757378809716055 step  421\n",
      "cost:  2.3642121737156034\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.008063090374372583 step  451\n",
      "cost:  2.102856530336323\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.009427362584438215 step  451\n",
      "cost:  1.8909463274274054\n",
      "opt took 0.00min,  451iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.008488391389228744 step  451\n",
      "cost:  1.7321518134661515\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.008171016356879779 step  471\n",
      "cost:  1.5425719304673287\n",
      "opt took 0.00min,  471iters\n",
      "Epoch: [10][10/14]Time: 0.300 (0.339) Data: 0.273 (0.311) Loss: 0.8812 (1.0690)\n",
      "error:  0.008294715768494387 step  491\n",
      "cost:  1.3403287196483968\n",
      "opt took 0.00min,  491iters\n",
      "error:  0.009270178948419638 step  421\n",
      "cost:  1.2624857009873178\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.009048143340714687 step  401\n",
      "cost:  1.2089849917292887\n",
      "opt took 0.00min,  401iters\n",
      "10-NN,s=0.1: TOP1:  75.53333333333333\n",
      "best accuracy: 76.40\n",
      "\n",
      "Epoch: 11\n",
      "VGG\n",
      "error:  0.008495817188729049 step  391\n",
      "cost:  6.149134205777175\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [11][0/14]Time: 0.282 (0.282) Data: 0.254 (0.254) Loss: 1.3857 (1.3857)\n",
      "error:  0.00844458597753639 step  351\n",
      "cost:  5.351823610686919\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.009737215014556266 step  301\n",
      "cost:  4.273384402195403\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.008034128589439193 step  241\n",
      "cost:  3.521078292404182\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.00825738941491938 step  241\n",
      "cost:  3.123490675115217\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.008054821708509796 step  261\n",
      "cost:  2.8890169457267434\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.007544183593995668 step  261\n",
      "cost:  2.611859917562784\n",
      "opt took 0.00min,  261iters\n",
      "error:  0.006635938019396215 step  171\n",
      "cost:  2.25512407567437\n",
      "opt took 0.00min,  171iters\n",
      "error:  0.006787524191117389 step  301\n",
      "cost:  1.8151200996889136\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.00816358653102467 step  331\n",
      "cost:  1.595878205269304\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.00845331517480663 step  371\n",
      "cost:  1.5320017219373356\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [11][10/14]Time: 0.276 (0.296) Data: 0.250 (0.268) Loss: 0.8836 (1.1115)\n",
      "error:  0.008865294637765198 step  411\n",
      "cost:  1.4392550088248894\n",
      "opt took 0.00min,  411iters\n",
      "error:  0.007815750823252254 step  431\n",
      "cost:  1.3410003969283786\n",
      "opt took 0.00min,  431iters\n",
      "error:  0.008662943854437555 step  461\n",
      "cost:  1.2709171450913868\n",
      "opt took 0.00min,  461iters\n",
      "10-NN,s=0.1: TOP1:  75.2\n",
      "best accuracy: 76.40\n",
      "\n",
      "Epoch: 12\n",
      "VGG\n",
      "error:  0.008437280854066809 step  431\n",
      "cost:  5.600057912998083\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [12][0/14]Time: 0.285 (0.285) Data: 0.259 (0.259) Loss: 1.3477 (1.3477)\n",
      "error:  0.008090194609663426 step  361\n",
      "cost:  4.569571981486408\n",
      "opt took 0.00min,  361iters\n",
      "error:  0.00852175994993587 step  221\n",
      "cost:  3.5513578954547085\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.007880560795693436 step  171\n",
      "cost:  2.969209274678987\n",
      "opt took 0.00min,  171iters\n",
      "error:  0.005520209487712036 step  151\n",
      "cost:  2.6977844658678993\n",
      "opt took 0.00min,  151iters\n",
      "error:  0.009948169685960795 step  191\n",
      "cost:  2.526697661181802\n",
      "opt took 0.00min,  191iters\n",
      "error:  0.008403363072116865 step  231\n",
      "cost:  2.371184233772306\n",
      "opt took 0.00min,  231iters\n",
      "error:  0.008085501013214813 step  251\n",
      "cost:  2.1528012737819635\n",
      "opt took 0.00min,  251iters\n",
      "error:  0.007506872021403765 step  331\n",
      "cost:  1.8753525402116302\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.007538223825795165 step  301\n",
      "cost:  1.6038886276766067\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.007582376269480329 step  351\n",
      "cost:  1.405863338881993\n",
      "opt took 0.00min,  351iters\n",
      "Epoch: [12][10/14]Time: 0.275 (0.300) Data: 0.249 (0.273) Loss: 0.8639 (1.0674)\n",
      "error:  0.009503478852233238 step  381\n",
      "cost:  1.3391318725302852\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.009636785049555452 step  381\n",
      "cost:  1.2023479516520819\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.008451247945974294 step  351\n",
      "cost:  1.175509094906703\n",
      "opt took 0.00min,  351iters\n",
      "10-NN,s=0.1: TOP1:  76.23333333333333\n",
      "best accuracy: 76.40\n",
      "\n",
      "Epoch: 13\n",
      "VGG\n",
      "error:  0.007839305704966049 step  391\n",
      "cost:  5.130822121848428\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [13][0/14]Time: 0.281 (0.281) Data: 0.254 (0.254) Loss: 1.2992 (1.2992)\n",
      "error:  0.008179386832172009 step  351\n",
      "cost:  4.473135933051659\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.007744216296216466 step  421\n",
      "cost:  3.727704511041345\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.009346114510860426 step  291\n",
      "cost:  3.176352834024934\n",
      "opt took 0.00min,  291iters\n",
      "error:  0.009208419271393153 step  351\n",
      "cost:  2.867897105415651\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.007991993498050798 step  431\n",
      "cost:  2.5878034495950857\n",
      "opt took 0.00min,  431iters\n",
      "error:  0.008094083364225457 step  411\n",
      "cost:  2.425578242972046\n",
      "opt took 0.00min,  411iters\n",
      "error:  0.00788964362337674 step  381\n",
      "cost:  2.1343926459298337\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.00827816426717376 step  371\n",
      "cost:  1.86914879057052\n",
      "opt took 0.00min,  371iters\n",
      "error:  0.008058685307417157 step  351\n",
      "cost:  1.715498920718354\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.008336982026523554 step  391\n",
      "cost:  1.6774428950365974\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [13][10/14]Time: 0.412 (0.319) Data: 0.386 (0.292) Loss: 0.8958 (1.0572)\n",
      "error:  0.007509830024550634 step  361\n",
      "cost:  1.6066493260770929\n",
      "opt took 0.00min,  361iters\n",
      "error:  0.008989970828636173 step  461\n",
      "cost:  1.495061767226463\n",
      "opt took 0.00min,  461iters\n",
      "error:  0.00872251976593752 step  411\n",
      "cost:  1.3044680242304962\n",
      "opt took 0.00min,  411iters\n",
      "10-NN,s=0.1: TOP1:  76.6\n",
      "Saving..\n",
      "best accuracy: 76.60\n",
      "\n",
      "Epoch: 14\n",
      "VGG\n",
      "error:  0.009556569729701092 step  391\n",
      "cost:  3.5993811510077363\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [14][0/14]Time: 0.320 (0.320) Data: 0.293 (0.293) Loss: 1.1524 (1.1524)\n",
      "error:  0.009888931632549114 step  401\n",
      "cost:  3.4330489620548366\n",
      "opt took 0.00min,  401iters\n",
      "error:  0.009501044897830324 step  421\n",
      "cost:  3.218800816831965\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.007901971984616285 step  461\n",
      "cost:  2.9514737736502195\n",
      "opt took 0.00min,  461iters\n",
      "error:  0.009571273759870547 step  451\n",
      "cost:  2.5428085006425656\n",
      "opt took 0.01min,  451iters\n",
      "error:  0.009630252290025454 step  351\n",
      "cost:  2.264695908631004\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.009373873136840016 step  351\n",
      "cost:  2.026747845622406\n",
      "opt took 0.00min,  351iters\n",
      "error:  0.009511040815958771 step  651\n",
      "cost:  1.774803401211671\n",
      "opt took 0.00min,  651iters\n",
      "error:  0.009159368047353955 step  341\n",
      "cost:  1.7954015271699935\n",
      "opt took 0.00min,  341iters\n",
      "error:  0.009378238565716623 step  391\n",
      "cost:  1.5584017153493337\n",
      "opt took 0.00min,  391iters\n",
      "error:  0.008996337055369619 step  541\n",
      "cost:  1.3550528360581169\n",
      "opt took 0.00min,  541iters\n",
      "Epoch: [14][10/14]Time: 0.346 (0.354) Data: 0.316 (0.326) Loss: 0.8159 (0.9657)\n",
      "error:  0.008170000583854198 step  331\n",
      "cost:  1.2839222792074252\n",
      "opt took 0.00min,  331iters\n",
      "error:  0.009160041090094495 step  731\n",
      "cost:  1.1743247959140948\n",
      "opt took 0.01min,  731iters\n",
      "error:  0.00791019762147216 step  371\n",
      "cost:  1.147523882788523\n",
      "opt took 0.00min,  371iters\n",
      "10-NN,s=0.1: TOP1:  77.46666666666667\n",
      "Saving..\n",
      "best accuracy: 77.47\n",
      "\n",
      "Epoch: 15\n",
      "VGG\n",
      "error:  0.009755513334976262 step  531\n",
      "cost:  2.2004608136160373\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [15][0/14]Time: 0.320 (0.320) Data: 0.293 (0.293) Loss: 1.0548 (1.0548)\n",
      "error:  0.008892113215633435 step  891\n",
      "cost:  2.2103681706822624\n",
      "opt took 0.00min,  891iters\n",
      "error:  0.009318654136262094 step  761\n",
      "cost:  1.9603674213145146\n",
      "opt took 0.00min,  761iters\n",
      "error:  0.009298594004609306 step  441\n",
      "cost:  1.720352363065254\n",
      "opt took 0.00min,  441iters\n",
      "error:  0.009728554638242182 step  1241\n",
      "cost:  1.747604271121561\n",
      "opt took 0.00min, 1241iters\n",
      "error:  0.008096333465821504 step  431\n",
      "cost:  1.4679679077402523\n",
      "opt took 0.00min,  431iters\n",
      "error:  0.009284395126959 step  801\n",
      "cost:  1.3641328864797717\n",
      "opt took 0.00min,  801iters\n",
      "error:  0.009931781316204091 step  971\n",
      "cost:  1.3341449659113342\n",
      "opt took 0.00min,  971iters\n",
      "error:  0.008610757662527724 step  361\n",
      "cost:  1.2701817065562284\n",
      "opt took 0.00min,  361iters\n",
      "error:  0.009527641181017565 step  781\n",
      "cost:  1.1873416034862037\n",
      "opt took 0.00min,  781iters\n",
      "error:  0.009461050366422086 step  1001\n",
      "cost:  1.1542690844825747\n",
      "opt took 0.00min, 1001iters\n",
      "Epoch: [15][10/14]Time: 0.354 (0.387) Data: 0.328 (0.360) Loss: 0.7622 (0.8745)\n",
      "error:  0.009002038714629346 step  421\n",
      "cost:  1.121548882146333\n",
      "opt took 0.00min,  421iters\n",
      "error:  0.009016499477875528 step  781\n",
      "cost:  0.9570515834382354\n",
      "opt took 0.00min,  781iters\n",
      "error:  0.009188256178694676 step  1061\n",
      "cost:  0.9214079666735322\n",
      "opt took 0.00min, 1061iters\n",
      "10-NN,s=0.1: TOP1:  78.13333333333334\n",
      "Saving..\n",
      "best accuracy: 78.13\n",
      "\n",
      "Epoch: 16\n",
      "VGG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009583208491015793 step  641\n",
      "cost:  0.9317895308880537\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [16][0/14]Time: 0.451 (0.451) Data: 0.421 (0.421) Loss: 0.7747 (0.7747)\n",
      "error:  0.009981832580348793 step  761\n",
      "cost:  0.8568916378553529\n",
      "opt took 0.00min,  761iters\n",
      "error:  0.008245651268275922 step  451\n",
      "cost:  0.8190343798840436\n",
      "opt took 0.00min,  451iters\n",
      "error:  0.008688226405503596 step  681\n",
      "cost:  0.7279847756556717\n",
      "opt took 0.00min,  681iters\n",
      "error:  0.009703477407217442 step  621\n",
      "cost:  0.6556456507340278\n",
      "opt took 0.00min,  621iters\n",
      "error:  0.00958458520511185 step  561\n",
      "cost:  0.6572853573125448\n",
      "opt took 0.00min,  561iters\n",
      "error:  0.009607642452980625 step  661\n",
      "cost:  0.5989193934627682\n",
      "opt took 0.00min,  661iters\n",
      "error:  0.008172189494451865 step  511\n",
      "cost:  0.5781826708151887\n",
      "opt took 0.01min,  511iters\n",
      "error:  0.00947482126794108 step  591\n",
      "cost:  0.6005754138047655\n",
      "opt took 0.00min,  591iters\n",
      "error:  0.00931535633334124 step  791\n",
      "cost:  0.5988463558022267\n",
      "opt took 0.00min,  791iters\n",
      "error:  0.009208618193776053 step  651\n",
      "cost:  0.5657041587568147\n",
      "opt took 0.00min,  651iters\n",
      "Epoch: [16][10/14]Time: 0.481 (0.424) Data: 0.441 (0.396) Loss: 0.6659 (0.6903)\n",
      "error:  0.009555192746796393 step  1291\n",
      "cost:  0.5443556222729158\n",
      "opt took 0.01min, 1291iters\n",
      "error:  0.009884479913807254 step  991\n",
      "cost:  0.5142176755836961\n",
      "opt took 0.00min,  991iters\n",
      "error: ng head 6  0.009262465971314304 step  1131\n",
      "cost:  0.46568945600406797\n",
      "opt took 0.00min, 1131iters\n",
      "10-NN,s=0.1: TOP1:  77.66666666666667\n",
      "best accuracy: 78.13\n",
      "\n",
      "Epoch: 17\n",
      "VGG\n",
      "error:  0.008320786379214629 step  501\n",
      "cost:  0.5452340891396686\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [17][0/14]Time: 0.351 (0.351) Data: 0.326 (0.326) Loss: 0.6655 (0.6655)\n",
      "error:  0.00901999472281001 step  721\n",
      "cost:  0.4552834991867157\n",
      "opt took 0.00min,  721iters\n",
      "error:  0.009875925540087538 step  471\n",
      "cost:  0.49000044628069617\n",
      "opt took 0.00min,  471iters\n",
      "error:  0.009196012545142684 step  481\n",
      "cost:  0.5107527656711331\n",
      "opt took 0.00min,  481iters\n",
      "error:  0.009599870368604102 step  671\n",
      "cost:  0.4764544683510339\n",
      "opt took 0.01min,  671iters\n",
      "error:  0.00893052193656274 step  591\n",
      "cost:  0.4676084003032282\n",
      "opt took 0.00min,  591iters\n",
      "error:  0.008939741978667537 step  471\n",
      "cost:  0.4805083338012437\n",
      "opt took 0.00min,  471iters\n",
      "error:  0.009095101922947713 step  641\n",
      "cost:  0.48158980673899776\n",
      "opt took 0.00min,  641iters\n",
      "error:  0.008634018041756564 step  551\n",
      "cost:  0.4489192855082965\n",
      "opt took 0.00min,  551iters\n",
      "error:  0.009038513461419817 step  631\n",
      "cost:  0.4720654276184258\n",
      "opt took 0.00min,  631iters\n",
      "error:  0.008422553071873895 step  491\n",
      "cost:  0.4464234956281759\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [17][10/14]Time: 0.356 (0.368) Data: 0.329 (0.341) Loss: 0.5947 (0.6459)\n",
      "error:  0.00952066726939138 step  741\n",
      "cost:  0.430437506860788\n",
      "opt took 0.00min,  741iters\n",
      "error:  0.009103754833962863 step  531\n",
      "cost:  0.4378747721747242\n",
      "opt took 0.00min,  531iters\n",
      "error:  0.009168589418014794 step  951\n",
      "cost:  0.4100090987217956\n",
      "opt took 0.00min,  951iters\n",
      "10-NN,s=0.1: TOP1:  77.9\n",
      "best accuracy: 78.13\n",
      "\n",
      "Epoch: 18\n",
      "VGG\n",
      "error:  0.009755476412010244 step  851\n",
      "cost:  0.42336565008754634\n",
      "opt took 0.00min,  851iters\n",
      "Epoch: [18][0/14]Time: 0.329 (0.329) Data: 0.303 (0.303) Loss: 0.6375 (0.6375)\n",
      "error:  0.009826037444368718 step  1061\n",
      "cost:  0.4174675167836258\n",
      "opt took 0.00min, 1061iters\n",
      "error:  0.00928484111510508 step  911\n",
      "cost:  0.39634427282397927\n",
      "opt took 0.00min,  911iters\n",
      "error:  0.009470231266982099 step  841\n",
      "cost:  0.3863103205254783\n",
      "opt took 0.01min,  841iters\n",
      "error:  0.009795975080082009 step  1051\n",
      "cost:  0.3842013761355967\n",
      "opt took 0.00min, 1051iters\n",
      "error:  0.009628736608839894 step  881\n",
      "cost:  0.3655638538636305\n",
      "opt took 0.00min,  881iters\n",
      "error:  0.009479256403863157 step  941\n",
      "cost:  0.36068034976641533\n",
      "opt took 0.00min,  941iters\n",
      "error:  0.009486659214117621 step  1011\n",
      "cost:  0.35239356633919017\n",
      "opt took 0.00min, 1011iters\n",
      "error:  0.00992637166018584 step  1031\n",
      "cost:  0.33852946539048906\n",
      "opt took 0.00min, 1031iters\n",
      "error:  0.009636018239398192 step  1031\n",
      "cost:  0.3392583496473663\n",
      "opt took 0.00min, 1031iters\n",
      "error:  0.009378191092599408 step  791\n",
      "cost:  0.33762158812631465\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [18][10/14]Time: 0.503 (0.407) Data: 0.472 (0.379) Loss: 0.5006 (0.5502)\n",
      "error:  0.009094463862201585 step  781\n",
      "cost:  0.33035982618660653\n",
      "opt took 0.00min,  781iters\n",
      "error:  0.009575358241069698 step  791\n",
      "cost:  0.34066342703957053\n",
      "opt took 0.01min,  791iters\n",
      "error:  0.009969625981498753 step  661\n",
      "cost:  0.341481910978483\n",
      "opt took 0.00min,  661iters\n",
      "10-NN,s=0.1: TOP1:  78.0\n",
      "best accuracy: 78.13\n",
      "\n",
      "Epoch: 19\n",
      "VGG\n",
      "error:  0.009156626506400478 step  1011\n",
      "cost:  0.3184403402210594\n",
      "opt took 0.00min, 1011iters\n",
      "Epoch: [19][0/14]Time: 0.387 (0.387) Data: 0.361 (0.361) Loss: 0.5183 (0.5183)\n",
      "error:  0.00892376489663338 step  801\n",
      "cost:  0.3309953322345177\n",
      "opt took 0.00min,  801iters\n",
      "error:  0.009991608680715913 step  221\n",
      "cost:  0.3228763123575709\n",
      "opt took 0.00min,  221iters\n",
      "error:  0.008372398129562364 step  641\n",
      "cost:  0.3518191701486953\n",
      "opt took 0.00min,  641iters\n",
      "error:  0.007606265055832728 step  241\n",
      "cost:  0.3313245359648386\n",
      "opt took 0.00min,  241iters\n",
      "error:  0.00996324276957039 step  701\n",
      "cost:  0.33065052411942003\n",
      "opt took 0.00min,  701iters\n",
      "error:  0.00874023170820204 step  701\n",
      "cost:  0.3433436078060247\n",
      "opt took 0.00min,  701iters\n",
      "error:  0.008033891124755499 step  381\n",
      "cost:  0.3308261414874542\n",
      "opt took 0.00min,  381iters\n",
      "error:  0.009334105422358907 step  821\n",
      "cost:  0.3438634735560564\n",
      "opt took 0.00min,  821iters\n",
      "error:  0.009930698660571169 step  1041\n",
      "cost:  0.3157885194146532\n",
      "opt took 0.00min, 1041iters\n",
      "error:  0.009871460039333924 step  991\n",
      "cost:  0.3133242537311147\n",
      "opt took 0.00min,  991iters\n",
      "Epoch: [19][10/14]Time: 0.438 (0.364) Data: 0.412 (0.337) Loss: 0.4916 (0.5393)\n",
      "error:  0.009793280068883914 step  1231\n",
      "cost:  0.33398982463026033\n",
      "opt took 0.00min, 1231iters\n",
      "error:  0.009984226678702468 step  301\n",
      "cost:  0.32636580739507404\n",
      "opt took 0.00min,  301iters\n",
      "error:  0.009998645096230385 step  311\n",
      "cost:  0.3108185530161692\n",
      "opt took 0.00min,  311iters\n",
      "10-NN,s=0.1: TOP1:  77.86666666666666\n",
      "best accuracy: 78.13\n",
      "doing PCA with 4 components ..done\n",
      "10-NN,s=0.1: TOP1:  73.9\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "best_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    \n",
    "    acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim)\n",
    "    accuracies.append(acc)\n",
    "    feature_return_switch(model, False)\n",
    "#     writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "        best_accuracies.append(best_acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = my_kNN(model, K=[50, 10], sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "#         for num_nn in [50, 10]:\n",
    "#             for sig in [0.1, 0.5]:\n",
    "#                 writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "#                 i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "end = time.time()\n",
    "\n",
    "# checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "# model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc(accuracies, model_name, dataset_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-labeling\n",
      "-------------------------\n",
      "Model:      VGG\n",
      "Dataset:    PenDigits\n",
      "Epochs:     20\n",
      "lr:         0.003\n",
      "Batch size: 500\n",
      "Heads:      10\n",
      "Time:       123.6963632106781\n",
      "Best acc:   78.13333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print (\"Self-labeling\")\n",
    "print (\"-------------------------\")\n",
    "print (\"Model:     \", model_name)\n",
    "print (\"Dataset:   \", dataset_name)\n",
    "print (\"Epochs:    \", epochs)\n",
    "print (\"lr:        \", lr)\n",
    "print (\"Batch size:\", batch_size)\n",
    "print (\"Heads:     \", hc)\n",
    "print (\"Time:      \", end-start)\n",
    "print (\"Best acc:  \", best_acc*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
