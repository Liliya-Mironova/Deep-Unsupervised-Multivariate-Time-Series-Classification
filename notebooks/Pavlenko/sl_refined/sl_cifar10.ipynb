{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"CIFAR-10\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 400   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './cifar_exp' # experiments results dir\n",
    "\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 128\n",
    "lr=0.03     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 4096\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters (AlexNet in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10       # number of heads\n",
    "ncl=128     # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "CFG = {\n",
    "    'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "    'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cifar-10-python.tar.gz', 'test', 'train', '.ipynb_checkpoints', 'cifar-10-batches-py']\n"
     ]
    }
   ],
   "source": [
    "print (os.listdir(datadir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Instance(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"CIFAR10Instance Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(CIFAR10Instance, self).__init__(root=root,\n",
    "                                                           train=train,\n",
    "                                                           transform=transform,\n",
    "                                                           target_transform=target_transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tfs.Compose([\n",
    "    tfs.Resize(256),\n",
    "    tfs.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n",
    "    tfs.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "    tfs.RandomGrayscale(p=0.2),\n",
    "    tfs.RandomHorizontalFlip(),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = tfs.Compose([\n",
    "        tfs.Resize(256),\n",
    "        tfs.CenterCrop(224),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CIFAR10Instance(root=datadir, train=True, download=False,\n",
    "                               transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "testset = CIFAR10Instance(root=datadir, train=False, download=False,\n",
    "                              transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "N = len(trainloader.dataset)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, features, num_classes, init=True):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.5),\n",
    "                            nn.Linear(256 * 6 * 6, 4096),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(4096, 4096),\n",
    "                            nn.ReLU(inplace=True))\n",
    "        self.headcount = len(num_classes)\n",
    "        self.return_features = False\n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Linear(4096, num_classes[0])\n",
    "        else:\n",
    "            for a,i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(4096, i))\n",
    "            self.top_layer = None  # this way headcount can act as switch.\n",
    "        if init:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(self.features)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        if self.return_features:\n",
    "            return x\n",
    "        x = self.classifier(x)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer: # this way headcount can act as switch.\n",
    "                x = self.top_layer(x)\n",
    "            return x\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(x))\n",
    "            return outp\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for y, m in enumerate(self.modules()):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                for i in range(m.out_channels):\n",
    "                    m.weight.data[i].normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def make_layers_features(cfg, input_dim, bn):\n",
    "    layers = []\n",
    "    in_channels = input_dim\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=3, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v[0], kernel_size=v[1], stride=v[2], padding=v[3])#,bias=False)\n",
    "            if bn:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v[0]), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v[0]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def alexnet(bn=True, num_classes=[1000], init=True, size='big'):\n",
    "    dim = 3\n",
    "    model = AlexNet(make_layers_features(CFG[size], dim, bn=bn), num_classes, init)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((len(trainloader.dataset), args.ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((len(trainloader.dataset), knn_dim))\n",
    "    for batch_idx, (data, _, _selected) in enumerate(trainloader):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data)\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy()\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(trainloader):\n",
    "        niter = epoch * len(trainloader) + batch_idx\n",
    "        if niter * trainloader.batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets, indexes = inputs.to(device), targets.to(device), indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h],\n",
    "                                                     selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, len(trainloader), batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "            writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*len(trainloader.dataset))\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet(num_classes=numc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [410.0, 401.0, 400.0, 398.99, 397.99, 396.98, 395.98, 394.97, 393.97, 392.96, 391.95, 390.95, 389.94, 388.94, 387.93, 386.93, 385.92, 384.92, 383.91, 382.91, 381.9, 380.9, 379.89, 378.89, 377.88, 376.88, 375.87, 374.87, 373.86, 372.86, 371.85, 370.85, 369.84, 368.84, 367.83, 366.83, 365.82, 364.82, 363.81, 362.81, 361.8, 360.8, 359.79, 358.79, 357.78, 356.78, 355.77, 354.77, 353.76, 352.76, 351.75, 350.75, 349.74, 348.74, 347.73, 346.73, 345.72, 344.72, 343.71, 342.71, 341.7, 340.7, 339.69, 338.69, 337.68, 336.68, 335.67, 334.67, 333.66, 332.66, 331.65, 330.65, 329.64, 328.64, 327.63, 326.63, 325.62, 324.62, 323.61, 322.61, 321.6, 320.6, 319.59, 318.59, 317.58, 316.58, 315.57, 314.57, 313.56, 312.56, 311.55, 310.55, 309.54, 308.54, 307.53, 306.53, 305.52, 304.52, 303.51, 302.51, 301.5, 300.5, 299.49, 298.49, 297.48, 296.48, 295.47, 294.47, 293.46, 292.46, 291.45, 290.45, 289.44, 288.44, 287.43, 286.43, 285.42, 284.42, 283.41, 282.41, 281.4, 280.4, 279.39, 278.39, 277.38, 276.38, 275.37, 274.37, 273.36, 272.36, 271.35, 270.35, 269.34, 268.34, 267.33, 266.33, 265.32, 264.32, 263.31, 262.31, 261.3, 260.3, 259.29, 258.29, 257.28, 256.28, 255.27, 254.27, 253.26, 252.26, 251.25, 250.25, 249.24, 248.24, 247.23, 246.23, 245.22, 244.22, 243.21, 242.21, 241.2, 240.2, 239.19, 238.19, 237.18, 236.18, 235.17, 234.17, 233.16, 232.16, 231.15, 230.15, 229.14, 228.14, 227.13, 226.13, 225.12, 224.12, 223.11, 222.11, 221.1, 220.1, 219.09, 218.09, 217.08, 216.08, 215.07, 214.07, 213.06, 212.06, 211.05, 210.05, 209.04, 208.04, 207.03, 206.03, 205.02, 204.02, 203.01, 202.01, 201.0, 200.0, 198.99, 197.99, 196.98, 195.98, 194.97, 193.97, 192.96, 191.96, 190.95, 189.95, 188.94, 187.94, 186.93, 185.93, 184.92, 183.92, 182.91, 181.91, 180.9, 179.9, 178.89, 177.89, 176.88, 175.88, 174.87, 173.87, 172.86, 171.86, 170.85, 169.85, 168.84, 167.84, 166.83, 165.83, 164.82, 163.82, 162.81, 161.81, 160.8, 159.8, 158.79, 157.79, 156.78, 155.78, 154.77, 153.77, 152.76, 151.76, 150.75, 149.75, 148.74, 147.74, 146.73, 145.73, 144.72, 143.72, 142.71, 141.71, 140.7, 139.7, 138.69, 137.69, 136.68, 135.68, 134.67, 133.67, 132.66, 131.66, 130.65, 129.65, 128.64, 127.64, 126.63, 125.63, 124.62, 123.62, 122.61, 121.61, 120.6, 119.6, 118.59, 117.59, 116.58, 115.58, 114.57, 113.57, 112.56, 111.56, 110.55, 109.55, 108.54, 107.54, 106.53, 105.53, 104.52, 103.52, 102.51, 101.51, 100.5, 99.5, 98.49, 97.49, 96.48, 95.48, 94.47, 93.47, 92.46, 91.46, 90.45, 89.45, 88.44, 87.44, 86.43, 85.43, 84.42, 83.42, 82.41, 81.41, 80.4, 79.4, 78.39, 77.39, 76.38, 75.38, 74.37, 73.37, 72.36, 71.36, 70.35, 69.35, 68.34, 67.34, 66.33, 65.33, 64.32, 63.32, 62.31, 61.31, 60.3, 59.3, 58.29, 57.29, 56.28, 55.28, 54.27, 53.27, 52.26, 51.26, 50.25, 49.25, 48.24, 47.24, 46.23, 45.23, 44.22, 43.22, 42.21, 41.21, 40.2, 39.2, 38.19, 37.19, 36.18, 35.18, 34.17, 33.17, 32.16, 31.16, 30.15, 29.15, 28.14, 27.14, 26.13, 25.13, 24.12, 23.12, 22.11, 21.11, 20.1, 19.1, 18.09, 17.09, 16.08, 15.08, 14.07, 13.07, 12.06, 11.06, 10.05, 9.05, 8.04, 7.04, 6.03, 5.03, 4.02, 3.02, 2.01, 1.01, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"AlexNetfromthepaper\"\n",
    "writer = SummaryWriter(f'./runs/cifar10/{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "AlexNetfromthepaper\n",
      "error:  0.003979012589635067 step  31\n",
      "cost:  4.0347929817170884\n",
      "opt took 0.01min,   31iters\n",
      "Epoch: [0][0/391]Time: 63.655 (63.655) Data: 63.556 (63.556) Loss: 4.9827 (4.9827)\n",
      "Epoch: [0][10/391]Time: 0.122 (5.895) Data: 0.017 (5.792) Loss: 4.9202 (4.9415)\n",
      "Epoch: [0][20/391]Time: 0.123 (3.146) Data: 0.018 (3.043) Loss: 4.9069 (4.9248)\n",
      "Epoch: [0][30/391]Time: 0.125 (2.177) Data: 0.021 (2.073) Loss: 4.8777 (4.9112)\n",
      "Epoch: [0][40/391]Time: 0.124 (1.687) Data: 0.019 (1.583) Loss: 4.8658 (4.9020)\n",
      "Epoch: [0][50/391]Time: 0.289 (1.393) Data: 0.186 (1.289) Loss: 4.8722 (4.8953)\n",
      "Epoch: [0][60/391]Time: 0.121 (1.193) Data: 0.016 (1.088) Loss: 4.8640 (4.8903)\n",
      "Epoch: [0][70/391]Time: 0.120 (1.052) Data: 0.016 (0.948) Loss: 4.8634 (4.8867)\n",
      "Epoch: [0][80/391]Time: 0.120 (0.942) Data: 0.018 (0.838) Loss: 4.8623 (4.8838)\n",
      "Epoch: [0][90/391]Time: 0.332 (0.860) Data: 0.230 (0.756) Loss: 4.8681 (4.8813)\n",
      "Epoch: [0][100/391]Time: 0.119 (0.789) Data: 0.016 (0.686) Loss: 4.8623 (4.8792)\n",
      "Epoch: [0][110/391]Time: 0.298 (0.738) Data: 0.194 (0.634) Loss: 4.8645 (4.8773)\n",
      "Epoch: [0][120/391]Time: 0.118 (0.692) Data: 0.016 (0.588) Loss: 4.8573 (4.8759)\n",
      "Epoch: [0][130/391]Time: 0.121 (0.650) Data: 0.018 (0.546) Loss: 4.8580 (4.8745)\n",
      "Epoch: [0][140/391]Time: 0.121 (0.616) Data: 0.017 (0.512) Loss: 4.8535 (4.8734)\n",
      "Epoch: [0][150/391]Time: 0.569 (0.588) Data: 0.467 (0.484) Loss: 4.8589 (4.8723)\n",
      "Epoch: [0][160/391]Time: 0.119 (0.561) Data: 0.016 (0.458) Loss: 4.8587 (4.8714)\n",
      "Epoch: [0][170/391]Time: 0.121 (0.538) Data: 0.019 (0.434) Loss: 4.8576 (4.8705)\n",
      "Epoch: [0][180/391]Time: 0.112 (0.518) Data: 0.010 (0.414) Loss: 4.8599 (4.8697)\n",
      "Epoch: [0][190/391]Time: 0.120 (0.503) Data: 0.016 (0.399) Loss: 4.8581 (4.8691)\n",
      "Epoch: [0][200/391]Time: 0.120 (0.487) Data: 0.016 (0.383) Loss: 4.8583 (4.8685)\n",
      "Epoch: [0][210/391]Time: 0.122 (0.471) Data: 0.019 (0.367) Loss: 4.8542 (4.8679)\n",
      "Epoch: [0][220/391]Time: 0.121 (0.456) Data: 0.017 (0.352) Loss: 4.8573 (4.8674)\n",
      "Epoch: [0][230/391]Time: 0.116 (0.443) Data: 0.013 (0.339) Loss: 4.8547 (4.8669)\n",
      "Epoch: [0][240/391]Time: 0.115 (0.430) Data: 0.014 (0.326) Loss: 4.8557 (4.8665)\n",
      "Epoch: [0][250/391]Time: 0.217 (0.419) Data: 0.110 (0.316) Loss: 4.8557 (4.8661)\n",
      "Epoch: [0][260/391]Time: 0.112 (0.408) Data: 0.011 (0.305) Loss: 4.8527 (4.8657)\n",
      "Epoch: [0][270/391]Time: 0.112 (0.398) Data: 0.010 (0.294) Loss: 4.8554 (4.8653)\n",
      "Epoch: [0][280/391]Time: 0.120 (0.389) Data: 0.016 (0.286) Loss: 4.8561 (4.8649)\n",
      "Epoch: [0][290/391]Time: 0.111 (0.381) Data: 0.011 (0.278) Loss: 4.8546 (4.8646)\n",
      "Epoch: [0][300/391]Time: 0.122 (0.373) Data: 0.016 (0.270) Loss: 4.8523 (4.8643)\n",
      "Epoch: [0][310/391]Time: 0.114 (0.365) Data: 0.012 (0.262) Loss: 4.8568 (4.8640)\n",
      "Epoch: [0][320/391]Time: 0.116 (0.358) Data: 0.012 (0.255) Loss: 4.8513 (4.8637)\n",
      "Epoch: [0][330/391]Time: 0.115 (0.351) Data: 0.012 (0.248) Loss: 4.8563 (4.8635)\n",
      "Epoch: [0][340/391]Time: 0.112 (0.345) Data: 0.011 (0.242) Loss: 4.8569 (4.8632)\n",
      "Epoch: [0][350/391]Time: 0.112 (0.339) Data: 0.011 (0.236) Loss: 4.8562 (4.8630)\n",
      "Epoch: [0][360/391]Time: 0.111 (0.333) Data: 0.010 (0.230) Loss: 4.8547 (4.8627)\n",
      "Epoch: [0][370/391]Time: 0.112 (0.328) Data: 0.010 (0.225) Loss: 4.8551 (4.8625)\n",
      "Epoch: [0][380/391]Time: 0.114 (0.322) Data: 0.013 (0.219) Loss: 4.8566 (4.8624)\n",
      "Epoch: [0][390/391]Time: 0.079 (0.317) Data: 0.010 (0.214) Loss: 4.8532 (4.8621)\n",
      "10-NN,s=0.1: TOP1:  37.11\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 128 components ..done\n",
      "50-NN,s=0.1: TOP1:  36.93\n",
      "50-NN,s=0.5: TOP1:  36.19\n",
      "10-NN,s=0.1: TOP1:  36.5\n",
      "10-NN,s=0.5: TOP1:  36.08\n",
      "best accuracy: 37.11\n",
      "\n",
      "Epoch: 1\n",
      "AlexNetfromthepaper\n",
      "Epoch: [1][0/391]Time: 1.707 (1.707) Data: 1.605 (1.605) Loss: 4.8550 (4.8550)\n",
      "error:  5.387579271598497e-12 step  11\n",
      "cost:  4.7346108968083085\n",
      "opt took 0.01min,   11iters\n",
      "Epoch: [1][10/391]Time: 0.118 (6.305) Data: 0.015 (6.202) Loss: 4.8531 (4.8527)\n",
      "Epoch: [1][20/391]Time: 0.127 (3.361) Data: 0.022 (3.257) Loss: 4.8517 (4.8525)\n",
      "Epoch: [1][30/391]Time: 0.127 (2.317) Data: 0.021 (2.213) Loss: 4.8548 (4.8528)\n",
      "Epoch: [1][40/391]Time: 0.123 (1.795) Data: 0.017 (1.690) Loss: 4.8548 (4.8529)\n",
      "Epoch: [1][50/391]Time: 0.123 (1.477) Data: 0.018 (1.373) Loss: 4.8517 (4.8529)\n",
      "Epoch: [1][60/391]Time: 0.125 (1.266) Data: 0.019 (1.162) Loss: 4.8502 (4.8528)\n",
      "Epoch: [1][70/391]Time: 0.113 (1.105) Data: 0.011 (1.000) Loss: 4.8547 (4.8529)\n",
      "Epoch: [1][80/391]Time: 0.114 (0.985) Data: 0.011 (0.881) Loss: 4.8512 (4.8529)\n",
      "Epoch: [1][90/391]Time: 0.119 (0.894) Data: 0.015 (0.790) Loss: 4.8543 (4.8530)\n",
      "Epoch: [1][100/391]Time: 0.118 (0.818) Data: 0.015 (0.714) Loss: 4.8537 (4.8530)\n",
      "Epoch: [1][110/391]Time: 0.111 (0.762) Data: 0.010 (0.658) Loss: 4.8542 (4.8529)\n",
      "Epoch: [1][120/391]Time: 0.112 (0.709) Data: 0.011 (0.605) Loss: 4.8551 (4.8529)\n",
      "Epoch: [1][130/391]Time: 0.113 (0.665) Data: 0.012 (0.561) Loss: 4.8539 (4.8529)\n",
      "Epoch: [1][140/391]Time: 0.112 (0.628) Data: 0.011 (0.524) Loss: 4.8525 (4.8528)\n",
      "Epoch: [1][150/391]Time: 0.119 (0.597) Data: 0.016 (0.493) Loss: 4.8520 (4.8528)\n",
      "Epoch: [1][160/391]Time: 0.118 (0.572) Data: 0.015 (0.468) Loss: 4.8526 (4.8528)\n",
      "Epoch: [1][170/391]Time: 0.122 (0.548) Data: 0.017 (0.444) Loss: 4.8534 (4.8527)\n",
      "Epoch: [1][180/391]Time: 0.112 (0.526) Data: 0.010 (0.422) Loss: 4.8514 (4.8527)\n",
      "Epoch: [1][190/391]Time: 0.111 (0.505) Data: 0.010 (0.401) Loss: 4.8529 (4.8528)\n",
      "Epoch: [1][200/391]Time: 0.113 (0.486) Data: 0.010 (0.382) Loss: 4.8534 (4.8528)\n",
      "Epoch: [1][210/391]Time: 0.120 (0.469) Data: 0.017 (0.366) Loss: 4.8495 (4.8527)\n",
      "Epoch: [1][220/391]Time: 0.112 (0.453) Data: 0.010 (0.350) Loss: 4.8513 (4.8527)\n",
      "Epoch: [1][230/391]Time: 0.120 (0.439) Data: 0.016 (0.336) Loss: 4.8506 (4.8527)\n",
      "Epoch: [1][240/391]Time: 0.119 (0.426) Data: 0.016 (0.323) Loss: 4.8517 (4.8526)\n",
      "Epoch: [1][250/391]Time: 0.117 (0.414) Data: 0.015 (0.311) Loss: 4.8536 (4.8526)\n",
      "Epoch: [1][260/391]Time: 0.173 (0.403) Data: 0.069 (0.300) Loss: 4.8519 (4.8526)\n",
      "Epoch: [1][270/391]Time: 0.125 (0.395) Data: 0.019 (0.291) Loss: 4.8487 (4.8525)\n",
      "Epoch: [1][280/391]Time: 0.113 (0.385) Data: 0.011 (0.281) Loss: 4.8506 (4.8525)\n",
      "Epoch: [1][290/391]Time: 0.117 (0.376) Data: 0.015 (0.272) Loss: 4.8518 (4.8525)\n",
      "Epoch: [1][300/391]Time: 0.122 (0.367) Data: 0.019 (0.264) Loss: 4.8518 (4.8525)\n",
      "Epoch: [1][310/391]Time: 0.116 (0.360) Data: 0.013 (0.257) Loss: 4.8536 (4.8525)\n",
      "Epoch: [1][320/391]Time: 0.118 (0.353) Data: 0.016 (0.250) Loss: 4.8528 (4.8525)\n",
      "Epoch: [1][330/391]Time: 0.126 (0.347) Data: 0.021 (0.243) Loss: 4.8564 (4.8525)\n",
      "Epoch: [1][340/391]Time: 0.122 (0.341) Data: 0.019 (0.238) Loss: 4.8554 (4.8525)\n",
      "Epoch: [1][350/391]Time: 0.122 (0.336) Data: 0.017 (0.232) Loss: 4.8537 (4.8525)\n",
      "Epoch: [1][360/391]Time: 0.282 (0.331) Data: 0.180 (0.227) Loss: 4.8514 (4.8525)\n",
      "Epoch: [1][370/391]Time: 0.114 (0.325) Data: 0.011 (0.222) Loss: 4.8499 (4.8524)\n",
      "Epoch: [1][380/391]Time: 0.113 (0.321) Data: 0.010 (0.218) Loss: 4.8479 (4.8524)\n",
      "Epoch: [1][390/391]Time: 0.081 (0.316) Data: 0.010 (0.213) Loss: 4.8518 (4.8524)\n",
      "10-NN,s=0.1: TOP1:  34.7\n",
      "best accuracy: 37.11\n",
      "\n",
      "Epoch: 2\n",
      "AlexNetfromthepaper\n",
      "Epoch: [2][0/391]Time: 1.629 (1.629) Data: 1.515 (1.515) Loss: 4.8452 (4.8452)\n",
      "error:  2.1727619703426626e-11 step  11\n",
      "cost:  4.7508339560657395\n",
      "opt took 0.01min,   11iters\n",
      "Epoch: [2][10/391]Time: 0.113 (5.762) Data: 0.011 (5.657) Loss: 4.8530 (4.8505)\n",
      "Epoch: [2][20/391]Time: 0.116 (3.073) Data: 0.013 (2.969) Loss: 4.8535 (4.8499)\n",
      "Epoch: [2][30/391]Time: 0.114 (2.119) Data: 0.012 (2.016) Loss: 4.8494 (4.8500)\n",
      "Epoch: [2][40/391]Time: 0.113 (1.630) Data: 0.011 (1.527) Loss: 4.8474 (4.8498)\n",
      "Epoch: [2][50/391]Time: 0.115 (1.333) Data: 0.013 (1.230) Loss: 4.8508 (4.8497)\n",
      "Epoch: [2][60/391]Time: 0.114 (1.133) Data: 0.012 (1.031) Loss: 4.8490 (4.8496)\n",
      "Epoch: [2][70/391]Time: 0.114 (0.990) Data: 0.013 (0.888) Loss: 4.8442 (4.8494)\n",
      "Epoch: [2][80/391]Time: 0.114 (0.884) Data: 0.012 (0.782) Loss: 4.8510 (4.8492)\n",
      "Epoch: [2][90/391]Time: 0.111 (0.800) Data: 0.010 (0.698) Loss: 4.8523 (4.8491)\n",
      "Epoch: [2][100/391]Time: 0.114 (0.733) Data: 0.014 (0.630) Loss: 4.8437 (4.8490)\n",
      "Epoch: [2][110/391]Time: 0.298 (0.680) Data: 0.196 (0.578) Loss: 4.8457 (4.8488)\n",
      "Epoch: [2][120/391]Time: 0.111 (0.634) Data: 0.010 (0.532) Loss: 4.8516 (4.8486)\n",
      "Epoch: [2][130/391]Time: 0.111 (0.595) Data: 0.010 (0.493) Loss: 4.8482 (4.8485)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][140/391]Time: 0.111 (0.562) Data: 0.010 (0.459) Loss: 4.8326 (4.8483)\n",
      "Epoch: [2][150/391]Time: 0.307 (0.534) Data: 0.201 (0.432) Loss: 4.8446 (4.8482)\n",
      "Epoch: [2][160/391]Time: 0.112 (0.511) Data: 0.011 (0.409) Loss: 4.8386 (4.8480)\n",
      "Epoch: [2][170/391]Time: 0.113 (0.490) Data: 0.011 (0.388) Loss: 4.8491 (4.8479)\n",
      "Epoch: [2][180/391]Time: 0.116 (0.472) Data: 0.014 (0.369) Loss: 4.8436 (4.8479)\n",
      "Epoch: [2][190/391]Time: 0.202 (0.454) Data: 0.098 (0.351) Loss: 4.8447 (4.8479)\n",
      "Epoch: [2][200/391]Time: 0.113 (0.437) Data: 0.011 (0.335) Loss: 4.8428 (4.8478)\n",
      "Epoch: [2][210/391]Time: 0.113 (0.422) Data: 0.011 (0.320) Loss: 4.8465 (4.8477)\n",
      "Epoch: [2][220/391]Time: 0.115 (0.409) Data: 0.012 (0.307) Loss: 4.8430 (4.8477)\n",
      "Epoch: [2][230/391]Time: 0.313 (0.400) Data: 0.210 (0.297) Loss: 4.8456 (4.8476)\n",
      "Epoch: [2][240/391]Time: 0.122 (0.389) Data: 0.017 (0.287) Loss: 4.8499 (4.8475)\n",
      "Epoch: [2][250/391]Time: 0.112 (0.380) Data: 0.011 (0.277) Loss: 4.8453 (4.8475)\n",
      "Epoch: [2][260/391]Time: 0.114 (0.370) Data: 0.012 (0.267) Loss: 4.8380 (4.8474)\n",
      "Epoch: [2][270/391]Time: 0.281 (0.361) Data: 0.178 (0.259) Loss: 4.8463 (4.8474)\n",
      "Epoch: [2][280/391]Time: 0.121 (0.353) Data: 0.017 (0.251) Loss: 4.8433 (4.8473)\n",
      "Epoch: [2][290/391]Time: 0.121 (0.345) Data: 0.017 (0.243) Loss: 4.8486 (4.8472)\n",
      "Epoch: [2][300/391]Time: 0.122 (0.339) Data: 0.017 (0.237) Loss: 4.8489 (4.8472)\n",
      "Epoch: [2][310/391]Time: 0.411 (0.335) Data: 0.310 (0.232) Loss: 4.8444 (4.8472)\n",
      "Epoch: [2][320/391]Time: 0.121 (0.329) Data: 0.016 (0.227) Loss: 4.8435 (4.8471)\n",
      "Epoch: [2][330/391]Time: 0.124 (0.325) Data: 0.018 (0.222) Loss: 4.8463 (4.8471)\n",
      "Epoch: [2][340/391]Time: 0.113 (0.320) Data: 0.011 (0.217) Loss: 4.8414 (4.8470)\n",
      "Epoch: [2][350/391]Time: 0.300 (0.315) Data: 0.194 (0.213) Loss: 4.8468 (4.8470)\n",
      "Epoch: [2][360/391]Time: 0.122 (0.311) Data: 0.017 (0.208) Loss: 4.8466 (4.8470)\n",
      "Epoch: [2][370/391]Time: 0.122 (0.307) Data: 0.018 (0.205) Loss: 4.8492 (4.8469)\n",
      "Epoch: [2][380/391]Time: 0.122 (0.304) Data: 0.018 (0.201) Loss: 4.8419 (4.8468)\n",
      "Epoch: [2][390/391]Time: 0.082 (0.300) Data: 0.012 (0.198) Loss: 4.8365 (4.8468)\n",
      "10-NN,s=0.1: TOP1:  32.71\n",
      "best accuracy: 37.11\n",
      "\n",
      "Epoch: 3\n",
      "AlexNetfromthepaper\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    acc = kNN(model, trainloader, testloader, K=10, sigma=0.1, dim=knn_dim)\n",
    "    feature_return_switch(model, False)\n",
    "    writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = kNN(model, trainloader, testloader, K=[50, 10],\n",
    "                  sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "        for num_nn in [50, 10]:\n",
    "            for sig in [0.1, 0.5]:\n",
    "                writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "                i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "\n",
    "checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = kNN(model, trainloader, testloader, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
