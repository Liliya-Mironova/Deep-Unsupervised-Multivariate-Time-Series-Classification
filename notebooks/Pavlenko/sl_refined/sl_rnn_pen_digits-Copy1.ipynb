{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "from scipy.io import arff\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sktime.utils.load_data import load_from_tsfile_to_dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = 'PenDigits/PenDigits'\n",
    "label_enc = True\n",
    "dims_num = 2\n",
    "num_classes = 10\n",
    "magic_dim = 256 # classifier_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"/root/data/Multivariate_ts\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 400   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './lstm_exp' # experiments results dir\n",
    "\n",
    "\n",
    "# other\n",
    "devc='1'  # cuda device\n",
    "batch_size = 500\n",
    "lr=0.1     #learning rate\n",
    "alr=0.9    #starting learning rate\n",
    "\n",
    "knn_dim = 10\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10       # number of heads\n",
    "ncl=num_classes       # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# numc = np.linspace(10, 1000, hc, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(X, dims_num):\n",
    "    X = X.applymap(np.array)\n",
    "    cols_lst = X.columns\n",
    "    \n",
    "    for col_name in cols_lst:\n",
    "        dimension_values = np.dstack(list(X[col_name].values))[0].T\n",
    "        time_steps_num = dimension_values.shape[1]\n",
    "        \n",
    "        for i in range(time_steps_num):\n",
    "            one_step_values = dimension_values[:, i]\n",
    "            X[col_name + f'_{i}'] = one_step_values\n",
    "    \n",
    "    X = X.drop(columns=cols_lst)\n",
    "    \n",
    "    step = int(X.shape[1] / dims_num)\n",
    "    X_3d = []\n",
    "    init = 0\n",
    "    for dim_num in range(dims_num):\n",
    "        X_3d.append(X.iloc[:, init:init + step])\n",
    "        init += step\n",
    "    X_3d = np.dstack(X_3d)\n",
    "    return X_3d\n",
    "\n",
    "def load_file(filepath, dims_num, is_arff=False):\n",
    "    if is_arff:\n",
    "        data = arff.loadarff(filepath) \n",
    "        data = pd.DataFrame(data[0])\n",
    "        X = data.iloc[:, :-1] # [30, 65] x 4 times\n",
    "        y = data.iloc[:, -1]\n",
    "        return X.values, y.values\n",
    "    else:\n",
    "        X, y = load_from_tsfile_to_dataframe(filepath)\n",
    "        X = preproc(X, dims_num)\n",
    "        return X, y\n",
    "\n",
    "def load_group(prefix, filenames, dims_num, is_arff=False):\n",
    "    loaded = []\n",
    "    if is_arff:\n",
    "        for name in filenames: \n",
    "            X, y = load_file(prefix + \"/\" + name, dims_num, is_arff=is_arff) # [30, 65]\n",
    "            loaded.append(X)\n",
    "            # stack group so that features are the 3rd dimension \n",
    "        loaded = np.dstack(loaded) # [30, 65, 4]\n",
    "    else:\n",
    "        loaded, y = load_file(prefix + \"/\" + filenames[0], dims_num, is_arff=is_arff) # [30, 65, 4]\n",
    "    return loaded, y\n",
    "\n",
    "def load_dataset_group(folder_path, ds_path, dims_num, is_train=True, label_enc=False, is_arff=False): \n",
    "    filenames = []\n",
    "    extension = \".arff\" if is_arff else \".ts\"\n",
    "    postfix = \"_TRAIN\" if is_train else \"_TEST\"\n",
    "    \n",
    "    if is_arff:\n",
    "        for dim_num in range(1, dims_num + 1):\n",
    "            filenames.append(ds_path + str(dim_num) + postfix + extension)\n",
    "    else:\n",
    "        filenames.append(ds_path + postfix + extension)\n",
    "\n",
    "    X, y = load_group(folder_path, filenames, dims_num, is_arff=is_arff) # [30, 65, 4]\n",
    "    X = torch.from_numpy(np.array(X, dtype=np.float64))\n",
    "    if label_enc:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        y = torch.from_numpy(np.array(y, dtype=np.int32))\n",
    "    else:\n",
    "        y = torch.from_numpy(np.array(y, dtype=np.int32)) - 1\n",
    "    X = X.transpose(1, 2)\n",
    "    return X, y\n",
    "\n",
    "def load_dataset(folder_path, ds_path, dims_num, label_enc=False, is_arff=False): \n",
    "    X_train, y_train = load_dataset_group(folder_path, ds_path, dims_num, \n",
    "                                          is_train=True, label_enc=label_enc, is_arff=is_arff) \n",
    "    X_test, y_test = load_dataset_group(folder_path, ds_path, dims_num, \n",
    "                                        is_train=False, label_enc=label_enc, is_arff=is_arff) \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: torch.Size([7494, 2, 8]) \n",
      "y_train.shape: torch.Size([7494])\n",
      "X_test.shape: torch.Size([3498, 2, 8]) \n",
      "y_test.shape: torch.Size([3498])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_dataset(datadir, ds_path, dims_num, label_enc=label_enc, is_arff=False)\n",
    "print(\"X_train.shape:\", X_train.shape, \"\\ny_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape, \"\\ny_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 7494\n"
     ]
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "print('train size:', N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFeatures(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, lstm_dropout=0.0):\n",
    "        super(LSTMFeatures, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=lstm_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        batch_size = x.shape[1]\n",
    "        hidden = (torch.rand(self.num_layers, batch_size, self.hidden_size, device=device),\n",
    "                  torch.rand(self.num_layers, batch_size, self.hidden_size, device=device))\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        return out, hidden\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, lstm_features, hidden_size, num_classes, classifier_dim=256):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm_features = lstm_features\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.5),\n",
    "                            nn.Linear(hidden_size, classifier_dim),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(classifier_dim, classifier_dim),\n",
    "                            nn.ReLU(inplace=True))\n",
    "        self.top_layer = nn.Linear(classifier_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm_features(x)\n",
    "        out = self.classifier(out[-1])\n",
    "        out = self.top_layer(out)\n",
    "        return out\n",
    "\n",
    "class LSTMMultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, lstm_features, hidden_size, num_classes, classifier_dim=256):\n",
    "        super(LSTMMultiHeadClassifier, self).__init__()\n",
    "        self.lstm_features = lstm_features\n",
    "        self.classifier = nn.Sequential(\n",
    "                            nn.BatchNorm1d(hidden_size),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(hidden_size, classifier_dim),\n",
    "                            nn.BatchNorm1d(classifier_dim),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(classifier_dim, classifier_dim),\n",
    "                            nn.ReLU(inplace=True))\n",
    "        self.num_classes = num_classes\n",
    "        self.headcount = len(num_classes)\n",
    "        self.return_features = False\n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Linear(classifier_dim, num_classes[0])\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(classifier_dim, i))\n",
    "            self.top_layer = None  # this way headcount can act as switch.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm_features(x)\n",
    "        x = out[-1]\n",
    "        if self.return_features:\n",
    "            return x\n",
    "        x = self.classifier(x)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer: # this way headcount can act as switch.\n",
    "                x = self.top_layer(x)\n",
    "            return x\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(x))\n",
    "            return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.X = X.float()\n",
    "        self.y = y.long()\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(epoch, net, optimizer, criterion, dataloader, losses):\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = net(X)\n",
    "        loss = criterion(pred, y)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_net(net, criterion, dataloader):\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    preds = []\n",
    "    ys = []\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = net(X)\n",
    "            loss = criterion(pred, y) * y.shape[0]\n",
    "            total_loss += loss.item()\n",
    "            total_count += y.shape[0]\n",
    "            preds.append(torch.argmax(pred, dim=1))\n",
    "            ys.append(y)\n",
    "    preds = torch.cat(preds)\n",
    "    ys = torch.cat(ys)\n",
    "    accuracy = (preds == ys).sum().item() / ys.shape[0]\n",
    "    return total_loss / total_count, accuracy, preds, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataset(X_train, y_train, transform=None)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_test = MyDataset(X_test, y_test, transform=None)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sample = dataset_train[0][0]\n",
    "for i in range(dims_num):\n",
    "    plt.plot(sample[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = LSTMFeatures(dims_num, 16 * dims_num, 4, lstm_dropout=0.0)\n",
    "net = LSTMClassifier(features, 16 * dims_num, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "test_losses = []\n",
    "test_iters = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.3026126536592337 test accuracy: 0.10377358490566038\n"
     ]
    }
   ],
   "source": [
    "loss, acc, _, _ = evaluate_net(net, criterion, dataloader_test)\n",
    "print('test loss:', loss, 'test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     print('epoch:', epoch)\n",
    "#     run_epoch(epoch, net, optimizer, criterion, dataloader_train, losses)\n",
    "#     loss, acc, _, _ = evaluate_net(net, criterion, dataloader_test)\n",
    "#     clear_output()\n",
    "#     plt.title('train and test losses')\n",
    "#     plt.plot(losses)\n",
    "#     test_losses.append(loss)\n",
    "#     test_accuracies.append(acc)\n",
    "#     test_iters.append(len(losses))\n",
    "#     plt.plot(test_iters, test_losses)\n",
    "#     plt.show()\n",
    "#     plt.title('test accuracy')\n",
    "#     plt.plot(test_iters, test_accuracies)\n",
    "#     plt.show()\n",
    "#     print('test loss:', loss, 'test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "        \n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((N, ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((N, magic_dim)) # knn_dim\n",
    "    \n",
    "    for batch_idx, (data, _, _selected) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data.float())\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy()\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N/batch_size + batch_idx\n",
    "        if niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h],\n",
    "                                                     selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "#         if True:\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N/batch_size, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "#             writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*N/batch_size)\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = LSTMFeatures(dims_num, 16 * dims_num, 4, lstm_dropout=0.0)\n",
    "model = LSTMMultiHeadClassifier(features, 16 * dims_num, numc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [410.0, 401.0, 400.0, 398.99, 397.99, 396.98, 395.98, 394.97, 393.97, 392.96, 391.95, 390.95, 389.94, 388.94, 387.93, 386.93, 385.92, 384.92, 383.91, 382.91, 381.9, 380.9, 379.89, 378.89, 377.88, 376.88, 375.87, 374.87, 373.86, 372.86, 371.85, 370.85, 369.84, 368.84, 367.83, 366.83, 365.82, 364.82, 363.81, 362.81, 361.8, 360.8, 359.79, 358.79, 357.78, 356.78, 355.77, 354.77, 353.76, 352.76, 351.75, 350.75, 349.74, 348.74, 347.73, 346.73, 345.72, 344.72, 343.71, 342.71, 341.7, 340.7, 339.69, 338.69, 337.68, 336.68, 335.67, 334.67, 333.66, 332.66, 331.65, 330.65, 329.64, 328.64, 327.63, 326.63, 325.62, 324.62, 323.61, 322.61, 321.6, 320.6, 319.59, 318.59, 317.58, 316.58, 315.57, 314.57, 313.56, 312.56, 311.55, 310.55, 309.54, 308.54, 307.53, 306.53, 305.52, 304.52, 303.51, 302.51, 301.5, 300.5, 299.49, 298.49, 297.48, 296.48, 295.47, 294.47, 293.46, 292.46, 291.45, 290.45, 289.44, 288.44, 287.43, 286.43, 285.42, 284.42, 283.41, 282.41, 281.4, 280.4, 279.39, 278.39, 277.38, 276.38, 275.37, 274.37, 273.36, 272.36, 271.35, 270.35, 269.34, 268.34, 267.33, 266.33, 265.32, 264.32, 263.31, 262.31, 261.3, 260.3, 259.29, 258.29, 257.28, 256.28, 255.27, 254.27, 253.26, 252.26, 251.25, 250.25, 249.24, 248.24, 247.23, 246.23, 245.22, 244.22, 243.21, 242.21, 241.2, 240.2, 239.19, 238.19, 237.18, 236.18, 235.17, 234.17, 233.16, 232.16, 231.15, 230.15, 229.14, 228.14, 227.13, 226.13, 225.12, 224.12, 223.11, 222.11, 221.1, 220.1, 219.09, 218.09, 217.08, 216.08, 215.07, 214.07, 213.06, 212.06, 211.05, 210.05, 209.04, 208.04, 207.03, 206.03, 205.02, 204.02, 203.01, 202.01, 201.0, 200.0, 198.99, 197.99, 196.98, 195.98, 194.97, 193.97, 192.96, 191.96, 190.95, 189.95, 188.94, 187.94, 186.93, 185.93, 184.92, 183.92, 182.91, 181.91, 180.9, 179.9, 178.89, 177.89, 176.88, 175.88, 174.87, 173.87, 172.86, 171.86, 170.85, 169.85, 168.84, 167.84, 166.83, 165.83, 164.82, 163.82, 162.81, 161.81, 160.8, 159.8, 158.79, 157.79, 156.78, 155.78, 154.77, 153.77, 152.76, 151.76, 150.75, 149.75, 148.74, 147.74, 146.73, 145.73, 144.72, 143.72, 142.71, 141.71, 140.7, 139.7, 138.69, 137.69, 136.68, 135.68, 134.67, 133.67, 132.66, 131.66, 130.65, 129.65, 128.64, 127.64, 126.63, 125.63, 124.62, 123.62, 122.61, 121.61, 120.6, 119.6, 118.59, 117.59, 116.58, 115.58, 114.57, 113.57, 112.56, 111.56, 110.55, 109.55, 108.54, 107.54, 106.53, 105.53, 104.52, 103.52, 102.51, 101.51, 100.5, 99.5, 98.49, 97.49, 96.48, 95.48, 94.47, 93.47, 92.46, 91.46, 90.45, 89.45, 88.44, 87.44, 86.43, 85.43, 84.42, 83.42, 82.41, 81.41, 80.4, 79.4, 78.39, 77.39, 76.38, 75.38, 74.37, 73.37, 72.36, 71.36, 70.35, 69.35, 68.34, 67.34, 66.33, 65.33, 64.32, 63.32, 62.31, 61.31, 60.3, 59.3, 58.29, 57.29, 56.28, 55.28, 54.27, 53.27, 52.26, 51.26, 50.25, 49.25, 48.24, 47.24, 46.23, 45.23, 44.22, 43.22, 42.21, 41.21, 40.2, 39.2, 38.19, 37.19, 36.18, 35.18, 34.17, 33.17, 32.16, 31.16, 30.15, 29.15, 28.14, 27.14, 26.13, 25.13, 24.12, 23.12, 22.11, 21.11, 20.1, 19.1, 18.09, 17.09, 16.08, 15.08, 14.07, 13.07, 12.06, 11.06, 10.05, 9.05, 8.04, 7.04, 6.03, 5.03, 4.02, 3.02, 2.01, 1.01, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LSTM\"\n",
    "writer = SummaryWriter(f'./runs/pendigits/{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(net, K, sigma=0.1, dim=128, use_pca=False):\n",
    "    net.eval()\n",
    "    # this part is ugly but made to be backwards-compatible. there was a change in cifar dataset's structure.\n",
    "    trainLabels = y_train\n",
    "    LEN = N\n",
    "    C = trainLabels.max() + 1\n",
    "\n",
    "    trainFeatures = torch.zeros((magic_dim, LEN))  # , device='cuda:0') # dim\n",
    "    normalize = Normalize()\n",
    "    normalize.to(device)\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=False)):\n",
    "        batchSize = batch_size\n",
    "        inputs = inputs.to(device)\n",
    "        features = net(inputs.float())\n",
    "        if not use_pca:\n",
    "            features = normalize(features)\n",
    "        tmp = trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize]\n",
    "        trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu()\n",
    "        \n",
    "    if use_pca:\n",
    "        comps = dim\n",
    "        print('doing PCA with %s components'%comps, end=' ')\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=comps, whiten=False)\n",
    "        trainFeatures = pca.fit_transform(trainFeatures.numpy().T)\n",
    "        trainFeatures = torch.Tensor(trainFeatures).to(device)\n",
    "        trainFeatures = normalize(trainFeatures).t().cpu()\n",
    "        print('..done')\n",
    "    def eval_k_s(K_,sigma_):\n",
    "        total = 0\n",
    "        top1 = 0.\n",
    "#         top5 = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            retrieval_one_hot = torch.zeros(K_, C)# .cuda()\n",
    "            for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=False)):\n",
    "                targets = targets # .cuda(async=True) # or without async for py3.7\n",
    "                inputs = inputs.to(device)\n",
    "                batchSize = batch_size\n",
    "                features = net(inputs.float())\n",
    "                if use_pca:\n",
    "                    features = pca.transform(features.cpu().numpy())\n",
    "                    features = torch.Tensor(features).to(device)\n",
    "                features = normalize(features).cpu()\n",
    "\n",
    "                dist = torch.mm(features, trainFeatures)\n",
    "\n",
    "                yd, yi = dist.topk(K_, dim=1, largest=True, sorted=True)\n",
    "                candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "                retrieval = torch.gather(candidates, 1, yi).long()\n",
    "\n",
    "                retrieval_one_hot.resize_(batchSize * K_, C).zero_()\n",
    "                retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1.)\n",
    "                \n",
    "                yd_transform = yd.clone().div_(sigma_).exp_()\n",
    "                probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C),\n",
    "                                            yd_transform.view(batchSize, -1, 1)),\n",
    "                                  1)\n",
    "                _, predictions = probs.sort(1, True)\n",
    "\n",
    "                # Find which predictions match the target\n",
    "                correct = predictions.eq(targets.data.view(-1, 1))\n",
    "\n",
    "                top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "#                 top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        print(f\"{K_}-NN,s={sigma_}: TOP1: \", top1 * 100. / total)\n",
    "        return top1 / total\n",
    "\n",
    "    if isinstance(K, list):\n",
    "        res = []\n",
    "        for K_ in K:\n",
    "            for sigma_ in sigma:\n",
    "                res.append(eval_k_s(K_, sigma_))\n",
    "        return res\n",
    "    else:\n",
    "        res = eval_k_s(K, sigma)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "LSTM\n",
      "error:  5.840405971069274e-07 step  21\n",
      "cost:  2.0644587053760888\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [0][0/14.988]Time: 0.396 (0.396) Data: 0.334 (0.334) Loss: 2.3307 (2.3307)\n",
      "Epoch: [0][10/14.988]Time: 0.014 (0.051) Data: 0.001 (0.031) Loss: 2.3105 (2.3169)\n",
      "10-NN,s=0.1: TOP1:  27.4\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  26.9\n",
      "50-NN,s=0.5: TOP1:  27.766666666666666\n",
      "10-NN,s=0.1: TOP1:  24.766666666666666\n",
      "10-NN,s=0.5: TOP1:  23.4\n",
      "best accuracy: 27.40\n",
      "\n",
      "Epoch: 1\n",
      "LSTM\n",
      "Epoch: [1][0/14.988]Time: 0.025 (0.025) Data: 0.004 (0.004) Loss: 2.3053 (2.3053)\n",
      "error:  2.8981607114175745e-09 step  11\n",
      "cost:  2.2061343142566314\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [1][10/14.988]Time: 0.014 (0.033) Data: 0.002 (0.017) Loss: 2.2985 (2.3030)\n",
      "10-NN,s=0.1: TOP1:  27.733333333333334\n",
      "Saving..\n",
      "best accuracy: 27.73\n",
      "\n",
      "Epoch: 2\n",
      "LSTM\n",
      "Epoch: [2][0/14.988]Time: 0.017 (0.017) Data: 0.001 (0.001) Loss: 2.2991 (2.2991)\n",
      "error:  7.986944439153376e-13 step  11\n",
      "cost:  2.248551555289554\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [2][10/14.988]Time: 0.020 (0.043) Data: 0.001 (0.019) Loss: 2.2953 (2.2981)\n",
      "10-NN,s=0.1: TOP1:  31.0\n",
      "Saving..\n",
      "best accuracy: 31.00\n",
      "\n",
      "Epoch: 3\n",
      "LSTM\n",
      "Epoch: [3][0/14.988]Time: 0.017 (0.017) Data: 0.001 (0.001) Loss: 2.2965 (2.2965)\n",
      "error:  2.5757762589506683e-10 step  11\n",
      "cost:  2.249184707367225\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [3][10/14.988]Time: 0.018 (0.031) Data: 0.000 (0.014) Loss: 2.2786 (2.2876)\n",
      "10-NN,s=0.1: TOP1:  30.766666666666666\n",
      "best accuracy: 31.00\n",
      "\n",
      "Epoch: 4\n",
      "LSTM\n",
      "Epoch: [4][0/14.988]Time: 0.021 (0.021) Data: 0.002 (0.002) Loss: 2.2760 (2.2760)\n",
      "error:  1.3776711320634405e-06 step  11\n",
      "cost:  2.2348043644395736\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [4][10/14.988]Time: 0.021 (0.032) Data: 0.001 (0.013) Loss: 2.2326 (2.2480)\n",
      "10-NN,s=0.1: TOP1:  41.56666666666667\n",
      "Saving..\n",
      "best accuracy: 41.57\n",
      "\n",
      "Epoch: 5\n",
      "LSTM\n",
      "Epoch: [5][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 2.2258 (2.2258)\n",
      "error:  7.81002057470559e-08 step  11\n",
      "cost:  2.250894216330678\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [5][10/14.988]Time: 0.014 (0.026) Data: 0.000 (0.009) Loss: 2.1599 (2.1915)\n",
      "10-NN,s=0.1: TOP1:  42.333333333333336\n",
      "Saving..\n",
      "best accuracy: 42.33\n",
      "\n",
      "Epoch: 6\n",
      "LSTM\n",
      "Epoch: [6][0/14.988]Time: 0.017 (0.017) Data: 0.001 (0.001) Loss: 2.1517 (2.1517)\n",
      "error:  0.007702832570929075 step  11\n",
      "cost:  2.193638358905393\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [6][10/14.988]Time: 0.015 (0.043) Data: 0.001 (0.020) Loss: 2.1134 (2.1193)\n",
      "10-NN,s=0.1: TOP1:  41.233333333333334\n",
      "best accuracy: 42.33\n",
      "\n",
      "Epoch: 7\n",
      "LSTM\n",
      "Epoch: [7][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 2.1195 (2.1195)\n",
      "error:  0.00012210323478245577 step  11\n",
      "cost:  2.2196131830887738\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [7][10/14.988]Time: 0.015 (0.037) Data: 0.001 (0.018) Loss: 2.0248 (2.0684)\n",
      "10-NN,s=0.1: TOP1:  48.166666666666664\n",
      "Saving..\n",
      "best accuracy: 48.17\n",
      "\n",
      "Epoch: 8\n",
      "LSTM\n",
      "Epoch: [8][0/14.988]Time: 0.017 (0.017) Data: 0.001 (0.001) Loss: 2.0089 (2.0089)\n",
      "error:  3.892484314393929e-05 step  11\n",
      "cost:  2.199945556120622\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [8][10/14.988]Time: 0.015 (0.023) Data: 0.001 (0.008) Loss: 1.9192 (1.9559)\n",
      "10-NN,s=0.1: TOP1:  45.2\n",
      "best accuracy: 48.17\n",
      "\n",
      "Epoch: 9\n",
      "LSTM\n",
      "Epoch: [9][0/14.988]Time: 0.016 (0.016) Data: 0.002 (0.002) Loss: 1.8996 (1.8996)\n",
      "error:  0.005254005065225886 step  11\n",
      "cost:  2.1827097597840304\n",
      "opt took 0.00min,   11iters\n",
      "Epoch: [9][10/14.988]Time: 0.015 (0.022) Data: 0.001 (0.007) Loss: 1.7720 (1.8324)\n",
      "10-NN,s=0.1: TOP1:  45.1\n",
      "best accuracy: 48.17\n",
      "\n",
      "Epoch: 10\n",
      "LSTM\n",
      "Epoch: [10][0/14.988]Time: 0.017 (0.017) Data: 0.001 (0.001) Loss: 1.7586 (1.7586)\n",
      "error:  1.4489083540736303e-05 step  21\n",
      "cost:  2.1564039867169877\n",
      "opt took 0.00min,   21iters\n",
      "Epoch: [10][10/14.988]Time: 0.020 (0.036) Data: 0.001 (0.022) Loss: 1.6595 (1.7263)\n",
      "10-NN,s=0.1: TOP1:  48.766666666666666\n",
      "Saving..\n",
      "best accuracy: 48.77\n",
      "\n",
      "Epoch: 11\n",
      "LSTM\n",
      "Epoch: [11][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.6949 (1.6949)\n",
      "error:  0.00023818518251450538 step  31\n",
      "cost:  2.0866111500506395\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [11][10/14.988]Time: 0.016 (0.030) Data: 0.001 (0.014) Loss: 1.6003 (1.6383)\n",
      "10-NN,s=0.1: TOP1:  50.3\n",
      "Saving..\n",
      "best accuracy: 50.30\n",
      "\n",
      "Epoch: 12\n",
      "LSTM\n",
      "Epoch: [12][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.5555 (1.5555)\n",
      "error:  7.7199113470372e-05 step  31\n",
      "cost:  2.087986581853571\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [12][10/14.988]Time: 0.015 (0.028) Data: 0.001 (0.012) Loss: 1.4882 (1.4991)\n",
      "10-NN,s=0.1: TOP1:  46.833333333333336\n",
      "best accuracy: 50.30\n",
      "\n",
      "Epoch: 13\n",
      "LSTM\n",
      "Epoch: [13][0/14.988]Time: 0.017 (0.017) Data: 0.002 (0.002) Loss: 1.4315 (1.4315)\n",
      "error:  0.009063228410418067 step  101\n",
      "cost:  1.825704393101548\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [13][10/14.988]Time: 0.018 (0.046) Data: 0.003 (0.024) Loss: 1.3470 (1.3896)\n",
      "10-NN,s=0.1: TOP1:  50.333333333333336\n",
      "Saving..\n",
      "best accuracy: 50.33\n",
      "\n",
      "Epoch: 14\n",
      "LSTM\n",
      "Epoch: [14][0/14.988]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 1.3535 (1.3535)\n",
      "error:  0.008548903889703752 step  251\n",
      "cost:  1.6325557341956878\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [14][10/14.988]Time: 0.015 (0.045) Data: 0.001 (0.023) Loss: 1.3503 (1.3313)\n",
      "10-NN,s=0.1: TOP1:  50.56666666666667\n",
      "Saving..\n",
      "best accuracy: 50.57\n",
      "\n",
      "Epoch: 15\n",
      "LSTM\n",
      "Epoch: [15][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.2767 (1.2767)\n",
      "error:  0.007006588502700084 step  181\n",
      "cost:  1.4782798975781024\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [15][10/14.988]Time: 0.016 (0.026) Data: 0.001 (0.010) Loss: 1.2471 (1.2729)\n",
      "10-NN,s=0.1: TOP1:  47.833333333333336\n",
      "best accuracy: 50.57\n",
      "\n",
      "Epoch: 16\n",
      "LSTM\n",
      "Epoch: [16][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.2495 (1.2495)\n",
      "error:  0.008776930446041886 step  521\n",
      "cost:  1.4544718047613654\n",
      "opt took 0.00min,  521iters\n",
      "Epoch: [16][10/14.988]Time: 0.011 (0.044) Data: 0.001 (0.029) Loss: 1.2664 (1.2696)\n",
      "10-NN,s=0.1: TOP1:  48.86666666666667\n",
      "best accuracy: 50.57\n",
      "\n",
      "Epoch: 17\n",
      "LSTM\n",
      "Epoch: [17][0/14.988]Time: 0.019 (0.019) Data: 0.002 (0.002) Loss: 1.2413 (1.2413)\n",
      "error:  0.005755941660224195 step  201\n",
      "cost:  1.2506037892065551\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [17][10/14.988]Time: 0.015 (0.036) Data: 0.001 (0.018) Loss: 1.2072 (1.2074)\n",
      "10-NN,s=0.1: TOP1:  51.8\n",
      "Saving..\n",
      "best accuracy: 51.80\n",
      "\n",
      "Epoch: 18\n",
      "LSTM\n",
      "Epoch: [18][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 1.2793 (1.2793)\n",
      "error:  0.007990716599797887 step  331\n",
      "cost:  1.205607560230114\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [18][10/14.988]Time: 0.016 (0.042) Data: 0.001 (0.023) Loss: 1.2701 (1.2641)\n",
      "10-NN,s=0.1: TOP1:  49.43333333333333\n",
      "best accuracy: 51.80\n",
      "\n",
      "Epoch: 19\n",
      "LSTM\n",
      "Epoch: [19][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.1946 (1.1946)\n",
      "error:  0.008900727382380014 step  471\n",
      "cost:  1.248863479221358\n",
      "opt took 0.01min,  471iters\n",
      "Epoch: [19][10/14.988]Time: 0.017 (0.065) Data: 0.001 (0.048) Loss: 1.1888 (1.2304)\n",
      "10-NN,s=0.1: TOP1:  50.233333333333334\n",
      "best accuracy: 51.80\n",
      "\n",
      "Epoch: 20\n",
      "LSTM\n",
      "Epoch: [20][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.2230 (1.2230)\n",
      "error:  0.009661493477851502 step  401\n",
      "cost:  1.0798530355568596\n",
      "opt took 0.00min,  401iters\n",
      "Epoch: [20][10/14.988]Time: 0.014 (0.030) Data: 0.001 (0.010) Loss: 1.1765 (1.1995)\n",
      "10-NN,s=0.1: TOP1:  50.2\n",
      "best accuracy: 51.80\n",
      "\n",
      "Epoch: 21\n",
      "LSTM\n",
      "Epoch: [21][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.1474 (1.1474)\n",
      "error:  0.009636511036757445 step  511\n",
      "cost:  0.8413926414956059\n",
      "opt took 0.00min,  511iters\n",
      "Epoch: [21][10/14.988]Time: 0.015 (0.054) Data: 0.001 (0.035) Loss: 1.1823 (1.1906)\n",
      "10-NN,s=0.1: TOP1:  46.833333333333336\n",
      "best accuracy: 51.80\n",
      "\n",
      "Epoch: 22\n",
      "LSTM\n",
      "Epoch: [22][0/14.988]Time: 0.020 (0.020) Data: 0.001 (0.001) Loss: 1.2240 (1.2240)\n",
      "error:  0.008186738115393277 step  291\n",
      "cost:  0.9108538852843879\n",
      "opt took 0.00min,  291iters\n",
      "Epoch: [22][10/14.988]Time: 0.013 (0.043) Data: 0.000 (0.024) Loss: 1.2026 (1.2002)\n",
      "10-NN,s=0.1: TOP1:  50.56666666666667\n",
      "best accuracy: 51.80\n",
      "\n",
      "Epoch: 23\n",
      "LSTM\n",
      "Epoch: [23][0/14.988]Time: 0.017 (0.017) Data: 0.001 (0.001) Loss: 1.1909 (1.1909)\n",
      "error:  0.009522422818584841 step  371\n",
      "cost:  0.8417673806486388\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [23][10/14.988]Time: 0.024 (0.049) Data: 0.001 (0.029) Loss: 1.1255 (1.1544)\n",
      "10-NN,s=0.1: TOP1:  51.96666666666667\n",
      "Saving..\n",
      "best accuracy: 51.97\n",
      "\n",
      "Epoch: 24\n",
      "LSTM\n",
      "Epoch: [24][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.1309 (1.1309)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.008363911712195526 step  271\n",
      "cost:  0.8741472291597109\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [24][10/14.988]Time: 0.015 (0.047) Data: 0.001 (0.032) Loss: 1.1647 (1.1696)\n",
      "10-NN,s=0.1: TOP1:  54.766666666666666\n",
      "Saving..\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 25\n",
      "LSTM\n",
      "Epoch: [25][0/14.988]Time: 0.016 (0.016) Data: 0.002 (0.002) Loss: 1.2367 (1.2367)\n",
      "error:  0.007412689448982435 step  281\n",
      "cost:  0.9403128305385614\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [25][10/14.988]Time: 0.017 (0.038) Data: 0.001 (0.018) Loss: 1.1301 (1.1963)\n",
      "10-NN,s=0.1: TOP1:  51.766666666666666\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 26\n",
      "LSTM\n",
      "Epoch: [26][0/14.988]Time: 0.015 (0.015) Data: 0.001 (0.001) Loss: 1.1051 (1.1051)\n",
      "error:  0.009186758990635702 step  321\n",
      "cost:  0.8559262571843768\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [26][10/14.988]Time: 0.015 (0.046) Data: 0.001 (0.027) Loss: 1.1381 (1.1105)\n",
      "10-NN,s=0.1: TOP1:  50.5\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 27\n",
      "LSTM\n",
      "Epoch: [27][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 1.1218 (1.1218)\n",
      "error:  0.00922635755022072 step  341\n",
      "cost:  0.7544099486493\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [27][10/14.988]Time: 0.015 (0.048) Data: 0.001 (0.029) Loss: 1.1504 (1.1393)\n",
      "10-NN,s=0.1: TOP1:  50.5\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 28\n",
      "LSTM\n",
      "Epoch: [28][0/14.988]Time: 0.024 (0.024) Data: 0.002 (0.002) Loss: 1.0828 (1.0828)\n",
      "error:  0.008644085950634706 step  371\n",
      "cost:  0.7577642879490805\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [28][10/14.988]Time: 0.015 (0.061) Data: 0.001 (0.041) Loss: 1.0655 (1.0690)\n",
      "10-NN,s=0.1: TOP1:  51.666666666666664\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 29\n",
      "LSTM\n",
      "Epoch: [29][0/14.988]Time: 0.016 (0.016) Data: 0.002 (0.002) Loss: 1.0778 (1.0778)\n",
      "error:  0.009445862745015576 step  441\n",
      "cost:  0.7613146265633569\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [29][10/14.988]Time: 0.012 (0.042) Data: 0.000 (0.023) Loss: 1.0358 (1.0617)\n",
      "10-NN,s=0.1: TOP1:  53.4\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 30\n",
      "LSTM\n",
      "Epoch: [30][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 1.0788 (1.0788)\n",
      "error:  0.009897056743522037 step  391\n",
      "cost:  0.7054479179856019\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [30][10/14.988]Time: 0.021 (0.039) Data: 0.001 (0.023) Loss: 1.0073 (1.0450)\n",
      "10-NN,s=0.1: TOP1:  53.733333333333334\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 31\n",
      "LSTM\n",
      "Epoch: [31][0/14.988]Time: 0.017 (0.017) Data: 0.001 (0.001) Loss: 1.0617 (1.0617)\n",
      "error:  0.008486097688040872 step  491\n",
      "cost:  0.750929415282867\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [31][10/14.988]Time: 0.021 (0.069) Data: 0.001 (0.047) Loss: 1.0416 (1.0710)\n",
      "10-NN,s=0.1: TOP1:  50.13333333333333\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 32\n",
      "LSTM\n",
      "Epoch: [32][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 1.0657 (1.0657)\n",
      "error:  0.009115690594593606 step  341\n",
      "cost:  0.6582099387182451\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [32][10/14.988]Time: 0.015 (0.042) Data: 0.001 (0.023) Loss: 1.0374 (1.0378)\n",
      "10-NN,s=0.1: TOP1:  50.06666666666667\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 33\n",
      "LSTM\n",
      "Epoch: [33][0/14.988]Time: 0.020 (0.020) Data: 0.003 (0.003) Loss: 1.0297 (1.0297)\n",
      "error:  0.009657355852975669 step  361\n",
      "cost:  0.6808841393536167\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [33][10/14.988]Time: 0.015 (0.058) Data: 0.001 (0.042) Loss: 1.0217 (1.0440)\n",
      "10-NN,s=0.1: TOP1:  52.56666666666667\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 34\n",
      "LSTM\n",
      "Epoch: [34][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 0.9891 (0.9891)\n",
      "error:  0.007882473274680946 step  331\n",
      "cost:  0.7254983823881826\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [34][10/14.988]Time: 0.011 (0.029) Data: 0.001 (0.016) Loss: 1.0667 (1.0358)\n",
      "10-NN,s=0.1: TOP1:  51.0\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 35\n",
      "LSTM\n",
      "Epoch: [35][0/14.988]Time: 0.018 (0.018) Data: 0.002 (0.002) Loss: 1.0729 (1.0729)\n",
      "error:  0.009336985311112933 step  271\n",
      "cost:  0.7777883521302353\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [35][10/14.988]Time: 0.021 (0.045) Data: 0.001 (0.022) Loss: 0.9746 (1.0238)\n",
      "10-NN,s=0.1: TOP1:  54.46666666666667\n",
      "best accuracy: 54.77\n",
      "\n",
      "Epoch: 36\n",
      "LSTM\n",
      "Epoch: [36][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 0.9887 (0.9887)\n",
      "error:  0.007250239560543159 step  301\n",
      "cost:  0.7495135931363195\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [36][10/14.988]Time: 0.015 (0.050) Data: 0.001 (0.027) Loss: 1.0156 (1.0230)\n",
      "10-NN,s=0.1: TOP1:  56.06666666666667\n",
      "Saving..\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 37\n",
      "LSTM\n",
      "Epoch: [37][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 0.9818 (0.9818)\n",
      "error:  0.0080520230897555 step  361\n",
      "cost:  0.7280076400999258\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [37][10/14.988]Time: 0.025 (0.047) Data: 0.001 (0.025) Loss: 1.0321 (1.0063)\n",
      "10-NN,s=0.1: TOP1:  52.6\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 38\n",
      "LSTM\n",
      "Epoch: [38][0/14.988]Time: 0.015 (0.015) Data: 0.004 (0.004) Loss: 1.0072 (1.0072)\n",
      "error:  0.00764711190608347 step  271\n",
      "cost:  0.6873069525999793\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [38][10/14.988]Time: 0.024 (0.041) Data: 0.003 (0.020) Loss: 1.0664 (1.0401)\n",
      "10-NN,s=0.1: TOP1:  53.8\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 39\n",
      "LSTM\n",
      "Epoch: [39][0/14.988]Time: 0.015 (0.015) Data: 0.002 (0.002) Loss: 1.0396 (1.0396)\n",
      "error:  0.0072246209153628005 step  301\n",
      "cost:  0.6945340294024285\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [39][10/14.988]Time: 0.015 (0.035) Data: 0.001 (0.017) Loss: 1.0006 (1.0189)\n",
      "10-NN,s=0.1: TOP1:  53.1\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 40\n",
      "LSTM\n",
      "Epoch: [40][0/14.988]Time: 0.021 (0.021) Data: 0.002 (0.002) Loss: 0.9925 (0.9925)\n",
      "error:  0.007793177510309168 step  311\n",
      "cost:  0.7104943775067339\n",
      "opt took 0.00min,  311iters\n",
      "Epoch: [40][10/14.988]Time: 0.019 (0.046) Data: 0.001 (0.026) Loss: 0.9729 (0.9770)\n",
      "10-NN,s=0.1: TOP1:  51.53333333333333\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 41\n",
      "LSTM\n",
      "Epoch: [41][0/14.988]Time: 0.018 (0.018) Data: 0.001 (0.001) Loss: 1.0120 (1.0120)\n",
      "error:  0.008378614766812786 step  361\n",
      "cost:  0.6511966163376033\n",
      "opt took 0.01min,  361iters\n",
      "Epoch: [41][10/14.988]Time: 0.015 (0.071) Data: 0.001 (0.047) Loss: 0.9901 (0.9877)\n",
      "10-NN,s=0.1: TOP1:  54.766666666666666\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 42\n",
      "LSTM\n",
      "Epoch: [42][0/14.988]Time: 0.016 (0.016) Data: 0.002 (0.002) Loss: 0.9823 (0.9823)\n",
      "error:  0.009269612690852691 step  431\n",
      "cost:  0.628448584107658\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [42][10/14.988]Time: 0.026 (0.059) Data: 0.001 (0.036) Loss: 0.9958 (0.9811)\n",
      "10-NN,s=0.1: TOP1:  52.43333333333333\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 43\n",
      "LSTM\n",
      "Epoch: [43][0/14.988]Time: 0.018 (0.018) Data: 0.003 (0.003) Loss: 1.0856 (1.0856)\n",
      "error:  0.009596130509498924 step  331\n",
      "cost:  0.6164316786126456\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [43][10/14.988]Time: 0.016 (0.053) Data: 0.001 (0.034) Loss: 1.0436 (1.0151)\n",
      "10-NN,s=0.1: TOP1:  54.833333333333336\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 44\n",
      "LSTM\n",
      "Epoch: [44][0/14.988]Time: 0.027 (0.027) Data: 0.004 (0.004) Loss: 0.9500 (0.9500)\n",
      "error:  0.008264526961662355 step  321\n",
      "cost:  0.6482874606183874\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [44][10/14.988]Time: 0.015 (0.035) Data: 0.001 (0.018) Loss: 0.9568 (0.9847)\n",
      "10-NN,s=0.1: TOP1:  52.666666666666664\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 45\n",
      "LSTM\n",
      "Epoch: [45][0/14.988]Time: 0.037 (0.037) Data: 0.011 (0.011) Loss: 0.9287 (0.9287)\n",
      "error:  0.00815035956120913 step  351\n",
      "cost:  0.7145417586472496\n",
      "opt took 0.00min,  351iters\n",
      "Epoch: [45][10/14.988]Time: 0.016 (0.054) Data: 0.001 (0.033) Loss: 0.9601 (0.9674)\n",
      "10-NN,s=0.1: TOP1:  53.86666666666667\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 46\n",
      "LSTM\n",
      "Epoch: [46][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 0.9734 (0.9734)\n",
      "error:  0.00922096520580351 step  331\n",
      "cost:  0.6632841142680757\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [46][10/14.988]Time: 0.016 (0.048) Data: 0.001 (0.028) Loss: 0.9399 (0.9862)\n",
      "10-NN,s=0.1: TOP1:  54.666666666666664\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 47\n",
      "LSTM\n",
      "Epoch: [47][0/14.988]Time: 0.017 (0.017) Data: 0.002 (0.002) Loss: 0.9466 (0.9466)\n",
      "error:  0.008645398772064383 step  301\n",
      "cost:  0.6618792521695033\n",
      "opt took 0.01min,  301iters\n",
      "Epoch: [47][10/14.988]Time: 0.021 (0.071) Data: 0.001 (0.048) Loss: 0.9334 (0.9328)\n",
      "10-NN,s=0.1: TOP1:  54.266666666666666\n",
      "best accuracy: 56.07\n",
      "\n",
      "Epoch: 48\n",
      "LSTM\n",
      "Epoch: [48][0/14.988]Time: 0.015 (0.015) Data: 0.001 (0.001) Loss: 0.9317 (0.9317)\n",
      "error:  0.008494866877649221 step  451\n",
      "cost:  0.6210099752085445\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [48][10/14.988]Time: 0.016 (0.053) Data: 0.001 (0.037) Loss: 0.9880 (0.9980)\n",
      "10-NN,s=0.1: TOP1:  60.8\n",
      "Saving..\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 49\n",
      "LSTM\n",
      "Epoch: [49][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9939 (0.9939)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009403687783453796 step  471\n",
      "cost:  0.6223744331517991\n",
      "opt took 0.00min,  471iters\n",
      "Epoch: [49][10/14.988]Time: 0.019 (0.050) Data: 0.001 (0.030) Loss: 0.9417 (0.9811)\n",
      "10-NN,s=0.1: TOP1:  56.333333333333336\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 50\n",
      "LSTM\n",
      "Epoch: [50][0/14.988]Time: 0.017 (0.017) Data: 0.002 (0.002) Loss: 0.9132 (0.9132)\n",
      "error:  0.009544959990712587 step  401\n",
      "cost:  0.626328328342588\n",
      "opt took 0.00min,  401iters\n",
      "Epoch: [50][10/14.988]Time: 0.017 (0.031) Data: 0.001 (0.012) Loss: 0.9941 (0.9960)\n",
      "10-NN,s=0.1: TOP1:  59.63333333333333\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  56.06666666666667\n",
      "50-NN,s=0.5: TOP1:  56.666666666666664\n",
      "10-NN,s=0.1: TOP1:  59.3\n",
      "10-NN,s=0.5: TOP1:  60.43333333333333\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 51\n",
      "LSTM\n",
      "Epoch: [51][0/14.988]Time: 0.015 (0.015) Data: 0.001 (0.001) Loss: 0.9473 (0.9473)\n",
      "error:  0.009248435786569309 step  691\n",
      "cost:  0.6192385576215909\n",
      "opt took 0.01min,  691iters\n",
      "Epoch: [51][10/14.988]Time: 0.015 (0.071) Data: 0.001 (0.056) Loss: 0.9449 (0.9699)\n",
      "10-NN,s=0.1: TOP1:  55.8\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 52\n",
      "LSTM\n",
      "Epoch: [52][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.9561 (0.9561)\n",
      "error:  0.008288298245396453 step  381\n",
      "cost:  0.6260473046087425\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [52][10/14.988]Time: 0.014 (0.022) Data: 0.001 (0.010) Loss: 0.9524 (0.9778)\n",
      "10-NN,s=0.1: TOP1:  57.766666666666666\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 53\n",
      "LSTM\n",
      "Epoch: [53][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 1.0030 (1.0030)\n",
      "error:  0.009288794577613557 step  291\n",
      "cost:  0.6190309177913157\n",
      "opt took 0.00min,  291iters\n",
      "Epoch: [53][10/14.988]Time: 0.014 (0.023) Data: 0.001 (0.010) Loss: 0.9106 (0.9628)\n",
      "10-NN,s=0.1: TOP1:  57.46666666666667\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 54\n",
      "LSTM\n",
      "Epoch: [54][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9269 (0.9269)\n",
      "error:  0.008909764045463064 step  491\n",
      "cost:  0.6512263224678575\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [54][10/14.988]Time: 0.014 (0.027) Data: 0.001 (0.015) Loss: 0.9574 (0.9643)\n",
      "10-NN,s=0.1: TOP1:  58.13333333333333\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 55\n",
      "LSTM\n",
      "Epoch: [55][0/14.988]Time: 0.015 (0.015) Data: 0.002 (0.002) Loss: 0.9807 (0.9807)\n",
      "error:  0.008088704869887997 step  361\n",
      "cost:  0.6490102720092625\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [55][10/14.988]Time: 0.018 (0.026) Data: 0.001 (0.011) Loss: 0.9793 (0.9776)\n",
      "10-NN,s=0.1: TOP1:  55.6\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 56\n",
      "LSTM\n",
      "Epoch: [56][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.9484 (0.9484)\n",
      "error:  0.008336572395423403 step  371\n",
      "cost:  0.6416318743606596\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [56][10/14.988]Time: 0.015 (0.030) Data: 0.001 (0.016) Loss: 1.0130 (0.9799)\n",
      "10-NN,s=0.1: TOP1:  55.03333333333333\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 57\n",
      "LSTM\n",
      "Epoch: [57][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 1.0023 (1.0023)\n",
      "error:  0.008968597553087987 step  591\n",
      "cost:  0.6542134395491631\n",
      "opt took 0.00min,  591iters\n",
      "Epoch: [57][10/14.988]Time: 0.017 (0.038) Data: 0.001 (0.023) Loss: 0.9522 (0.9662)\n",
      "10-NN,s=0.1: TOP1:  53.9\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 58\n",
      "LSTM\n",
      "Epoch: [58][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9346 (0.9346)\n",
      "error:  0.008554422598257672 step  301\n",
      "cost:  0.6524234341133734\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [58][10/14.988]Time: 0.014 (0.022) Data: 0.001 (0.009) Loss: 1.0087 (0.9892)\n",
      "10-NN,s=0.1: TOP1:  58.333333333333336\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 59\n",
      "LSTM\n",
      "Epoch: [59][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 1.1109 (1.1109)\n",
      "error:  0.009455707753040277 step  621\n",
      "cost:  0.6614426598864939\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [59][10/14.988]Time: 0.017 (0.035) Data: 0.001 (0.021) Loss: 1.0383 (1.0333)\n",
      "10-NN,s=0.1: TOP1:  56.333333333333336\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 60\n",
      "LSTM\n",
      "Epoch: [60][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9525 (0.9525)\n",
      "error:  0.009478488425847309 step  301\n",
      "cost:  0.6478242694126503\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [60][10/14.988]Time: 0.015 (0.024) Data: 0.001 (0.009) Loss: 0.9924 (0.9983)\n",
      "10-NN,s=0.1: TOP1:  59.4\n",
      "best accuracy: 60.80\n",
      "\n",
      "Epoch: 61\n",
      "LSTM\n",
      "Epoch: [61][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.9869 (0.9869)\n",
      "error:  0.009297239113177258 step  471\n",
      "cost:  0.6205989202799078\n",
      "opt took 0.00min,  471iters\n",
      "Epoch: [61][10/14.988]Time: 0.017 (0.029) Data: 0.001 (0.013) Loss: 0.9216 (0.9369)\n",
      "10-NN,s=0.1: TOP1:  61.36666666666667\n",
      "Saving..\n",
      "best accuracy: 61.37\n",
      "\n",
      "Epoch: 62\n",
      "LSTM\n",
      "Epoch: [62][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.9712 (0.9712)\n",
      "error:  0.0077394375335142485 step  281\n",
      "cost:  0.5880116790886708\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [62][10/14.988]Time: 0.017 (0.025) Data: 0.001 (0.010) Loss: 0.9656 (0.9553)\n",
      "10-NN,s=0.1: TOP1:  60.36666666666667\n",
      "best accuracy: 61.37\n",
      "\n",
      "Epoch: 63\n",
      "LSTM\n",
      "Epoch: [63][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.9514 (0.9514)\n",
      "error:  0.008926913113611268 step  291\n",
      "cost:  0.612499564201079\n",
      "opt took 0.00min,  291iters\n",
      "Epoch: [63][10/14.988]Time: 0.012 (0.048) Data: 0.001 (0.031) Loss: 0.9911 (0.9772)\n",
      "10-NN,s=0.1: TOP1:  54.1\n",
      "best accuracy: 61.37\n",
      "\n",
      "Epoch: 64\n",
      "LSTM\n",
      "Epoch: [64][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.9623 (0.9623)\n",
      "error: ng head 4  0.008702165240082493 step  581\n",
      "cost:  0.6117147537064789\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [64][10/14.988]Time: 0.014 (0.027) Data: 0.001 (0.013) Loss: 0.9556 (0.9530)\n",
      "10-NN,s=0.1: TOP1:  63.1\n",
      "Saving..\n",
      "best accuracy: 63.10\n",
      "\n",
      "Epoch: 65\n",
      "LSTM\n",
      "Epoch: [65][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9876 (0.9876)\n",
      "error:  0.00945117905086823 step  541\n",
      "cost:  0.6522622008547324\n",
      "opt took 0.00min,  541iters\n",
      "Epoch: [65][10/14.988]Time: 0.013 (0.028) Data: 0.001 (0.012) Loss: 0.9065 (0.9824)\n",
      "10-NN,s=0.1: TOP1:  58.7\n",
      "best accuracy: 63.10\n",
      "\n",
      "Epoch: 66\n",
      "LSTM\n",
      "Epoch: [66][0/14.988]Time: 0.016 (0.016) Data: 0.002 (0.002) Loss: 0.9647 (0.9647)\n",
      "error:  0.008317162769587716 step  451\n",
      "cost:  0.6276608971625588\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [66][10/14.988]Time: 0.017 (0.027) Data: 0.001 (0.012) Loss: 0.9380 (0.9552)\n",
      "10-NN,s=0.1: TOP1:  60.833333333333336\n",
      "best accuracy: 63.10\n",
      "\n",
      "Epoch: 67\n",
      "LSTM\n",
      "Epoch: [67][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9102 (0.9102)\n",
      "error:  0.009780184214119192 step  451\n",
      "cost:  0.5919683750664085\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [67][10/14.988]Time: 0.014 (0.022) Data: 0.001 (0.010) Loss: 0.8903 (0.9293)\n",
      "10-NN,s=0.1: TOP1:  62.93333333333333\n",
      "best accuracy: 63.10\n",
      "\n",
      "Epoch: 68\n",
      "LSTM\n",
      "Epoch: [68][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.9363 (0.9363)\n",
      "error:  0.009648964747769773 step  431\n",
      "cost:  0.616802870881084\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [68][10/14.988]Time: 0.018 (0.033) Data: 0.001 (0.019) Loss: 0.9492 (0.9502)\n",
      "10-NN,s=0.1: TOP1:  58.53333333333333\n",
      "best accuracy: 63.10\n",
      "\n",
      "Epoch: 69\n",
      "LSTM\n",
      "Epoch: [69][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.9446 (0.9446)\n",
      "error:  0.009907383222305888 step  531\n",
      "cost:  0.5921907295908501\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [69][10/14.988]Time: 0.017 (0.028) Data: 0.001 (0.014) Loss: 0.9317 (0.9291)\n",
      "10-NN,s=0.1: TOP1:  63.9\n",
      "Saving..\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 70\n",
      "LSTM\n",
      "Epoch: [70][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9589 (0.9589)\n",
      "error:  0.007713519217259823 step  371\n",
      "cost:  0.6228290205243631\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [70][10/14.988]Time: 0.014 (0.029) Data: 0.001 (0.017) Loss: 0.8842 (0.9232)\n",
      "10-NN,s=0.1: TOP1:  58.666666666666664\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 71\n",
      "LSTM\n",
      "Epoch: [71][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9051 (0.9051)\n",
      "error:  0.009303377613407271 step  461\n",
      "cost:  0.5792770074496375\n",
      "opt took 0.00min,  461iters\n",
      "Epoch: [71][10/14.988]Time: 0.018 (0.026) Data: 0.001 (0.013) Loss: 0.8450 (0.8861)\n",
      "10-NN,s=0.1: TOP1:  60.8\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 72\n",
      "LSTM\n",
      "Epoch: [72][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8849 (0.8849)\n",
      "error:  0.009536033487023476 step  641\n",
      "cost:  0.5638474383051137\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [72][10/14.988]Time: 0.015 (0.026) Data: 0.001 (0.013) Loss: 1.0072 (0.9044)\n",
      "10-NN,s=0.1: TOP1:  60.86666666666667\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 73\n",
      "LSTM\n",
      "Epoch: [73][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8959 (0.8959)\n",
      "error:  0.00946683849878116 step  321\n",
      "cost:  0.6347777116215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt took 0.00min,  321iters\n",
      "Epoch: [73][10/14.988]Time: 0.020 (0.033) Data: 0.001 (0.018) Loss: 0.9217 (0.9431)\n",
      "10-NN,s=0.1: TOP1:  61.266666666666666\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 74\n",
      "LSTM\n",
      "Epoch: [74][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9093 (0.9093)\n",
      "error:  0.009101520021954856 step  691\n",
      "cost:  0.5840362440897935\n",
      "opt took 0.00min,  691iters\n",
      "Epoch: [74][10/14.988]Time: 0.015 (0.028) Data: 0.001 (0.015) Loss: 0.9536 (0.9668)\n",
      "10-NN,s=0.1: TOP1:  61.86666666666667\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 75\n",
      "LSTM\n",
      "Epoch: [75][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 1.0090 (1.0090)\n",
      "error:  0.008333232159520532 step  601\n",
      "cost:  0.6911106684093993\n",
      "opt took 0.00min,  601iters\n",
      "Epoch: [75][10/14.988]Time: 0.025 (0.027) Data: 0.001 (0.012) Loss: 0.9631 (0.9765)\n",
      "10-NN,s=0.1: TOP1:  61.233333333333334\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 76\n",
      "LSTM\n",
      "Epoch: [76][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8759 (0.8759)\n",
      "error:  0.009436261938215007 step  411\n",
      "cost:  0.5872780324794064\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [76][10/14.988]Time: 0.015 (0.023) Data: 0.001 (0.011) Loss: 0.9138 (0.9024)\n",
      "10-NN,s=0.1: TOP1:  61.4\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 77\n",
      "LSTM\n",
      "Epoch: [77][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.8959 (0.8959)\n",
      "error:  0.00822928782060528 step  441\n",
      "cost:  0.606550102523708\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [77][10/14.988]Time: 0.017 (0.029) Data: 0.001 (0.015) Loss: 0.9668 (0.9090)\n",
      "10-NN,s=0.1: TOP1:  58.86666666666667\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 78\n",
      "LSTM\n",
      "Epoch: [78][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.9676 (0.9676)\n",
      "error:  0.009297955512985179 step  621\n",
      "cost:  0.6021622369164046\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [78][10/14.988]Time: 0.015 (0.029) Data: 0.001 (0.016) Loss: 0.9069 (0.9113)\n",
      "10-NN,s=0.1: TOP1:  58.766666666666666\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 79\n",
      "LSTM\n",
      "Epoch: [79][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9460 (0.9460)\n",
      "error:  0.009478884198043436 step  631\n",
      "cost:  0.5842376157015714\n",
      "opt took 0.00min,  631iters\n",
      "Epoch: [79][10/14.988]Time: 0.030 (0.030) Data: 0.001 (0.016) Loss: 0.9255 (0.9465)\n",
      "10-NN,s=0.1: TOP1:  58.63333333333333\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 80\n",
      "LSTM\n",
      "Epoch: [80][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8864 (0.8864)\n",
      "error:  0.009139609234250723 step  641\n",
      "cost:  0.5696122784749346\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [80][10/14.988]Time: 0.014 (0.024) Data: 0.001 (0.012) Loss: 0.8801 (0.8729)\n",
      "10-NN,s=0.1: TOP1:  58.766666666666666\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 81\n",
      "LSTM\n",
      "Epoch: [81][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.8393 (0.8393)\n",
      "error:  0.008210138385522758 step  431\n",
      "cost:  0.564209363972634\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [81][10/14.988]Time: 0.014 (0.021) Data: 0.001 (0.010) Loss: 0.8679 (0.8620)\n",
      "10-NN,s=0.1: TOP1:  60.06666666666667\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 82\n",
      "LSTM\n",
      "Epoch: [82][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.8645 (0.8645)\n",
      "error:  0.009989609260604548 step  601\n",
      "cost:  0.5801550378103032\n",
      "opt took 0.01min,  601iters\n",
      "Epoch: [82][10/14.988]Time: 0.015 (0.044) Data: 0.001 (0.032) Loss: 0.8906 (0.8848)\n",
      "10-NN,s=0.1: TOP1:  59.93333333333333\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 83\n",
      "LSTM\n",
      "Epoch: [83][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9354 (0.9354)\n",
      "error:  0.008501547780378926 step  631\n",
      "cost:  0.6068828644356168\n",
      "opt took 0.00min,  631iters\n",
      "Epoch: [83][10/14.988]Time: 0.015 (0.028) Data: 0.001 (0.017) Loss: 0.9217 (0.9339)\n",
      "10-NN,s=0.1: TOP1:  59.1\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 84\n",
      "LSTM\n",
      "Epoch: [84][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.8914 (0.8914)\n",
      "error:  0.008972741890275415 step  321\n",
      "cost:  0.5693490237707316\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [84][10/14.988]Time: 0.014 (0.022) Data: 0.001 (0.010) Loss: 0.9389 (0.9197)\n",
      "10-NN,s=0.1: TOP1:  62.5\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 85\n",
      "LSTM\n",
      "Epoch: [85][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8830 (0.8830)\n",
      "error:  0.009645329001072067 step  471\n",
      "cost:  0.600925831471208\n",
      "opt took 0.00min,  471iters\n",
      "Epoch: [85][10/14.988]Time: 0.014 (0.031) Data: 0.001 (0.020) Loss: 0.9534 (0.9065)\n",
      "10-NN,s=0.1: TOP1:  58.53333333333333\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 86\n",
      "LSTM\n",
      "Epoch: [86][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.9109 (0.9109)\n",
      "error:  0.007985182296102789 step  421\n",
      "cost:  0.6267236218435354\n",
      "opt took 0.00min,  421iters\n",
      "Epoch: [86][10/14.988]Time: 0.026 (0.028) Data: 0.001 (0.012) Loss: 0.9191 (0.9297)\n",
      "10-NN,s=0.1: TOP1:  61.2\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 87\n",
      "LSTM\n",
      "Epoch: [87][0/14.988]Time: 0.015 (0.015) Data: 0.001 (0.001) Loss: 0.8628 (0.8628)\n",
      "error:  0.008899088856051285 step  451\n",
      "cost:  0.5434540948586827\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [87][10/14.988]Time: 0.018 (0.027) Data: 0.001 (0.014) Loss: 0.8694 (0.8604)\n",
      "10-NN,s=0.1: TOP1:  62.53333333333333\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 88\n",
      "LSTM\n",
      "Epoch: [88][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 1.0689 (1.0689)\n",
      "error:  0.008206628471597699 step  371\n",
      "cost:  0.7017814465536008\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [88][10/14.988]Time: 0.017 (0.024) Data: 0.001 (0.011) Loss: 0.8784 (0.9672)\n",
      "10-NN,s=0.1: TOP1:  61.03333333333333\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 89\n",
      "LSTM\n",
      "Epoch: [89][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8871 (0.8871)\n",
      "error:  0.009287068441325763 step  601\n",
      "cost:  0.5670023450394623\n",
      "opt took 0.00min,  601iters\n",
      "Epoch: [89][10/14.988]Time: 0.015 (0.035) Data: 0.001 (0.023) Loss: 0.8936 (0.8967)\n",
      "10-NN,s=0.1: TOP1:  61.43333333333333\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 90\n",
      "LSTM\n",
      "Epoch: [90][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.9264 (0.9264)\n",
      "error:  0.009163400219533546 step  441\n",
      "cost:  0.5750746801378904\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [90][10/14.988]Time: 0.017 (0.029) Data: 0.001 (0.015) Loss: 0.9211 (0.9054)\n",
      "10-NN,s=0.1: TOP1:  60.6\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 91\n",
      "LSTM\n",
      "Epoch: [91][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8894 (0.8894)\n",
      "error:  0.008944446335540923 step  661\n",
      "cost:  0.5644640007690674\n",
      "opt took 0.00min,  661iters\n",
      "Epoch: [91][10/14.988]Time: 0.014 (0.028) Data: 0.001 (0.015) Loss: 0.8947 (0.8963)\n",
      "10-NN,s=0.1: TOP1:  62.3\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 92\n",
      "LSTM\n",
      "Epoch: [92][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8832 (0.8832)\n",
      "error:  0.008677820675178638 step  711\n",
      "cost:  0.561828989974491\n",
      "opt took 0.00min,  711iters\n",
      "Epoch: [92][10/14.988]Time: 0.019 (0.040) Data: 0.001 (0.026) Loss: 0.9538 (0.8966)\n",
      "10-NN,s=0.1: TOP1:  63.06666666666667\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 93\n",
      "LSTM\n",
      "Epoch: [93][0/14.988]Time: 0.015 (0.015) Data: 0.002 (0.002) Loss: 0.9004 (0.9004)\n",
      "error:  0.009808085570004677 step  341\n",
      "cost:  0.5940354409582733\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [93][10/14.988]Time: 0.014 (0.023) Data: 0.001 (0.010) Loss: 0.9078 (0.8783)\n",
      "10-NN,s=0.1: TOP1:  60.233333333333334\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 94\n",
      "LSTM\n",
      "Epoch: [94][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.8886 (0.8886)\n",
      "error:  0.008460615436966856 step  431\n",
      "cost:  0.5335877471103541\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [94][10/14.988]Time: 0.017 (0.025) Data: 0.001 (0.012) Loss: 0.8623 (0.8615)\n",
      "10-NN,s=0.1: TOP1:  62.03333333333333\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 95\n",
      "LSTM\n",
      "Epoch: [95][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.8328 (0.8328)\n",
      "error:  0.00996821537057524 step  591\n",
      "cost:  0.5593710614780859\n",
      "opt took 0.00min,  591iters\n",
      "Epoch: [95][10/14.988]Time: 0.017 (0.033) Data: 0.001 (0.021) Loss: 0.8310 (0.8249)\n",
      "10-NN,s=0.1: TOP1:  60.6\n",
      "best accuracy: 63.90\n",
      "\n",
      "Epoch: 96\n",
      "LSTM\n",
      "Epoch: [96][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8182 (0.8182)\n",
      "error:  0.00782108164831774 step  391\n",
      "cost:  0.6349046551243254\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [96][10/14.988]Time: 0.017 (0.024) Data: 0.001 (0.012) Loss: 0.8959 (0.8702)\n",
      "10-NN,s=0.1: TOP1:  64.1\n",
      "Saving..\n",
      "best accuracy: 64.10\n",
      "\n",
      "Epoch: 97\n",
      "LSTM\n",
      "Epoch: [97][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8545 (0.8545)\n",
      "error:  0.009796625613835874 step  471\n",
      "cost:  0.5235734356509052\n",
      "opt took 0.00min,  471iters\n",
      "Epoch: [97][10/14.988]Time: 0.015 (0.024) Data: 0.001 (0.012) Loss: 0.7864 (0.8377)\n",
      "10-NN,s=0.1: TOP1:  61.766666666666666\n",
      "best accuracy: 64.10\n",
      "\n",
      "Epoch: 98\n",
      "LSTM\n",
      "Epoch: [98][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7768 (0.7768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009111917434368122 step  431\n",
      "cost:  0.6394009241333213\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [98][10/14.988]Time: 0.015 (0.026) Data: 0.001 (0.014) Loss: 0.9037 (0.8794)\n",
      "10-NN,s=0.1: TOP1:  59.63333333333333\n",
      "best accuracy: 64.10\n",
      "\n",
      "Epoch: 99\n",
      "LSTM\n",
      "Epoch: [99][0/14.988]Time: 0.015 (0.015) Data: 0.001 (0.001) Loss: 0.8679 (0.8679)\n",
      "error:  0.008601701700504893 step  591\n",
      "cost:  0.5657812505042379\n",
      "opt took 0.00min,  591iters\n",
      "Epoch: [99][10/14.988]Time: 0.017 (0.034) Data: 0.001 (0.020) Loss: 0.8673 (0.8687)\n",
      "10-NN,s=0.1: TOP1:  64.9\n",
      "Saving..\n",
      "best accuracy: 64.90\n",
      "\n",
      "Epoch: 100\n",
      "LSTM\n",
      "Epoch: [100][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.8809 (0.8809)\n",
      "error:  0.00736219972460439 step  281\n",
      "cost:  0.5682788255629369\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [100][10/14.988]Time: 0.017 (0.025) Data: 0.001 (0.010) Loss: 0.8245 (0.8663)\n",
      "10-NN,s=0.1: TOP1:  62.03333333333333\n",
      "Saving..\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  58.9\n",
      "50-NN,s=0.5: TOP1:  58.833333333333336\n",
      "10-NN,s=0.1: TOP1:  63.266666666666666\n",
      "10-NN,s=0.5: TOP1:  63.13333333333333\n",
      "best accuracy: 64.90\n",
      "\n",
      "Epoch: 101\n",
      "LSTM\n",
      "Epoch: [101][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7896 (0.7896)\n",
      "error:  0.009926081563445543 step  791\n",
      "cost:  0.5198853618140059\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [101][10/14.988]Time: 0.015 (0.036) Data: 0.001 (0.025) Loss: 0.8362 (0.8127)\n",
      "10-NN,s=0.1: TOP1:  61.03333333333333\n",
      "best accuracy: 64.90\n",
      "\n",
      "Epoch: 102\n",
      "LSTM\n",
      "Epoch: [102][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8611 (0.8611)\n",
      "error:  0.008715565449898377 step  581\n",
      "cost:  0.6064848054986882\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [102][10/14.988]Time: 0.014 (0.021) Data: 0.001 (0.010) Loss: 0.9405 (0.9029)\n",
      "10-NN,s=0.1: TOP1:  65.53333333333333\n",
      "Saving..\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 103\n",
      "LSTM\n",
      "Epoch: [103][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.9166 (0.9166)\n",
      "error:  0.00947005498579212 step  561\n",
      "cost:  0.5457773296165785\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [103][10/14.988]Time: 0.015 (0.024) Data: 0.001 (0.013) Loss: 0.8844 (0.8740)\n",
      "10-NN,s=0.1: TOP1:  61.53333333333333\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 104\n",
      "LSTM\n",
      "Epoch: [104][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8482 (0.8482)\n",
      "error:  0.00925500815369562 step  621\n",
      "cost:  0.5486126366707894\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [104][10/14.988]Time: 0.015 (0.024) Data: 0.001 (0.012) Loss: 0.8799 (0.8652)\n",
      "10-NN,s=0.1: TOP1:  61.56666666666667\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 105\n",
      "LSTM\n",
      "Epoch: [105][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8734 (0.8734)\n",
      "error:  0.009587617268299597 step  811\n",
      "cost:  0.5319046233932024\n",
      "opt took 0.00min,  811iters\n",
      "Epoch: [105][10/14.988]Time: 0.026 (0.035) Data: 0.001 (0.022) Loss: 0.9200 (0.8545)\n",
      "10-NN,s=0.1: TOP1:  62.06666666666667\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 106\n",
      "LSTM\n",
      "Epoch: [106][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.8338 (0.8338)\n",
      "error:  0.009014581566594626 step  411\n",
      "cost:  0.5506526010823283\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [106][10/14.988]Time: 0.018 (0.036) Data: 0.001 (0.022) Loss: 0.8523 (0.8486)\n",
      "10-NN,s=0.1: TOP1:  61.6\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 107\n",
      "LSTM\n",
      "Epoch: [107][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8768 (0.8768)\n",
      "error:  0.009599295946460917 step  701\n",
      "cost:  0.512282539931413\n",
      "opt took 0.00min,  701iters\n",
      "Epoch: [107][10/14.988]Time: 0.018 (0.038) Data: 0.001 (0.025) Loss: 0.8039 (0.8416)\n",
      "10-NN,s=0.1: TOP1:  62.833333333333336\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 108\n",
      "LSTM\n",
      "Epoch: [108][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8854 (0.8854)\n",
      "error:  0.008219548885030847 step  411\n",
      "cost:  0.6263296647294857\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [108][10/14.988]Time: 0.014 (0.023) Data: 0.001 (0.011) Loss: 0.8618 (0.8496)\n",
      "10-NN,s=0.1: TOP1:  58.13333333333333\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 109\n",
      "LSTM\n",
      "Epoch: [109][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.9043 (0.9043)\n",
      "error:  0.008601573020925124 step  501\n",
      "cost:  0.5417077839275967\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [109][10/14.988]Time: 0.014 (0.022) Data: 0.001 (0.010) Loss: 0.8378 (0.8611)\n",
      "10-NN,s=0.1: TOP1:  59.93333333333333\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 110\n",
      "LSTM\n",
      "Epoch: [110][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8238 (0.8238)\n",
      "error:  0.009427864747227344 step  941\n",
      "cost:  0.534051748522241\n",
      "opt took 0.00min,  941iters\n",
      "Epoch: [110][10/14.988]Time: 0.014 (0.027) Data: 0.001 (0.016) Loss: 0.8482 (0.8467)\n",
      "10-NN,s=0.1: TOP1:  63.46666666666667\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 111\n",
      "LSTM\n",
      "Epoch: [111][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8482 (0.8482)\n",
      "error:  0.009040394016029185 step  791\n",
      "cost:  0.5843530600891697\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [111][10/14.988]Time: 0.016 (0.041) Data: 0.001 (0.027) Loss: 0.8417 (0.8534)\n",
      "10-NN,s=0.1: TOP1:  62.96666666666667\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 112\n",
      "LSTM\n",
      "Epoch: [112][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8334 (0.8334)\n",
      "error:  0.009899042767690047 step  541\n",
      "cost:  0.565798181270689\n",
      "opt took 0.00min,  541iters\n",
      "Epoch: [112][10/14.988]Time: 0.015 (0.026) Data: 0.001 (0.015) Loss: 0.8388 (0.8377)\n",
      "10-NN,s=0.1: TOP1:  62.733333333333334\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 113\n",
      "LSTM\n",
      "Epoch: [113][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8590 (0.8590)\n",
      "error:  0.009262226941212037 step  391\n",
      "cost:  0.6118050028602073\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [113][10/14.988]Time: 0.015 (0.022) Data: 0.001 (0.011) Loss: 0.8866 (0.8609)\n",
      "10-NN,s=0.1: TOP1:  60.36666666666667\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 114\n",
      "LSTM\n",
      "Epoch: [114][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8524 (0.8524)\n",
      "error:  0.009129101343607782 step  481\n",
      "cost:  0.5698859985406545\n",
      "opt took 0.00min,  481iters\n",
      "Epoch: [114][10/14.988]Time: 0.015 (0.024) Data: 0.001 (0.013) Loss: 0.9080 (0.8830)\n",
      "10-NN,s=0.1: TOP1:  61.43333333333333\n",
      "best accuracy: 65.53\n",
      "\n",
      "Epoch: 115\n",
      "LSTM\n",
      "Epoch: [115][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8934 (0.8934)\n",
      "error:  0.008727784559559582 step  601\n",
      "cost:  0.5636419064334598\n",
      "opt took 0.00min,  601iters\n",
      "Epoch: [115][10/14.988]Time: 0.014 (0.023) Data: 0.001 (0.012) Loss: 0.9078 (0.8860)\n",
      "10-NN,s=0.1: TOP1:  65.6\n",
      "Saving..\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 116\n",
      "LSTM\n",
      "Epoch: [116][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.9135 (0.9135)\n",
      "error:  0.009216368377156003 step  851\n",
      "cost:  0.5531411643447901\n",
      "opt took 0.00min,  851iters\n",
      "Epoch: [116][10/14.988]Time: 0.017 (0.030) Data: 0.001 (0.017) Loss: 0.8449 (0.8608)\n",
      "10-NN,s=0.1: TOP1:  62.03333333333333\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 117\n",
      "LSTM\n",
      "Epoch: [117][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8900 (0.8900)\n",
      "error:  0.009434440215260742 step  791\n",
      "cost:  0.5306171456196704\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [117][10/14.988]Time: 0.015 (0.027) Data: 0.001 (0.016) Loss: 0.8924 (0.8793)\n",
      "10-NN,s=0.1: TOP1:  65.2\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 118\n",
      "LSTM\n",
      "Epoch: [118][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8552 (0.8552)\n",
      "error:  0.008852540045497737 step  491\n",
      "cost:  0.6424306290481884\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [118][10/14.988]Time: 0.016 (0.025) Data: 0.001 (0.014) Loss: 0.9763 (0.9014)\n",
      "10-NN,s=0.1: TOP1:  63.53333333333333\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 119\n",
      "LSTM\n",
      "Epoch: [119][0/14.988]Time: 0.015 (0.015) Data: 0.002 (0.002) Loss: 0.8567 (0.8567)\n",
      "error:  0.00993203066714965 step  761\n",
      "cost:  0.5135123057324505\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [119][10/14.988]Time: 0.017 (0.030) Data: 0.001 (0.017) Loss: 0.8799 (0.8712)\n",
      "10-NN,s=0.1: TOP1:  64.33333333333333\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 120\n",
      "LSTM\n",
      "Epoch: [120][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.9124 (0.9124)\n",
      "error:  0.00857968990551472 step  661\n",
      "cost:  0.5137893600388422\n",
      "opt took 0.00min,  661iters\n",
      "Epoch: [120][10/14.988]Time: 0.173 (0.026) Data: 0.158 (0.015) Loss: 0.8645 (0.8834)\n",
      "10-NN,s=0.1: TOP1:  62.4\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 121\n",
      "LSTM\n",
      "Epoch: [121][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.8447 (0.8447)\n",
      "error:  0.008941731595402791 step  581\n",
      "cost:  0.565074289054596\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [121][10/14.988]Time: 0.142 (0.023) Data: 0.126 (0.012) Loss: 0.8923 (0.8692)\n",
      "10-NN,s=0.1: TOP1:  64.06666666666666\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 122\n",
      "LSTM\n",
      "Epoch: [122][0/14.988]Time: 0.015 (0.015) Data: 0.003 (0.003) Loss: 0.9160 (0.9160)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009891888694305906 step  531\n",
      "cost:  0.5955727131515733\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [122][10/14.988]Time: 0.256 (0.034) Data: 0.234 (0.022) Loss: 0.8864 (0.9109)\n",
      "10-NN,s=0.1: TOP1:  62.733333333333334\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 123\n",
      "LSTM\n",
      "Epoch: [123][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8294 (0.8294)\n",
      "error:  0.008686043543944977 step  671\n",
      "cost:  0.561113648620798\n",
      "opt took 0.00min,  671iters\n",
      "Epoch: [123][10/14.988]Time: 0.158 (0.024) Data: 0.144 (0.014) Loss: 0.8362 (0.8547)\n",
      "10-NN,s=0.1: TOP1:  63.56666666666667\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 124\n",
      "LSTM\n",
      "Epoch: [124][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8673 (0.8673)\n",
      "error:  0.008603465619176798 step  531\n",
      "cost:  0.5340222903319344\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [124][10/14.988]Time: 0.142 (0.023) Data: 0.125 (0.012) Loss: 0.8941 (0.8402)\n",
      "10-NN,s=0.1: TOP1:  62.7\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 125\n",
      "LSTM\n",
      "Epoch: [125][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.8661 (0.8661)\n",
      "error:  0.009678473604415094 step  761\n",
      "cost:  0.5950268048800389\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [125][10/14.988]Time: 0.186 (0.028) Data: 0.168 (0.016) Loss: 0.8705 (0.8594)\n",
      "10-NN,s=0.1: TOP1:  63.13333333333333\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 126\n",
      "LSTM\n",
      "Epoch: [126][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8684 (0.8684)\n",
      "error:  0.00951988055035946 step  921\n",
      "cost:  0.554177238820188\n",
      "opt took 0.00min,  921iters\n",
      "Epoch: [126][10/14.988]Time: 0.268 (0.035) Data: 0.251 (0.023) Loss: 0.8477 (0.8549)\n",
      "10-NN,s=0.1: TOP1:  62.96666666666667\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 127\n",
      "LSTM\n",
      "Epoch: [127][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.8360 (0.8360)\n",
      "error:  0.00936172955942971 step  991\n",
      "cost:  0.5593175913244132\n",
      "opt took 0.01min,  991iters\n",
      "Epoch: [127][10/14.988]Time: 0.496 (0.055) Data: 0.461 (0.042) Loss: 0.8766 (0.8847)\n",
      "10-NN,s=0.1: TOP1:  63.733333333333334\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 128\n",
      "LSTM\n",
      "Epoch: [128][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.8764 (0.8764)\n",
      "error:  0.009755425246977034 step  291\n",
      "cost:  0.6355625586323349\n",
      "opt took 0.00min,  291iters\n",
      "Epoch: [128][10/14.988]Time: 0.229 (0.032) Data: 0.214 (0.020) Loss: 0.8628 (0.8757)\n",
      "10-NN,s=0.1: TOP1:  64.03333333333333\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 129\n",
      "LSTM\n",
      "Epoch: [129][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8396 (0.8396)\n",
      "error:  0.009169069945897501 step  991\n",
      "cost:  0.5222710287860739\n",
      "opt took 0.00min,  991iters\n",
      "Epoch: [129][10/14.988]Time: 0.196 (0.028) Data: 0.180 (0.017) Loss: 0.8444 (0.8413)\n",
      "10-NN,s=0.1: TOP1:  61.93333333333333\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 130\n",
      "LSTM\n",
      "Epoch: [130][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8734 (0.8734)\n",
      "error:  0.00856318370847986 step  581\n",
      "cost:  0.5060584451477612\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [130][10/14.988]Time: 0.216 (0.031) Data: 0.200 (0.019) Loss: 0.8445 (0.8437)\n",
      "10-NN,s=0.1: TOP1:  62.13333333333333\n",
      "best accuracy: 65.60\n",
      "\n",
      "Epoch: 131\n",
      "LSTM\n",
      "Epoch: [131][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8470 (0.8470)\n",
      "error:  0.009112886347056315 step  931\n",
      "cost:  0.5669555985218647\n",
      "opt took 0.01min,  931iters\n",
      "Epoch: [131][10/14.988]Time: 0.387 (0.045) Data: 0.369 (0.034) Loss: 0.8470 (0.8381)\n",
      "10-NN,s=0.1: TOP1:  66.03333333333333\n",
      "Saving..\n",
      "best accuracy: 66.03\n",
      "\n",
      "Epoch: 132\n",
      "LSTM\n",
      "Epoch: [132][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.8454 (0.8454)\n",
      "error:  0.00927825658325565 step  561\n",
      "cost:  0.5565040775656259\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [132][10/14.988]Time: 0.173 (0.025) Data: 0.156 (0.015) Loss: 0.7809 (0.8485)\n",
      "10-NN,s=0.1: TOP1:  64.46666666666667\n",
      "best accuracy: 66.03\n",
      "\n",
      "Epoch: 133\n",
      "LSTM\n",
      "Epoch: [133][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7962 (0.7962)\n",
      "error:  0.009544075286459508 step  541\n",
      "cost:  0.5395910411302444\n",
      "opt took 0.00min,  541iters\n",
      "Epoch: [133][10/14.988]Time: 0.202 (0.029) Data: 0.186 (0.018) Loss: 0.8292 (0.7999)\n",
      "10-NN,s=0.1: TOP1:  62.13333333333333\n",
      "best accuracy: 66.03\n",
      "\n",
      "Epoch: 134\n",
      "LSTM\n",
      "Epoch: [134][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8330 (0.8330)\n",
      "Epoch: [134][10/14.988]Time: 0.010 (0.011) Data: 0.001 (0.001) Loss: 0.8418 (0.8157)\n",
      "error:  0.009544064112140704 step  861\n",
      "cost:  0.5152214647209227\n",
      "opt took 0.01min,  861iters\n",
      "10-NN,s=0.1: TOP1:  64.66666666666667\n",
      "best accuracy: 66.03\n",
      "\n",
      "Epoch: 135\n",
      "LSTM\n",
      "Epoch: [135][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8566 (0.8566)\n",
      "Epoch: [135][10/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.8514 (0.8157)\n",
      "error:  0.00995022778671606 step  1021\n",
      "cost:  0.5110494625177223\n",
      "opt took 0.00min, 1021iters\n",
      "10-NN,s=0.1: TOP1:  64.33333333333333\n",
      "best accuracy: 66.03\n",
      "\n",
      "Epoch: 136\n",
      "LSTM\n",
      "Epoch: [136][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7668 (0.7668)\n",
      "Epoch: [136][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.000) Loss: 0.7882 (0.7799)\n",
      "error:  0.009919229423342113 step  771\n",
      "cost:  0.48396559640340897\n",
      "opt took 0.00min,  771iters\n",
      "10-NN,s=0.1: TOP1:  62.333333333333336\n",
      "best accuracy: 66.03\n",
      "\n",
      "Epoch: 137\n",
      "LSTM\n",
      "Epoch: [137][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7618 (0.7618)\n",
      "Epoch: [137][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.8053 (0.7821)\n",
      "error:  0.009031908685591894 step  641\n",
      "cost:  0.51281881316918\n",
      "opt took 0.00min,  641iters\n",
      "10-NN,s=0.1: TOP1:  65.9\n",
      "best accuracy: 66.03\n",
      "\n",
      "Epoch: 138\n",
      "LSTM\n",
      "Epoch: [138][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7802 (0.7802)\n",
      "Epoch: [138][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.000) Loss: 0.8161 (0.7835)\n",
      "error:  0.008571886792076211 step  371\n",
      "cost:  0.5756799216178357\n",
      "opt took 0.00min,  371iters\n",
      "10-NN,s=0.1: TOP1:  66.9\n",
      "Saving..\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 139\n",
      "LSTM\n",
      "Epoch: [139][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8232 (0.8232)\n",
      "Epoch: [139][10/14.988]Time: 0.010 (0.011) Data: 0.001 (0.001) Loss: 0.8760 (0.8222)\n",
      "error:  0.009300862353462946 step  431\n",
      "cost:  0.541226006726028\n",
      "opt took 0.00min,  431iters\n",
      "10-NN,s=0.1: TOP1:  64.3\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 140\n",
      "LSTM\n",
      "Epoch: [140][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8565 (0.8565)\n",
      "Epoch: [140][10/14.988]Time: 0.011 (0.012) Data: 0.000 (0.001) Loss: 0.8127 (0.8129)\n",
      "error:  0.009971706041586703 step  431\n",
      "cost:  0.5093799224802437\n",
      "opt took 0.00min,  431iters\n",
      "10-NN,s=0.1: TOP1:  64.33333333333333\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 141\n",
      "LSTM\n",
      "Epoch: [141][0/14.988]Time: 0.015 (0.015) Data: 0.003 (0.003) Loss: 0.8664 (0.8664)\n",
      "Epoch: [141][10/14.988]Time: 0.014 (0.013) Data: 0.001 (0.001) Loss: 0.8584 (0.8435)\n",
      "error:  0.008891402274433857 step  701\n",
      "cost:  0.5458801634786483\n",
      "opt took 0.00min,  701iters\n",
      "10-NN,s=0.1: TOP1:  64.93333333333334\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 142\n",
      "LSTM\n",
      "Epoch: [142][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.7996 (0.7996)\n",
      "Epoch: [142][10/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8225 (0.7805)\n",
      "error:  0.009472528154928384 step  531\n",
      "cost:  0.500797838803607\n",
      "opt took 0.00min,  531iters\n",
      "10-NN,s=0.1: TOP1:  62.63333333333333\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 143\n",
      "LSTM\n",
      "Epoch: [143][0/14.988]Time: 0.016 (0.016) Data: 0.001 (0.001) Loss: 0.7583 (0.7583)\n",
      "Epoch: [143][10/14.988]Time: 0.012 (0.013) Data: 0.000 (0.001) Loss: 0.8035 (0.7604)\n",
      "error:  0.009976116581523975 step  801\n",
      "cost:  0.5291544711925287\n",
      "opt took 0.00min,  801iters\n",
      "10-NN,s=0.1: TOP1:  64.8\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 144\n",
      "LSTM\n",
      "Epoch: [144][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7606 (0.7606)\n",
      "Epoch: [144][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.8656 (0.7802)\n",
      "error:  0.009841066259369646 step  871\n",
      "cost:  0.5292222307904912\n",
      "opt took 0.00min,  871iters\n",
      "10-NN,s=0.1: TOP1:  66.13333333333334\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 145\n",
      "LSTM\n",
      "Epoch: [145][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8627 (0.8627)\n",
      "Epoch: [145][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.8086 (0.8383)\n",
      "error:  0.009579851499240344 step  881\n",
      "cost:  0.524720511644905\n",
      "opt took 0.00min,  881iters\n",
      "10-NN,s=0.1: TOP1:  65.96666666666667\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 146\n",
      "LSTM\n",
      "Epoch: [146][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.7635 (0.7635)\n",
      "Epoch: [146][10/14.988]Time: 0.019 (0.013) Data: 0.000 (0.001) Loss: 0.8843 (0.8497)\n",
      "error:  0.009416032911472638 step  561\n",
      "cost:  0.5674252941357428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt took 0.00min,  561iters\n",
      "10-NN,s=0.1: TOP1:  65.1\n",
      "best accuracy: 66.90\n",
      "\n",
      "Epoch: 147\n",
      "LSTM\n",
      "Epoch: [147][0/14.988]Time: 0.020 (0.020) Data: 0.002 (0.002) Loss: 0.9524 (0.9524)\n",
      "Epoch: [147][10/14.988]Time: 0.013 (0.017) Data: 0.001 (0.001) Loss: 0.7976 (0.8746)\n",
      "error:  0.009087683621662568 step  631\n",
      "cost:  0.5547169610358622\n",
      "opt took 0.00min,  631iters\n",
      "10-NN,s=0.1: TOP1:  69.36666666666666\n",
      "Saving..\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 148\n",
      "LSTM\n",
      "Epoch: [148][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8524 (0.8524)\n",
      "Epoch: [148][10/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.7910 (0.8264)\n",
      "error:  0.00934539887109842 step  941\n",
      "cost:  0.552168988401251\n",
      "opt took 0.00min,  941iters\n",
      "10-NN,s=0.1: TOP1:  69.16666666666667\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 149\n",
      "LSTM\n",
      "Epoch: [149][0/14.988]Time: 0.019 (0.019) Data: 0.002 (0.002) Loss: 0.8156 (0.8156)\n",
      "Epoch: [149][10/14.988]Time: 0.012 (0.014) Data: 0.000 (0.001) Loss: 0.8211 (0.8237)\n",
      "error:  0.007404533125084378 step  331\n",
      "cost:  0.5312115725373964\n",
      "opt took 0.00min,  331iters\n",
      "10-NN,s=0.1: TOP1:  68.4\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 150\n",
      "LSTM\n",
      "Epoch: [150][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8145 (0.8145)\n",
      "Epoch: [150][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.8236 (0.8248)\n",
      "error:  0.009924432155380525 step  851\n",
      "cost:  0.5447167247782562\n",
      "opt took 0.00min,  851iters\n",
      "10-NN,s=0.1: TOP1:  68.13333333333334\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  63.56666666666667\n",
      "50-NN,s=0.5: TOP1:  63.166666666666664\n",
      "10-NN,s=0.1: TOP1:  68.83333333333333\n",
      "10-NN,s=0.5: TOP1:  68.7\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 151\n",
      "LSTM\n",
      "Epoch: [151][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8600 (0.8600)\n",
      "Epoch: [151][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.000) Loss: 0.8946 (0.8646)\n",
      "error:  0.00895648207878208 step  531\n",
      "cost:  0.5680244730977061\n",
      "opt took 0.00min,  531iters\n",
      "10-NN,s=0.1: TOP1:  65.6\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 152\n",
      "LSTM\n",
      "Epoch: [152][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.8188 (0.8188)\n",
      "Epoch: [152][10/14.988]Time: 0.010 (0.011) Data: 0.001 (0.001) Loss: 0.7814 (0.8303)\n",
      "error:  0.009175301307370987 step  841\n",
      "cost:  0.5595981153325558\n",
      "opt took 0.00min,  841iters\n",
      "10-NN,s=0.1: TOP1:  65.63333333333334\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 153\n",
      "LSTM\n",
      "Epoch: [153][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8856 (0.8856)\n",
      "Epoch: [153][10/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.9153 (0.8904)\n",
      "error:  0.008053298641642392 step  391\n",
      "cost:  0.6106768980581606\n",
      "opt took 0.00min,  391iters\n",
      "10-NN,s=0.1: TOP1:  67.66666666666667\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 154\n",
      "LSTM\n",
      "Epoch: [154][0/14.988]Time: 0.018 (0.018) Data: 0.002 (0.002) Loss: 0.9261 (0.9261)\n",
      "Epoch: [154][10/14.988]Time: 0.013 (0.016) Data: 0.001 (0.001) Loss: 0.8657 (0.9034)\n",
      "error:  0.00862504504880468 step  641\n",
      "cost:  0.5670428092677902\n",
      "opt took 0.00min,  641iters\n",
      "10-NN,s=0.1: TOP1:  69.03333333333333\n",
      "best accuracy: 69.37\n",
      "\n",
      "Epoch: 155\n",
      "LSTM\n",
      "Epoch: [155][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.8272 (0.8272)\n",
      "Epoch: [155][10/14.988]Time: 0.015 (0.014) Data: 0.001 (0.001) Loss: 0.8459 (0.8479)\n",
      "error:  0.009810399765655453 step  711\n",
      "cost:  0.5356331792414537\n",
      "opt took 0.00min,  711iters\n",
      "10-NN,s=0.1: TOP1:  70.2\n",
      "Saving..\n",
      "best accuracy: 70.20\n",
      "\n",
      "Epoch: 156\n",
      "LSTM\n",
      "Epoch: [156][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8363 (0.8363)\n",
      "Epoch: [156][10/14.988]Time: 0.012 (0.012) Data: 0.000 (0.001) Loss: 0.8130 (0.8316)\n",
      "error:  0.009276382902424052 step  741\n",
      "cost:  0.55461554136648\n",
      "opt took 0.00min,  741iters\n",
      "10-NN,s=0.1: TOP1:  71.66666666666667\n",
      "Saving..\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 157\n",
      "LSTM\n",
      "Epoch: [157][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8151 (0.8151)\n",
      "Epoch: [157][10/14.988]Time: 0.012 (0.012) Data: 0.000 (0.001) Loss: 0.8196 (0.8130)\n",
      "error:  0.00916249425559612 step  391\n",
      "cost:  0.5415590980145383\n",
      "opt took 0.00min,  391iters\n",
      "10-NN,s=0.1: TOP1:  69.73333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 158\n",
      "LSTM\n",
      "Epoch: [158][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.8242 (0.8242)\n",
      "Epoch: [158][10/14.988]Time: 0.011 (0.012) Data: 0.001 (0.001) Loss: 0.8881 (0.8110)\n",
      "error:  0.009827926717928759 step  761\n",
      "cost:  0.5468943983984188\n",
      "opt took 0.00min,  761iters\n",
      "10-NN,s=0.1: TOP1:  70.23333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 159\n",
      "LSTM\n",
      "Epoch: [159][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.8100 (0.8100)\n",
      "Epoch: [159][10/14.988]Time: 0.012 (0.013) Data: 0.001 (0.001) Loss: 0.8325 (0.8133)\n",
      "error:  0.0093987028758874 step  271\n",
      "cost:  0.5167216695490509\n",
      "opt took 0.00min,  271iters\n",
      "10-NN,s=0.1: TOP1:  68.7\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 160\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [160][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.8635 (0.8635)\n",
      "Epoch: [160][10/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8185 (0.8378)\n",
      "error:  0.009454037332381593 step  961\n",
      "cost:  0.5519963778801072\n",
      "opt took 0.00min,  961iters\n",
      "10-NN,s=0.1: TOP1:  68.2\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 161\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [161][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8406 (0.8406)\n",
      "Epoch: [161][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.8464 (0.8210)\n",
      "error:  0.009254385836222512 step  541\n",
      "cost:  0.5369625195880642\n",
      "opt took 0.00min,  541iters\n",
      "10-NN,s=0.1: TOP1:  69.6\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 162\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [162][0/14.988]Time: 0.015 (0.015) Data: 0.002 (0.002) Loss: 0.8259 (0.8259)\n",
      "Epoch: [162][10/14.988]Time: 0.014 (0.013) Data: 0.000 (0.001) Loss: 0.7872 (0.8217)\n",
      "error:  0.008722790712223816 step  371\n",
      "cost:  0.5773638738996968\n",
      "opt took 0.00min,  371iters\n",
      "10-NN,s=0.1: TOP1:  67.7\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 163\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [163][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8064 (0.8064)\n",
      "Epoch: [163][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.8143 (0.7868)\n",
      "error:  0.009246650008811863 step  591\n",
      "cost:  0.5489174430589514\n",
      "opt took 0.00min,  591iters\n",
      "10-NN,s=0.1: TOP1:  70.7\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 164\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [164][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8612 (0.8612)\n",
      "Epoch: [164][10/14.988]Time: 0.013 (0.012) Data: 0.001 (0.001) Loss: 0.7930 (0.8375)\n",
      "error:  0.009542362656487557 step  871\n",
      "cost:  0.5587696343558181\n",
      "opt took 0.01min,  871iters\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 165\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [165][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8691 (0.8691)\n",
      "Epoch: [165][10/14.988]Time: 0.011 (0.012) Data: 0.001 (0.001) Loss: 0.7960 (0.8284)\n",
      "error:  0.009524609979346788 step  861\n",
      "cost:  0.47818916091126273\n",
      "opt took 0.00min,  861iters\n",
      "10-NN,s=0.1: TOP1:  70.06666666666666\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 166\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [166][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.7792 (0.7792)\n",
      "Epoch: [166][10/14.988]Time: 0.013 (0.014) Data: 0.001 (0.001) Loss: 0.8115 (0.7822)\n",
      "error:  0.009044410135466974 step  861\n",
      "cost:  0.5378018594124347\n",
      "opt took 0.00min,  861iters\n",
      "10-NN,s=0.1: TOP1:  69.26666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 167\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [167][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.7641 (0.7641)\n",
      "Epoch: [167][10/14.988]Time: 0.011 (0.012) Data: 0.001 (0.001) Loss: 0.7617 (0.7758)\n",
      "error:  0.009279517473071097 step  991\n",
      "cost:  0.5196010437972086\n",
      "opt took 0.00min,  991iters\n",
      "10-NN,s=0.1: TOP1:  69.7\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 168\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [168][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.7297 (0.7297)\n",
      "Epoch: [168][10/14.988]Time: 0.011 (0.012) Data: 0.000 (0.001) Loss: 0.7573 (0.7745)\n",
      "error:  0.00953457799988422 step  561\n",
      "cost:  0.5278672838551389\n",
      "opt took 0.00min,  561iters\n",
      "10-NN,s=0.1: TOP1:  66.56666666666666\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 169\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [169][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.8138 (0.8138)\n",
      "Epoch: [169][10/14.988]Time: 0.012 (0.014) Data: 0.000 (0.001) Loss: 0.7544 (0.8333)\n",
      "error: ng head 9  0.009622041612995957 step  1281\n",
      "cost:  0.5488278826394339\n",
      "opt took 0.00min, 1281iters\n",
      "10-NN,s=0.1: TOP1:  68.8\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 170\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [170][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 1.0023 (1.0023)\n",
      "Epoch: [170][10/14.988]Time: 0.012 (0.013) Data: 0.001 (0.001) Loss: 0.8449 (0.8674)\n",
      "error:  0.009553590118543731 step  1381\n",
      "cost:  0.5372673089268548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt took 0.01min, 1381iters\n",
      "10-NN,s=0.1: TOP1:  69.96666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 171\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [171][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8462 (0.8462)\n",
      "Epoch: [171][10/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.7651 (0.7928)\n",
      "error:  0.009784957910220382 step  621\n",
      "cost:  0.4763268823591198\n",
      "opt took 0.00min,  621iters\n",
      "10-NN,s=0.1: TOP1:  69.83333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 172\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [172][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.8123 (0.8123)\n",
      "Epoch: [172][10/14.988]Time: 0.011 (0.012) Data: 0.000 (0.001) Loss: 0.7734 (0.7869)\n",
      "error:  0.008884064103727995 step  491\n",
      "cost:  0.5399695909686855\n",
      "opt took 0.00min,  491iters\n",
      "10-NN,s=0.1: TOP1:  69.3\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 173\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [173][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8024 (0.8024)\n",
      "Epoch: [173][10/14.988]Time: 0.011 (0.012) Data: 0.000 (0.001) Loss: 0.8376 (0.7836)\n",
      "error:  0.00952630521034925 step  871\n",
      "cost:  0.5601080119602475\n",
      "opt took 0.00min,  871iters\n",
      "10-NN,s=0.1: TOP1:  70.96666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 174\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [174][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.9161 (0.9161)\n",
      "Epoch: [174][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.8020 (0.8502)\n",
      "10-NN,s=0.1: TOP1:  67.26666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 175\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.00994861082902987 step  701\n",
      "cost:  0.4978212637038596\n",
      "opt took 0.00min,  701iters\n",
      "Epoch: [175][0/14.988]Time: 0.205 (0.205) Data: 0.189 (0.189) Loss: 0.8024 (0.8024)\n",
      "Epoch: [175][10/14.988]Time: 0.010 (0.032) Data: 0.000 (0.018) Loss: 0.7811 (0.8048)\n",
      "10-NN,s=0.1: TOP1:  65.3\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 176\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.00893759918957615 step  591\n",
      "cost:  0.5226806760549945\n",
      "opt took 0.00min,  591iters\n",
      "Epoch: [176][0/14.988]Time: 0.334 (0.334) Data: 0.317 (0.317) Loss: 0.7469 (0.7469)\n",
      "Epoch: [176][10/14.988]Time: 0.011 (0.045) Data: 0.001 (0.030) Loss: 0.7344 (0.7637)\n",
      "10-NN,s=0.1: TOP1:  66.53333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 177\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.008638180612559943 step  681\n",
      "cost:  0.48183993244215767\n",
      "opt took 0.00min,  681iters\n",
      "Epoch: [177][0/14.988]Time: 0.191 (0.191) Data: 0.176 (0.176) Loss: 0.7825 (0.7825)\n",
      "Epoch: [177][10/14.988]Time: 0.010 (0.031) Data: 0.000 (0.017) Loss: 0.7828 (0.7817)\n",
      "10-NN,s=0.1: TOP1:  70.3\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 178\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.008857468829144954 step  781\n",
      "cost:  0.5571388818104649\n",
      "opt took 0.01min,  781iters\n",
      "Epoch: [178][0/14.988]Time: 0.409 (0.409) Data: 0.386 (0.386) Loss: 0.8943 (0.8943)\n",
      "Epoch: [178][10/14.988]Time: 0.012 (0.051) Data: 0.001 (0.036) Loss: 0.8126 (0.8500)\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 179\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009700941703399635 step  571\n",
      "cost:  0.5031272783531965\n",
      "opt took 0.00min,  571iters\n",
      "Epoch: [179][0/14.988]Time: 0.159 (0.159) Data: 0.142 (0.142) Loss: 0.8044 (0.8044)\n",
      "Epoch: [179][10/14.988]Time: 0.011 (0.027) Data: 0.000 (0.013) Loss: 0.7786 (0.7988)\n",
      "10-NN,s=0.1: TOP1:  67.5\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 180\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009428732809222296 step  1001\n",
      "cost:  0.4979277806673235\n",
      "opt took 0.00min, 1001iters\n",
      "Epoch: [180][0/14.988]Time: 0.234 (0.234) Data: 0.219 (0.219) Loss: 0.7626 (0.7626)\n",
      "Epoch: [180][10/14.988]Time: 0.010 (0.033) Data: 0.000 (0.020) Loss: 0.7761 (0.7741)\n",
      "10-NN,s=0.1: TOP1:  65.96666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 181\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.008777792874263501 step  751\n",
      "cost:  0.4722835854159869\n",
      "opt took 0.00min,  751iters\n",
      "Epoch: [181][0/14.988]Time: 0.185 (0.185) Data: 0.170 (0.170) Loss: 0.7337 (0.7337)\n",
      "Epoch: [181][10/14.988]Time: 0.011 (0.029) Data: 0.000 (0.016) Loss: 0.7758 (0.7596)\n",
      "10-NN,s=0.1: TOP1:  70.5\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 182\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.008688491885133542 step  561\n",
      "cost:  0.5061999262322697\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [182][0/14.988]Time: 0.288 (0.288) Data: 0.267 (0.267) Loss: 0.7394 (0.7394)\n",
      "Epoch: [182][10/14.988]Time: 0.012 (0.040) Data: 0.001 (0.025) Loss: 0.7710 (0.7489)\n",
      "10-NN,s=0.1: TOP1:  67.93333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 183\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.008891009783061454 step  421\n",
      "cost:  0.5846195632528463\n",
      "opt took 0.00min,  421iters\n",
      "Epoch: [183][0/14.988]Time: 0.144 (0.144) Data: 0.127 (0.127) Loss: 0.7852 (0.7852)\n",
      "Epoch: [183][10/14.988]Time: 0.012 (0.027) Data: 0.001 (0.012) Loss: 0.7985 (0.7816)\n",
      "10-NN,s=0.1: TOP1:  67.6\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 184\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009446139123274078 step  801\n",
      "cost:  0.542670000176742\n",
      "opt took 0.00min,  801iters\n",
      "Epoch: [184][0/14.988]Time: 0.225 (0.225) Data: 0.209 (0.209) Loss: 0.7306 (0.7306)\n",
      "Epoch: [184][10/14.988]Time: 0.012 (0.035) Data: 0.000 (0.020) Loss: 0.7087 (0.7331)\n",
      "10-NN,s=0.1: TOP1:  68.73333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 185\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.00949003199874554 step  831\n",
      "cost:  0.5059541830291858\n",
      "opt took 0.00min,  831iters\n",
      "Epoch: [185][0/14.988]Time: 0.195 (0.195) Data: 0.178 (0.178) Loss: 0.7431 (0.7431)\n",
      "Epoch: [185][10/14.988]Time: 0.012 (0.033) Data: 0.001 (0.017) Loss: 0.8678 (0.7628)\n",
      "10-NN,s=0.1: TOP1:  65.8\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 186\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009685873050828686 step  601\n",
      "cost:  0.570471073501091\n",
      "opt took 0.00min,  601iters\n",
      "Epoch: [186][0/14.988]Time: 0.158 (0.158) Data: 0.141 (0.141) Loss: 0.8106 (0.8106)\n",
      "Epoch: [186][10/14.988]Time: 0.011 (0.028) Data: 0.000 (0.013) Loss: 0.7992 (0.8206)\n",
      "10-NN,s=0.1: TOP1:  69.66666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 187\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009579251930649235 step  481\n",
      "cost:  0.5278505438669336\n",
      "opt took 0.00min,  481iters\n",
      "Epoch: [187][0/14.988]Time: 0.177 (0.177) Data: 0.159 (0.159) Loss: 0.8473 (0.8473)\n",
      "Epoch: [187][10/14.988]Time: 0.011 (0.029) Data: 0.000 (0.015) Loss: 0.7566 (0.8025)\n",
      "10-NN,s=0.1: TOP1:  68.9\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 188\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009441207851663469 step  581\n",
      "cost:  0.5499606095706073\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [188][0/14.988]Time: 0.187 (0.187) Data: 0.171 (0.171) Loss: 0.7471 (0.7471)\n",
      "Epoch: [188][10/14.988]Time: 0.012 (0.031) Data: 0.000 (0.016) Loss: 0.7679 (0.7716)\n",
      "10-NN,s=0.1: TOP1:  69.1\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 189\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.00952541530883122 step  531\n",
      "cost:  0.5172898839634653\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [189][0/14.988]Time: 0.158 (0.158) Data: 0.141 (0.141) Loss: 0.7795 (0.7795)\n",
      "Epoch: [189][10/14.988]Time: 0.012 (0.030) Data: 0.001 (0.014) Loss: 0.8001 (0.7969)\n",
      "10-NN,s=0.1: TOP1:  67.3\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 190\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.008701470657741073 step  491\n",
      "cost:  0.4740041248135186\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [190][0/14.988]Time: 0.140 (0.140) Data: 0.123 (0.123) Loss: 0.8063 (0.8063)\n",
      "Epoch: [190][10/14.988]Time: 0.011 (0.027) Data: 0.000 (0.012) Loss: 0.7785 (0.7987)\n",
      "10-NN,s=0.1: TOP1:  65.23333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 191\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009549884816329235 step  791\n",
      "cost:  0.523537046019059\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [191][0/14.988]Time: 0.331 (0.331) Data: 0.312 (0.312) Loss: 0.7975 (0.7975)\n",
      "Epoch: [191][10/14.988]Time: 0.021 (0.048) Data: 0.001 (0.029) Loss: 0.8065 (0.8083)\n",
      "10-NN,s=0.1: TOP1:  69.53333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 192\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.0093025058719024 step  671\n",
      "cost:  0.5051363678797633\n",
      "opt took 0.00min,  671iters\n",
      "Epoch: [192][0/14.988]Time: 0.182 (0.182) Data: 0.164 (0.164) Loss: 0.7441 (0.7441)\n",
      "Epoch: [192][10/14.988]Time: 0.012 (0.032) Data: 0.000 (0.015) Loss: 0.8212 (0.7822)\n",
      "10-NN,s=0.1: TOP1:  70.46666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 193\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009967568009101924 step  711\n",
      "cost:  0.5123870093328792\n",
      "opt took 0.00min,  711iters\n",
      "Epoch: [193][0/14.988]Time: 0.150 (0.150) Data: 0.136 (0.136) Loss: 0.7323 (0.7323)\n",
      "Epoch: [193][10/14.988]Time: 0.012 (0.026) Data: 0.000 (0.013) Loss: 0.7640 (0.7452)\n",
      "10-NN,s=0.1: TOP1:  68.46666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 194\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009106476002583519 step  931\n",
      "cost:  0.5075207128566863\n",
      "opt took 0.00min,  931iters\n",
      "Epoch: [194][0/14.988]Time: 0.230 (0.230) Data: 0.213 (0.213) Loss: 0.7844 (0.7844)\n",
      "Epoch: [194][10/14.988]Time: 0.010 (0.034) Data: 0.000 (0.020) Loss: 0.8018 (0.7876)\n",
      "10-NN,s=0.1: TOP1:  66.9\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 195\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009495678492817738 step  971\n",
      "cost:  0.5215874986590077\n",
      "opt took 0.00min,  971iters\n",
      "Epoch: [195][0/14.988]Time: 0.281 (0.281) Data: 0.266 (0.266) Loss: 0.7521 (0.7521)\n",
      "Epoch: [195][10/14.988]Time: 0.011 (0.038) Data: 0.000 (0.025) Loss: 0.7307 (0.7763)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-NN,s=0.1: TOP1:  68.43333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 196\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009666592202586699 step  401\n",
      "cost:  0.5650159452925055\n",
      "opt took 0.00min,  401iters\n",
      "Epoch: [196][0/14.988]Time: 0.162 (0.162) Data: 0.146 (0.146) Loss: 0.7347 (0.7347)\n",
      "Epoch: [196][10/14.988]Time: 0.011 (0.027) Data: 0.001 (0.014) Loss: 0.7196 (0.7642)\n",
      "10-NN,s=0.1: TOP1:  67.03333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 197\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009641584899991673 step  611\n",
      "cost:  0.5040234520126825\n",
      "opt took 0.00min,  611iters\n",
      "Epoch: [197][0/14.988]Time: 0.168 (0.168) Data: 0.154 (0.154) Loss: 0.7598 (0.7598)\n",
      "Epoch: [197][10/14.988]Time: 0.010 (0.030) Data: 0.000 (0.015) Loss: 0.7623 (0.8064)\n",
      "10-NN,s=0.1: TOP1:  70.23333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 198\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009445405445295707 step  621\n",
      "cost:  0.5208551553379883\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [198][0/14.988]Time: 0.267 (0.267) Data: 0.251 (0.251) Loss: 0.8055 (0.8055)\n",
      "Epoch: [198][10/14.988]Time: 0.011 (0.037) Data: 0.001 (0.024) Loss: 0.8717 (0.7960)\n",
      "10-NN,s=0.1: TOP1:  70.2\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 199\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.009261324199099419 step  501\n",
      "cost:  0.5532268904974746\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [199][0/14.988]Time: 0.195 (0.195) Data: 0.179 (0.179) Loss: 0.9285 (0.9285)\n",
      "Epoch: [199][10/14.988]Time: 0.011 (0.031) Data: 0.000 (0.017) Loss: 0.8312 (0.8662)\n",
      "10-NN,s=0.1: TOP1:  68.36666666666666\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 200\n",
      "LSTM\n",
      "0.9\n",
      "error:  0.008722094304867256 step  561\n",
      "cost:  0.5611419831244467\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [200][0/14.988]Time: 0.206 (0.206) Data: 0.191 (0.191) Loss: 0.9074 (0.9074)\n",
      "Epoch: [200][10/14.988]Time: 0.010 (0.031) Data: 0.000 (0.018) Loss: 0.8999 (0.8983)\n",
      "10-NN,s=0.1: TOP1:  68.93333333333334\n",
      "Saving..\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  65.86666666666666\n",
      "50-NN,s=0.5: TOP1:  65.46666666666667\n",
      "10-NN,s=0.1: TOP1:  69.9\n",
      "10-NN,s=0.5: TOP1:  70.3\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 201\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [201][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.8685 (0.8685)\n",
      "error:  0.00950528031220954 step  911\n",
      "cost:  0.5376730551845584\n",
      "opt took 0.00min,  911iters\n",
      "Epoch: [201][10/14.988]Time: 0.011 (0.042) Data: 0.001 (0.029) Loss: 0.7563 (0.8116)\n",
      "10-NN,s=0.1: TOP1:  65.43333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 202\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [202][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7905 (0.7905)\n",
      "error:  0.00890783251439653 step  761\n",
      "cost:  0.49064165304801793\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [202][10/14.988]Time: 0.011 (0.029) Data: 0.001 (0.016) Loss: 0.7970 (0.8223)\n",
      "10-NN,s=0.1: TOP1:  68.83333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 203\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [203][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8010 (0.8010)\n",
      "error:  0.009060466562724412 step  501\n",
      "cost:  0.5311928497294889\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [203][10/14.988]Time: 0.011 (0.030) Data: 0.001 (0.016) Loss: 0.7621 (0.7888)\n",
      "10-NN,s=0.1: TOP1:  69.8\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 204\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [204][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8107 (0.8107)\n",
      "error:  0.009454055645609971 step  651\n",
      "cost:  0.49923100872147935\n",
      "opt took 0.00min,  651iters\n",
      "Epoch: [204][10/14.988]Time: 0.010 (0.030) Data: 0.001 (0.015) Loss: 0.8618 (0.7936)\n",
      "10-NN,s=0.1: TOP1:  68.2\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 205\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [205][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.7714 (0.7714)\n",
      "error:  0.009703277403005361 step  761\n",
      "cost:  0.4826942794099333\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [205][10/14.988]Time: 0.010 (0.026) Data: 0.000 (0.013) Loss: 0.7702 (0.7898)\n",
      "10-NN,s=0.1: TOP1:  67.9\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 206\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [206][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.7540 (0.7540)\n",
      "error:  0.009347809248473338 step  671\n",
      "cost:  0.5296704292096539\n",
      "opt took 0.00min,  671iters\n",
      "Epoch: [206][10/14.988]Time: 0.010 (0.033) Data: 0.000 (0.018) Loss: 0.8697 (0.8245)\n",
      "10-NN,s=0.1: TOP1:  69.03333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 207\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [207][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.8680 (0.8680)\n",
      "error:  0.009840205640924138 step  661\n",
      "cost:  0.539803748850115\n",
      "opt took 0.00min,  661iters\n",
      "Epoch: [207][10/14.988]Time: 0.010 (0.028) Data: 0.000 (0.014) Loss: 0.7939 (0.8248)\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 208\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [208][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7866 (0.7866)\n",
      "error:  0.009811659476350543 step  631\n",
      "cost:  0.5364241611439677\n",
      "opt took 0.00min,  631iters\n",
      "Epoch: [208][10/14.988]Time: 0.010 (0.026) Data: 0.000 (0.013) Loss: 0.7948 (0.8054)\n",
      "10-NN,s=0.1: TOP1:  69.1\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 209\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [209][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.7546 (0.7546)\n",
      "error:  0.009662458984617928 step  1341\n",
      "cost:  0.502790889626147\n",
      "opt took 0.01min, 1341iters\n",
      "Epoch: [209][10/14.988]Time: 0.010 (0.046) Data: 0.001 (0.034) Loss: 0.7589 (0.7710)\n",
      "10-NN,s=0.1: TOP1:  69.1\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 210\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [210][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.8163 (0.8163)\n",
      "error:  0.009040306755299987 step  821\n",
      "cost:  0.5369113530886628\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [210][10/14.988]Time: 0.010 (0.027) Data: 0.000 (0.014) Loss: 0.9010 (0.8367)\n",
      "10-NN,s=0.1: TOP1:  68.1\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 211\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [211][0/14.988]Time: 0.015 (0.015) Data: 0.002 (0.002) Loss: 0.7977 (0.7977)\n",
      "error:  0.00813723934015076 step  411\n",
      "cost:  0.5651443739748492\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [211][10/14.988]Time: 0.012 (0.026) Data: 0.001 (0.012) Loss: 0.8160 (0.8373)\n",
      "10-NN,s=0.1: TOP1:  68.03333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 212\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [212][0/14.988]Time: 0.017 (0.017) Data: 0.002 (0.002) Loss: 0.7808 (0.7808)\n",
      "error:  0.009241832288274465 step  801\n",
      "cost:  0.5550440514865097\n",
      "opt took 0.00min,  801iters\n",
      "Epoch: [212][10/14.988]Time: 0.011 (0.039) Data: 0.000 (0.023) Loss: 0.7905 (0.7786)\n",
      "10-NN,s=0.1: TOP1:  69.1\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 213\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [213][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7793 (0.7793)\n",
      "error:  0.009744247149150898 step  531\n",
      "cost:  0.5209814202799518\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [213][10/14.988]Time: 0.011 (0.024) Data: 0.000 (0.011) Loss: 0.7855 (0.7775)\n",
      "10-NN,s=0.1: TOP1:  67.3\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 214\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [214][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7896 (0.7896)\n",
      "error:  0.009546507232778056 step  891\n",
      "cost:  0.5298613294478229\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [214][10/14.988]Time: 0.012 (0.031) Data: 0.000 (0.018) Loss: 0.7458 (0.7655)\n",
      "10-NN,s=0.1: TOP1:  69.93333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 215\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [215][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7764 (0.7764)\n",
      "error:  0.008912815985268496 step  531\n",
      "cost:  0.4873329743644196\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [215][10/14.988]Time: 0.014 (0.024) Data: 0.001 (0.011) Loss: 0.7898 (0.7656)\n",
      "10-NN,s=0.1: TOP1:  68.46666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 216\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [216][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.7830 (0.7830)\n",
      "error:  0.009926878695616592 step  1011\n",
      "cost:  0.49996967695012995\n",
      "opt took 0.00min, 1011iters\n",
      "Epoch: [216][10/14.988]Time: 0.013 (0.042) Data: 0.001 (0.026) Loss: 0.8423 (0.7791)\n",
      "10-NN,s=0.1: TOP1:  67.9\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 217\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [217][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7388 (0.7388)\n",
      "error:  0.009244931658979105 step  441\n",
      "cost:  0.5550586916681012\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [217][10/14.988]Time: 0.011 (0.025) Data: 0.000 (0.011) Loss: 0.7229 (0.7601)\n",
      "10-NN,s=0.1: TOP1:  69.06666666666666\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 218\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [218][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8255 (0.8255)\n",
      "error:  0.008030274623560096 step  361\n",
      "cost:  0.48942681202096244\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [218][10/14.988]Time: 0.011 (0.025) Data: 0.001 (0.011) Loss: 0.7811 (0.7710)\n",
      "10-NN,s=0.1: TOP1:  69.46666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 219\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [219][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.7412 (0.7412)\n",
      "error:  0.008674290150020303 step  611\n",
      "cost:  0.4629583308522766\n",
      "opt took 0.00min,  611iters\n",
      "Epoch: [219][10/14.988]Time: 0.013 (0.038) Data: 0.001 (0.023) Loss: 0.7652 (0.7378)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-NN,s=0.1: TOP1:  67.43333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 220\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [220][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.7628 (0.7628)\n",
      "error:  0.008796354968154674 step  611\n",
      "cost:  0.45727562354201123\n",
      "opt took 0.00min,  611iters\n",
      "Epoch: [220][10/14.988]Time: 0.013 (0.030) Data: 0.000 (0.013) Loss: 0.7558 (0.7541)\n",
      "10-NN,s=0.1: TOP1:  68.66666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 221\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [221][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.7027 (0.7027)\n",
      "error:  0.00996301011630818 step  861\n",
      "cost:  0.5046845657668461\n",
      "opt took 0.00min,  861iters\n",
      "Epoch: [221][10/14.988]Time: 0.012 (0.034) Data: 0.000 (0.018) Loss: 0.8206 (0.8077)\n",
      "10-NN,s=0.1: TOP1:  67.66666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 222\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [222][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7683 (0.7683)\n",
      "error:  0.00983083116167316 step  981\n",
      "cost:  0.5010545709295738\n",
      "opt took 0.00min,  981iters\n",
      "Epoch: [222][10/14.988]Time: 0.011 (0.032) Data: 0.001 (0.017) Loss: 0.7822 (0.7584)\n",
      "10-NN,s=0.1: TOP1:  67.46666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 223\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [223][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.8083 (0.8083)\n",
      "error:  0.008947246908520334 step  791\n",
      "cost:  0.5544342851931938\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [223][10/14.988]Time: 0.012 (0.045) Data: 0.001 (0.028) Loss: 0.7555 (0.7906)\n",
      "10-NN,s=0.1: TOP1:  68.4\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 224\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [224][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7224 (0.7224)\n",
      "error:  0.00926900198172742 step  781\n",
      "cost:  0.5012868943276058\n",
      "opt took 0.00min,  781iters\n",
      "Epoch: [224][10/14.988]Time: 0.011 (0.031) Data: 0.000 (0.018) Loss: 0.7830 (0.7421)\n",
      "10-NN,s=0.1: TOP1:  68.93333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 225\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [225][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7507 (0.7507)\n",
      "error:  0.008202487168606387 step  371\n",
      "cost:  0.5184657084494148\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [225][10/14.988]Time: 0.013 (0.032) Data: 0.001 (0.016) Loss: 0.7382 (0.7556)\n",
      "10-NN,s=0.1: TOP1:  66.8\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 226\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [226][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.7623 (0.7623)\n",
      "error:  0.009381697465571714 step  711\n",
      "cost:  0.5458183634185526\n",
      "opt took 0.00min,  711iters\n",
      "Epoch: [226][10/14.988]Time: 0.011 (0.028) Data: 0.000 (0.014) Loss: 0.7568 (0.7320)\n",
      "10-NN,s=0.1: TOP1:  68.7\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 227\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [227][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.6953 (0.6953)\n",
      "error:  0.009133354249385328 step  451\n",
      "cost:  0.4637622725065258\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [227][10/14.988]Time: 0.012 (0.026) Data: 0.000 (0.013) Loss: 0.7708 (0.7130)\n",
      "10-NN,s=0.1: TOP1:  68.93333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 228\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [228][0/14.988]Time: 0.020 (0.020) Data: 0.002 (0.002) Loss: 0.8199 (0.8199)\n",
      "error:  0.009442952658565651 step  541\n",
      "cost:  0.5584246136866089\n",
      "opt took 0.00min,  541iters\n",
      "Epoch: [228][10/14.988]Time: 0.011 (0.034) Data: 0.000 (0.017) Loss: 0.7467 (0.7751)\n",
      "10-NN,s=0.1: TOP1:  70.46666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 229\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [229][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7539 (0.7539)\n",
      "error:  0.009214295429496366 step  821\n",
      "cost:  0.48203326593817974\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [229][10/14.988]Time: 0.014 (0.029) Data: 0.001 (0.016) Loss: 0.8368 (0.7870)\n",
      "10-NN,s=0.1: TOP1:  69.36666666666666\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 230\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [230][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.7701 (0.7701)\n",
      "error:  0.009694575434447406 step  861\n",
      "cost:  0.49520366434156693\n",
      "opt took 0.00min,  861iters\n",
      "Epoch: [230][10/14.988]Time: 0.012 (0.038) Data: 0.001 (0.024) Loss: 0.8620 (0.7644)\n",
      "10-NN,s=0.1: TOP1:  67.66666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 231\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [231][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.7537 (0.7537)\n",
      "error:  0.009676593787216303 step  1541\n",
      "cost:  0.4900277641708235\n",
      "opt took 0.01min, 1541iters\n",
      "Epoch: [231][10/14.988]Time: 0.011 (0.054) Data: 0.001 (0.038) Loss: 0.7513 (0.7657)\n",
      "10-NN,s=0.1: TOP1:  66.3\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 232\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [232][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8124 (0.8124)\n",
      "error:  0.008262291766750973 step  481\n",
      "cost:  0.5261323408158728\n",
      "opt took 0.00min,  481iters\n",
      "Epoch: [232][10/14.988]Time: 0.010 (0.027) Data: 0.000 (0.013) Loss: 0.7976 (0.7788)\n",
      "10-NN,s=0.1: TOP1:  68.73333333333333\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 233\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [233][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.7686 (0.7686)\n",
      "error:  0.009170139106832287 step  981\n",
      "cost:  0.5359608953063225\n",
      "opt took 0.00min,  981iters\n",
      "Epoch: [233][10/14.988]Time: 0.013 (0.033) Data: 0.001 (0.020) Loss: 0.7528 (0.7580)\n",
      "10-NN,s=0.1: TOP1:  68.36666666666666\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 234\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [234][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7669 (0.7669)\n",
      "error:  0.009898191048280824 step  1151\n",
      "cost:  0.480649127137364\n",
      "opt took 0.00min, 1151iters\n",
      "Epoch: [234][10/14.988]Time: 0.012 (0.045) Data: 0.001 (0.031) Loss: 0.6973 (0.7534)\n",
      "10-NN,s=0.1: TOP1:  70.4\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 235\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [235][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.7708 (0.7708)\n",
      "error:  0.009217757663682491 step  601\n",
      "cost:  0.4864517023657349\n",
      "opt took 0.00min,  601iters\n",
      "Epoch: [235][10/14.988]Time: 0.014 (0.032) Data: 0.001 (0.019) Loss: 0.7179 (0.7268)\n",
      "10-NN,s=0.1: TOP1:  67.16666666666667\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 236\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [236][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.6563 (0.6563)\n",
      "error:  0.009249063248520817 step  601\n",
      "cost:  0.48913442394168366\n",
      "opt took 0.00min,  601iters\n",
      "Epoch: [236][10/14.988]Time: 0.011 (0.035) Data: 0.000 (0.021) Loss: 0.7416 (0.7151)\n",
      "10-NN,s=0.1: TOP1:  68.93333333333334\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 237\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [237][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7525 (0.7525)\n",
      "error:  0.009550369419274451 step  1431\n",
      "cost:  0.4844632110680604\n",
      "opt took 0.01min, 1431iters\n",
      "Epoch: [237][10/14.988]Time: 0.012 (0.056) Data: 0.001 (0.041) Loss: 0.7803 (0.8067)\n",
      "10-NN,s=0.1: TOP1:  68.4\n",
      "best accuracy: 71.67\n",
      "\n",
      "Epoch: 238\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [238][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8526 (0.8526)\n",
      "error:  0.009107026002489915 step  711\n",
      "cost:  0.5779175588055595\n",
      "opt took 0.00min,  711iters\n",
      "Epoch: [238][10/14.988]Time: 0.013 (0.028) Data: 0.001 (0.015) Loss: 0.7784 (0.8773)\n",
      "10-NN,s=0.1: TOP1:  72.63333333333334\n",
      "Saving..\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 239\n",
      "LSTM\n",
      "0.9\n",
      "Epoch: [239][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.9296 (0.9296)\n",
      "error:  0.009284448931745182 step  981\n",
      "cost:  0.5442601061668272\n",
      "opt took 0.00min,  981iters\n",
      "Epoch: [239][10/14.988]Time: 0.013 (0.037) Data: 0.001 (0.024) Loss: 0.8852 (0.8664)\n",
      "10-NN,s=0.1: TOP1:  69.7\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 240\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [240][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.8045 (0.8045)\n",
      "error:  0.00949190586481874 step  561\n",
      "cost:  0.5110762817287288\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [240][10/14.988]Time: 0.014 (0.022) Data: 0.001 (0.010) Loss: 0.7316 (0.7563)\n",
      "10-NN,s=0.1: TOP1:  68.66666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 241\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [241][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.7106 (0.7106)\n",
      "error:  0.009179487772174255 step  631\n",
      "cost:  0.4610658589913768\n",
      "opt took 0.00min,  631iters\n",
      "Epoch: [241][10/14.988]Time: 0.014 (0.026) Data: 0.001 (0.013) Loss: 0.6915 (0.7085)\n",
      "10-NN,s=0.1: TOP1:  69.73333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 242\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [242][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.7016 (0.7016)\n",
      "error:  0.00997760252205937 step  821\n",
      "cost:  0.4911275866927574\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [242][10/14.988]Time: 0.015 (0.037) Data: 0.001 (0.023) Loss: 0.6507 (0.6824)\n",
      "10-NN,s=0.1: TOP1:  69.43333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 243\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [243][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.6738 (0.6738)\n",
      "error:  0.009741076275673999 step  441\n",
      "cost:  0.4668685960975175\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [243][10/14.988]Time: 0.014 (0.024) Data: 0.001 (0.011) Loss: 0.6414 (0.6688)\n",
      "10-NN,s=0.1: TOP1:  68.1\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 244\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [244][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.6836 (0.6836)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009170316588970828 step  511\n",
      "cost:  0.45488371749081025\n",
      "opt took 0.00min,  511iters\n",
      "Epoch: [244][10/14.988]Time: 0.011 (0.027) Data: 0.001 (0.014) Loss: 0.6631 (0.6608)\n",
      "10-NN,s=0.1: TOP1:  67.93333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 245\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [245][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.6551 (0.6551)\n",
      "error:  0.00894750823757906 step  791\n",
      "cost:  0.4411364298962272\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [245][10/14.988]Time: 0.015 (0.025) Data: 0.001 (0.012) Loss: 0.6308 (0.6552)\n",
      "10-NN,s=0.1: TOP1:  68.03333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 246\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [246][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.6481 (0.6481)\n",
      "error:  0.009130500969193212 step  731\n",
      "cost:  0.4319822898862079\n",
      "opt took 0.00min,  731iters\n",
      "Epoch: [246][10/14.988]Time: 0.014 (0.032) Data: 0.001 (0.017) Loss: 0.6468 (0.6410)\n",
      "10-NN,s=0.1: TOP1:  68.6\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 247\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [247][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.6151 (0.6151)\n",
      "error:  0.009997615070261423 step  961\n",
      "cost:  0.4383405169663759\n",
      "opt took 0.00min,  961iters\n",
      "Epoch: [247][10/14.988]Time: 0.014 (0.031) Data: 0.001 (0.018) Loss: 0.6210 (0.6179)\n",
      "10-NN,s=0.1: TOP1:  68.1\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 248\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [248][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5900 (0.5900)\n",
      "error:  0.00953570493696898 step  851\n",
      "cost:  0.483295462226579\n",
      "opt took 0.00min,  851iters\n",
      "Epoch: [248][10/14.988]Time: 0.014 (0.026) Data: 0.001 (0.013) Loss: 0.6449 (0.6168)\n",
      "10-NN,s=0.1: TOP1:  67.53333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 249\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [249][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.6387 (0.6387)\n",
      "error:  0.009085688776222067 step  821\n",
      "cost:  0.4542793924128083\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [249][10/14.988]Time: 0.014 (0.036) Data: 0.001 (0.023) Loss: 0.6076 (0.6028)\n",
      "10-NN,s=0.1: TOP1:  67.06666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 250\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [250][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5970 (0.5970)\n",
      "error:  0.00991071981073166 step  751\n",
      "cost:  0.41244265466745467\n",
      "opt took 0.00min,  751iters\n",
      "Epoch: [250][10/14.988]Time: 0.015 (0.041) Data: 0.001 (0.027) Loss: 0.5660 (0.5754)\n",
      "10-NN,s=0.1: TOP1:  68.03333333333333\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  64.06666666666666\n",
      "50-NN,s=0.5: TOP1:  64.0\n",
      "10-NN,s=0.1: TOP1:  67.93333333333334\n",
      "10-NN,s=0.5: TOP1:  67.53333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 251\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [251][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.5581 (0.5581)\n",
      "error:  0.009026448521704977 step  831\n",
      "cost:  0.37857093473871056\n",
      "opt took 0.00min,  831iters\n",
      "Epoch: [251][10/14.988]Time: 0.014 (0.031) Data: 0.001 (0.017) Loss: 0.5365 (0.5620)\n",
      "10-NN,s=0.1: TOP1:  67.7\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 252\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [252][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.5483 (0.5483)\n",
      "error:  0.00924317714555789 step  731\n",
      "cost:  0.4089790712991177\n",
      "opt took 0.00min,  731iters\n",
      "Epoch: [252][10/14.988]Time: 0.017 (0.027) Data: 0.001 (0.013) Loss: 0.5517 (0.5619)\n",
      "10-NN,s=0.1: TOP1:  67.43333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 253\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [253][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5866 (0.5866)\n",
      "error:  0.00936515258298154 step  771\n",
      "cost:  0.42050812295193213\n",
      "opt took 0.00min,  771iters\n",
      "Epoch: [253][10/14.988]Time: 0.014 (0.036) Data: 0.001 (0.024) Loss: 0.5560 (0.5706)\n",
      "10-NN,s=0.1: TOP1:  68.0\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 254\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [254][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5619 (0.5619)\n",
      "error:  0.009888995137163015 step  931\n",
      "cost:  0.41005170401139335\n",
      "opt took 0.00min,  931iters\n",
      "Epoch: [254][10/14.988]Time: 0.016 (0.033) Data: 0.001 (0.021) Loss: 0.5173 (0.5542)\n",
      "10-NN,s=0.1: TOP1:  67.5\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 255\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [255][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5839 (0.5839)\n",
      "error:  0.009385779783633086 step  901\n",
      "cost:  0.40353311546772713\n",
      "opt took 0.00min,  901iters\n",
      "Epoch: [255][10/14.988]Time: 0.014 (0.047) Data: 0.001 (0.030) Loss: 0.5689 (0.5581)\n",
      "10-NN,s=0.1: TOP1:  67.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 256\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [256][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.5596 (0.5596)\n",
      "error:  0.008491139399488867 step  561\n",
      "cost:  0.3931979560435083\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [256][10/14.988]Time: 0.014 (0.029) Data: 0.001 (0.016) Loss: 0.5675 (0.5810)\n",
      "10-NN,s=0.1: TOP1:  67.5\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 257\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [257][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.6326 (0.6326)\n",
      "error:  0.009989824717280826 step  951\n",
      "cost:  0.3818205958621108\n",
      "opt took 0.00min,  951iters\n",
      "Epoch: [257][10/14.988]Time: 0.014 (0.030) Data: 0.001 (0.018) Loss: 0.5879 (0.5810)\n",
      "10-NN,s=0.1: TOP1:  67.93333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 258\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [258][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.5535 (0.5535)\n",
      "error:  0.00932067164780681 step  711\n",
      "cost:  0.4238079390456531\n",
      "opt took 0.00min,  711iters\n",
      "Epoch: [258][10/14.988]Time: 0.020 (0.032) Data: 0.001 (0.018) Loss: 0.5397 (0.5580)\n",
      "10-NN,s=0.1: TOP1:  67.16666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 259\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [259][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5840 (0.5840)\n",
      "error:  0.00981334654774757 step  891\n",
      "cost:  0.4054048812494147\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [259][10/14.988]Time: 0.015 (0.043) Data: 0.001 (0.031) Loss: 0.5433 (0.5502)\n",
      "10-NN,s=0.1: TOP1:  66.63333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 260\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [260][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5691 (0.5691)\n",
      "error:  0.00958138072376713 step  1471\n",
      "cost:  0.3901179064456727\n",
      "opt took 0.00min, 1471iters\n",
      "Epoch: [260][10/14.988]Time: 0.014 (0.035) Data: 0.001 (0.023) Loss: 0.5579 (0.5401)\n",
      "10-NN,s=0.1: TOP1:  66.16666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 261\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [261][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5426 (0.5426)\n",
      "error:  0.009167110763543707 step  941\n",
      "cost:  0.36088477007668396\n",
      "opt took 0.00min,  941iters\n",
      "Epoch: [261][10/14.988]Time: 0.012 (0.041) Data: 0.001 (0.025) Loss: 0.5196 (0.5274)\n",
      "10-NN,s=0.1: TOP1:  66.7\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 262\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [262][0/14.988]Time: 0.016 (0.016) Data: 0.002 (0.002) Loss: 0.5326 (0.5326)\n",
      "error:  0.009879185984608019 step  1001\n",
      "cost:  0.3853666147103128\n",
      "opt took 0.00min, 1001iters\n",
      "Epoch: [262][10/14.988]Time: 0.022 (0.037) Data: 0.001 (0.022) Loss: 0.5104 (0.5276)\n",
      "10-NN,s=0.1: TOP1:  66.2\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 263\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [263][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5503 (0.5503)\n",
      "error:  0.009100176782525948 step  461\n",
      "cost:  0.4007872017435433\n",
      "opt took 0.00min,  461iters\n",
      "Epoch: [263][10/14.988]Time: 0.015 (0.035) Data: 0.001 (0.020) Loss: 0.5215 (0.5316)\n",
      "10-NN,s=0.1: TOP1:  67.4\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 264\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [264][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5333 (0.5333)\n",
      "error:  0.00996412506979627 step  591\n",
      "cost:  0.39202646150021275\n",
      "opt took 0.00min,  591iters\n",
      "Epoch: [264][10/14.988]Time: 0.014 (0.024) Data: 0.001 (0.011) Loss: 0.5133 (0.5319)\n",
      "10-NN,s=0.1: TOP1:  66.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 265\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [265][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.5018 (0.5018)\n",
      "error:  0.009196465044896462 step  831\n",
      "cost:  0.3799701823008871\n",
      "opt took 0.00min,  831iters\n",
      "Epoch: [265][10/14.988]Time: 0.016 (0.032) Data: 0.001 (0.018) Loss: 0.5397 (0.5164)\n",
      "10-NN,s=0.1: TOP1:  66.03333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 266\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [266][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.5054 (0.5054)\n",
      "error:  0.00981927599211474 step  821\n",
      "cost:  0.3955341732258191\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [266][10/14.988]Time: 0.015 (0.032) Data: 0.001 (0.018) Loss: 0.5423 (0.5246)\n",
      "10-NN,s=0.1: TOP1:  67.2\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 267\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [267][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5661 (0.5661)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009527304250717017 step  1121\n",
      "cost:  0.3669254007246366\n",
      "opt took 0.00min, 1121iters\n",
      "Epoch: [267][10/14.988]Time: 0.012 (0.044) Data: 0.001 (0.028) Loss: 0.5072 (0.5398)\n",
      "10-NN,s=0.1: TOP1:  67.6\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 268\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [268][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5279 (0.5279)\n",
      "error:  0.00936768268899102 step  1111\n",
      "cost:  0.38771781997757887\n",
      "opt took 0.01min, 1111iters\n",
      "Epoch: [268][10/14.988]Time: 0.015 (0.068) Data: 0.001 (0.056) Loss: 0.5280 (0.5342)\n",
      "10-NN,s=0.1: TOP1:  68.0\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 269\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [269][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5477 (0.5477)\n",
      "error:  0.009605736090084749 step  1211\n",
      "cost:  0.38890495192265206\n",
      "opt took 0.00min, 1211iters\n",
      "Epoch: [269][10/14.988]Time: 0.021 (0.035) Data: 0.001 (0.020) Loss: 0.5749 (0.5506)\n",
      "10-NN,s=0.1: TOP1:  66.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 270\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [270][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5607 (0.5607)\n",
      "error:  0.00955033754102963 step  1111\n",
      "cost:  0.38359579661862786\n",
      "opt took 0.01min, 1111iters\n",
      "Epoch: [270][10/14.988]Time: 0.017 (0.050) Data: 0.001 (0.038) Loss: 0.5132 (0.5495)\n",
      "10-NN,s=0.1: TOP1:  66.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 271\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [271][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.5452 (0.5452)\n",
      "error:  0.009657143455305883 step  1921\n",
      "cost:  0.34744171985172667\n",
      "opt took 0.00min, 1921iters\n",
      "Epoch: [271][10/14.988]Time: 0.014 (0.039) Data: 0.001 (0.027) Loss: 0.5198 (0.5207)\n",
      "10-NN,s=0.1: TOP1:  67.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 272\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [272][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.5016 (0.5016)\n",
      "error:  0.009819273165021958 step  1131\n",
      "cost:  0.38899549207040884\n",
      "opt took 0.00min, 1131iters\n",
      "Epoch: [272][10/14.988]Time: 0.014 (0.035) Data: 0.001 (0.024) Loss: 0.5364 (0.5192)\n",
      "10-NN,s=0.1: TOP1:  67.7\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 273\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [273][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4839 (0.4839)\n",
      "error:  0.009168957199884087 step  821\n",
      "cost:  0.39918014792208456\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [273][10/14.988]Time: 0.015 (0.041) Data: 0.001 (0.029) Loss: 0.5294 (0.5202)\n",
      "10-NN,s=0.1: TOP1:  68.03333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 274\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [274][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5227 (0.5227)\n",
      "error:  0.009584953929243967 step  791\n",
      "cost:  0.38306778573472405\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [274][10/14.988]Time: 0.019 (0.043) Data: 0.001 (0.030) Loss: 0.4881 (0.5110)\n",
      "10-NN,s=0.1: TOP1:  66.9\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 275\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [275][0/14.988]Time: 0.018 (0.018) Data: 0.002 (0.002) Loss: 0.4946 (0.4946)\n",
      "error:  0.009822612948944709 step  1201\n",
      "cost:  0.3652586085862147\n",
      "opt took 0.00min, 1201iters\n",
      "Epoch: [275][10/14.988]Time: 0.015 (0.036) Data: 0.001 (0.020) Loss: 0.5185 (0.5110)\n",
      "10-NN,s=0.1: TOP1:  67.0\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 276\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [276][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5079 (0.5079)\n",
      "error:  0.009356876161763616 step  1121\n",
      "cost:  0.3883223229676455\n",
      "opt took 0.01min, 1121iters\n",
      "Epoch: [276][10/14.988]Time: 0.015 (0.048) Data: 0.001 (0.036) Loss: 0.4838 (0.5034)\n",
      "10-NN,s=0.1: TOP1:  67.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 277\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [277][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4741 (0.4741)\n",
      "error:  0.009419069265751867 step  1261\n",
      "cost:  0.35829589896914793\n",
      "opt took 0.00min, 1261iters\n",
      "Epoch: [277][10/14.988]Time: 0.022 (0.044) Data: 0.001 (0.029) Loss: 0.5030 (0.4965)\n",
      "10-NN,s=0.1: TOP1:  67.63333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 278\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [278][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4992 (0.4992)\n",
      "error:  0.009349148861375611 step  851\n",
      "cost:  0.36504316128409636\n",
      "opt took 0.00min,  851iters\n",
      "Epoch: [278][10/14.988]Time: 0.015 (0.028) Data: 0.001 (0.015) Loss: 0.4951 (0.4987)\n",
      "10-NN,s=0.1: TOP1:  66.1\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 279\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [279][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4750 (0.4750)\n",
      "error:  0.009763829883346786 step  771\n",
      "cost:  0.37463862529065106\n",
      "opt took 0.00min,  771iters\n",
      "Epoch: [279][10/14.988]Time: 0.019 (0.043) Data: 0.001 (0.028) Loss: 0.4896 (0.4979)\n",
      "10-NN,s=0.1: TOP1:  66.46666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 280\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [280][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5128 (0.5128)\n",
      "error:  0.009708517690952045 step  891\n",
      "cost:  0.3686499315075893\n",
      "opt took 0.01min,  891iters\n",
      "Epoch: [280][10/14.988]Time: 0.014 (0.051) Data: 0.001 (0.039) Loss: 0.4988 (0.5001)\n",
      "10-NN,s=0.1: TOP1:  66.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 281\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [281][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4663 (0.4663)\n",
      "error:  0.00932180764887891 step  1111\n",
      "cost:  0.34974858458010355\n",
      "opt took 0.00min, 1111iters\n",
      "Epoch: [281][10/14.988]Time: 0.017 (0.038) Data: 0.001 (0.024) Loss: 0.5104 (0.5072)\n",
      "10-NN,s=0.1: TOP1:  65.83333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 282\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [282][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4809 (0.4809)\n",
      "error:  0.009610050893293698 step  1321\n",
      "cost:  0.3657991278621429\n",
      "opt took 0.00min, 1321iters\n",
      "Epoch: [282][10/14.988]Time: 0.016 (0.035) Data: 0.001 (0.023) Loss: 0.5306 (0.5177)\n",
      "10-NN,s=0.1: TOP1:  66.7\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 283\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [283][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4932 (0.4932)\n",
      "error:  0.00949966051779949 step  1091\n",
      "cost:  0.3872882697972782\n",
      "opt took 0.00min, 1091iters\n",
      "Epoch: [283][10/14.988]Time: 0.014 (0.031) Data: 0.001 (0.020) Loss: 0.5822 (0.5269)\n",
      "10-NN,s=0.1: TOP1:  66.5\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 284\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [284][0/14.988]Time: 0.018 (0.018) Data: 0.002 (0.002) Loss: 0.5007 (0.5007)\n",
      "error:  0.00978526559992643 step  351\n",
      "cost:  0.38712331462928906\n",
      "opt took 0.00min,  351iters\n",
      "Epoch: [284][10/14.988]Time: 0.016 (0.027) Data: 0.001 (0.011) Loss: 0.4924 (0.4955)\n",
      "10-NN,s=0.1: TOP1:  66.3\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 285\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [285][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4687 (0.4687)\n",
      "error:  0.009920051883656411 step  1161\n",
      "cost:  0.35117776587845057\n",
      "opt took 0.01min, 1161iters\n",
      "Epoch: [285][10/14.988]Time: 0.016 (0.068) Data: 0.001 (0.055) Loss: 0.4778 (0.4923)\n",
      "10-NN,s=0.1: TOP1:  66.7\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 286\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [286][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4921 (0.4921)\n",
      "error:  0.009649864141330289 step  1151\n",
      "cost:  0.3741307783853542\n",
      "opt took 0.00min, 1151iters\n",
      "Epoch: [286][10/14.988]Time: 0.027 (0.034) Data: 0.001 (0.021) Loss: 0.4721 (0.4888)\n",
      "10-NN,s=0.1: TOP1:  67.26666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 287\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [287][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.4610 (0.4610)\n",
      "error:  0.009957671248984612 step  941\n",
      "cost:  0.35050049428261965\n",
      "opt took 0.00min,  941iters\n",
      "Epoch: [287][10/14.988]Time: 0.015 (0.039) Data: 0.001 (0.026) Loss: 0.4645 (0.4863)\n",
      "10-NN,s=0.1: TOP1:  65.83333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 288\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [288][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.4896 (0.4896)\n",
      "error:  0.009815611735779273 step  1661\n",
      "cost:  0.34285429992872757\n",
      "opt took 0.01min, 1661iters\n",
      "Epoch: [288][10/14.988]Time: 0.014 (0.048) Data: 0.000 (0.036) Loss: 0.5181 (0.4975)\n",
      "10-NN,s=0.1: TOP1:  67.86666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 289\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [289][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.5105 (0.5105)\n",
      "error:  0.009367681799260397 step  941\n",
      "cost:  0.36994038661948897\n",
      "opt took 0.00min,  941iters\n",
      "Epoch: [289][10/14.988]Time: 0.021 (0.031) Data: 0.001 (0.017) Loss: 0.5011 (0.4967)\n",
      "10-NN,s=0.1: TOP1:  67.3\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 290\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [290][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4795 (0.4795)\n",
      "error:  0.00912847084126156 step  851\n",
      "cost:  0.3655779215787739\n",
      "opt took 0.00min,  851iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [290][10/14.988]Time: 0.015 (0.031) Data: 0.001 (0.019) Loss: 0.5035 (0.4913)\n",
      "10-NN,s=0.1: TOP1:  67.26666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 291\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [291][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4547 (0.4547)\n",
      "error:  0.009855417687183232 step  991\n",
      "cost:  0.3462313543030362\n",
      "opt took 0.01min,  991iters\n",
      "Epoch: [291][10/14.988]Time: 0.024 (0.049) Data: 0.001 (0.035) Loss: 0.4815 (0.4894)\n",
      "10-NN,s=0.1: TOP1:  66.46666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 292\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [292][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4695 (0.4695)\n",
      "error:  0.009389933380751558 step  1241\n",
      "cost:  0.3690494854856009\n",
      "opt took 0.01min, 1241iters\n",
      "Epoch: [292][10/14.988]Time: 0.014 (0.074) Data: 0.001 (0.062) Loss: 0.5094 (0.5008)\n",
      "10-NN,s=0.1: TOP1:  66.8\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 293\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [293][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5133 (0.5133)\n",
      "error:  0.009787596111992869 step  641\n",
      "cost:  0.40933462352573613\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [293][10/14.988]Time: 0.026 (0.029) Data: 0.001 (0.015) Loss: 0.4878 (0.4970)\n",
      "10-NN,s=0.1: TOP1:  67.1\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 294\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [294][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4772 (0.4772)\n",
      "error:  0.008994172659631539 step  601\n",
      "cost:  0.3702936936759343\n",
      "opt took 0.00min,  601iters\n",
      "Epoch: [294][10/14.988]Time: 0.014 (0.039) Data: 0.001 (0.027) Loss: 0.4732 (0.4780)\n",
      "10-NN,s=0.1: TOP1:  67.36666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 295\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [295][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5027 (0.5027)\n",
      "error:  0.009691230889527946 step  1121\n",
      "cost:  0.35579710449187213\n",
      "opt took 0.00min, 1121iters\n",
      "Epoch: [295][10/14.988]Time: 0.015 (0.032) Data: 0.001 (0.020) Loss: 0.5105 (0.4924)\n",
      "10-NN,s=0.1: TOP1:  65.8\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 296\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [296][0/14.988]Time: 0.017 (0.017) Data: 0.002 (0.002) Loss: 0.4836 (0.4836)\n",
      "error:  0.009261286422004189 step  961\n",
      "cost:  0.37562941902851316\n",
      "opt took 0.00min,  961iters\n",
      "Epoch: [296][10/14.988]Time: 0.015 (0.047) Data: 0.001 (0.033) Loss: 0.4967 (0.4852)\n",
      "10-NN,s=0.1: TOP1:  67.26666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 297\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [297][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4827 (0.4827)\n",
      "error:  0.009947805910566099 step  1301\n",
      "cost:  0.357346089483205\n",
      "opt took 0.00min, 1301iters\n",
      "Epoch: [297][10/14.988]Time: 0.015 (0.031) Data: 0.001 (0.020) Loss: 0.4883 (0.4826)\n",
      "10-NN,s=0.1: TOP1:  67.13333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 298\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [298][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4800 (0.4800)\n",
      "error:  0.009883253651008173 step  1121\n",
      "cost:  0.35322545795148996\n",
      "opt took 0.00min, 1121iters\n",
      "Epoch: [298][10/14.988]Time: 0.025 (0.033) Data: 0.001 (0.020) Loss: 0.5141 (0.4825)\n",
      "10-NN,s=0.1: TOP1:  67.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 299\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [299][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5211 (0.5211)\n",
      "error:  0.009422471342198757 step  1571\n",
      "cost:  0.3747925412231122\n",
      "opt took 0.00min, 1571iters\n",
      "Epoch: [299][10/14.988]Time: 0.014 (0.039) Data: 0.001 (0.027) Loss: 0.5731 (0.5260)\n",
      "10-NN,s=0.1: TOP1:  66.33333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 300\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [300][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.5345 (0.5345)\n",
      "error:  0.009471436524235433 step  1011\n",
      "cost:  0.36827005118424794\n",
      "opt took 0.00min, 1011iters\n",
      "Epoch: [300][10/14.988]Time: 0.016 (0.033) Data: 0.001 (0.020) Loss: 0.5047 (0.5263)\n",
      "10-NN,s=0.1: TOP1:  68.23333333333333\n",
      "Saving..\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  66.53333333333333\n",
      "50-NN,s=0.5: TOP1:  66.16666666666667\n",
      "10-NN,s=0.1: TOP1:  68.86666666666666\n",
      "10-NN,s=0.5: TOP1:  68.86666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 301\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [301][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5282 (0.5282)\n",
      "error:  0.009872591290621013 step  741\n",
      "cost:  0.3537390178220575\n",
      "opt took 0.00min,  741iters\n",
      "Epoch: [301][10/14.988]Time: 0.018 (0.027) Data: 0.001 (0.014) Loss: 0.4931 (0.4999)\n",
      "10-NN,s=0.1: TOP1:  67.8\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 302\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [302][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4522 (0.4522)\n",
      "error:  0.00948840291361952 step  1401\n",
      "cost:  0.3574218623944453\n",
      "opt took 0.00min, 1401iters\n",
      "Epoch: [302][10/14.988]Time: 0.016 (0.031) Data: 0.001 (0.020) Loss: 0.5091 (0.4902)\n",
      "10-NN,s=0.1: TOP1:  67.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 303\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [303][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4942 (0.4942)\n",
      "error:  0.009247811821509466 step  891\n",
      "cost:  0.4099475054746516\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [303][10/14.988]Time: 0.015 (0.029) Data: 0.001 (0.017) Loss: 0.4807 (0.4965)\n",
      "10-NN,s=0.1: TOP1:  67.3\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 304\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [304][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4980 (0.4980)\n",
      "error:  0.009697244513166448 step  1041\n",
      "cost:  0.35915267387574495\n",
      "opt took 0.00min, 1041iters\n",
      "Epoch: [304][10/14.988]Time: 0.015 (0.032) Data: 0.001 (0.021) Loss: 0.5107 (0.5153)\n",
      "10-NN,s=0.1: TOP1:  67.13333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 305\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [305][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.5473 (0.5473)\n",
      "error:  0.009305004819306295 step  1051\n",
      "cost:  0.3609951873574802\n",
      "opt took 0.00min, 1051iters\n",
      "Epoch: [305][10/14.988]Time: 0.015 (0.037) Data: 0.001 (0.026) Loss: 0.5551 (0.5389)\n",
      "10-NN,s=0.1: TOP1:  68.06666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 306\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [306][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5367 (0.5367)\n",
      "error:  0.009665776742401988 step  1731\n",
      "cost:  0.3982539001563049\n",
      "opt took 0.00min, 1731iters\n",
      "Epoch: [306][10/14.988]Time: 0.015 (0.042) Data: 0.001 (0.031) Loss: 0.5505 (0.5563)\n",
      "10-NN,s=0.1: TOP1:  68.16666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 307\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [307][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.5137 (0.5137)\n",
      "error:  0.00938171847368141 step  801\n",
      "cost:  0.36932116490355665\n",
      "opt took 0.00min,  801iters\n",
      "Epoch: [307][10/14.988]Time: 0.018 (0.032) Data: 0.001 (0.019) Loss: 0.5008 (0.5285)\n",
      "10-NN,s=0.1: TOP1:  66.86666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 308\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [308][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5332 (0.5332)\n",
      "error:  0.009894728254185914 step  861\n",
      "cost:  0.3673857401736711\n",
      "opt took 0.00min,  861iters\n",
      "Epoch: [308][10/14.988]Time: 0.021 (0.035) Data: 0.001 (0.023) Loss: 0.5066 (0.5200)\n",
      "10-NN,s=0.1: TOP1:  68.13333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 309\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [309][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.5398 (0.5398)\n",
      "error:  0.009274549805999843 step  1021\n",
      "cost:  0.39491081827344116\n",
      "opt took 0.00min, 1021iters\n",
      "Epoch: [309][10/14.988]Time: 0.016 (0.037) Data: 0.001 (0.025) Loss: 0.5579 (0.5411)\n",
      "10-NN,s=0.1: TOP1:  66.86666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 310\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [310][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.5552 (0.5552)\n",
      "error:  0.009574280654002676 step  941\n",
      "cost:  0.37395026858382685\n",
      "opt took 0.00min,  941iters\n",
      "Epoch: [310][10/14.988]Time: 0.020 (0.032) Data: 0.001 (0.019) Loss: 0.5625 (0.5621)\n",
      "10-NN,s=0.1: TOP1:  67.9\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 311\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [311][0/14.988]Time: 0.018 (0.018) Data: 0.002 (0.002) Loss: 0.5455 (0.5455)\n",
      "error:  0.009235955339176649 step  771\n",
      "cost:  0.3494219947705639\n",
      "opt took 0.00min,  771iters\n",
      "Epoch: [311][10/14.988]Time: 0.017 (0.043) Data: 0.001 (0.029) Loss: 0.4860 (0.5218)\n",
      "10-NN,s=0.1: TOP1:  68.93333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 312\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [312][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4982 (0.4982)\n",
      "error:  0.009720971293183722 step  1401\n",
      "cost:  0.3662193512794742\n",
      "opt took 0.01min, 1401iters\n",
      "Epoch: [312][10/14.988]Time: 0.015 (0.048) Data: 0.001 (0.037) Loss: 0.5382 (0.5107)\n",
      "10-NN,s=0.1: TOP1:  68.76666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 313\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [313][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.5372 (0.5372)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009632503585377705 step  1191\n",
      "cost:  0.41625810931946516\n",
      "opt took 0.00min, 1191iters\n",
      "Epoch: [313][10/14.988]Time: 0.015 (0.031) Data: 0.001 (0.020) Loss: 0.5314 (0.5181)\n",
      "10-NN,s=0.1: TOP1:  68.26666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 314\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [314][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5139 (0.5139)\n",
      "error:  0.009686539710522357 step  1011\n",
      "cost:  0.38084740670960276\n",
      "opt took 0.00min, 1011iters\n",
      "Epoch: [314][10/14.988]Time: 0.013 (0.032) Data: 0.001 (0.021) Loss: 0.5133 (0.5137)\n",
      "10-NN,s=0.1: TOP1:  69.03333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 315\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [315][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.5029 (0.5029)\n",
      "error:  0.009605959236276163 step  1101\n",
      "cost:  0.3699347750352784\n",
      "opt took 0.00min, 1101iters\n",
      "Epoch: [315][10/14.988]Time: 0.014 (0.027) Data: 0.001 (0.016) Loss: 0.5174 (0.5076)\n",
      "10-NN,s=0.1: TOP1:  67.63333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 316\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [316][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.5107 (0.5107)\n",
      "error:  0.00953764166710347 step  791\n",
      "cost:  0.38562872766199424\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [316][10/14.988]Time: 0.018 (0.029) Data: 0.001 (0.018) Loss: 0.4856 (0.5067)\n",
      "10-NN,s=0.1: TOP1:  68.1\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 317\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [317][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.5143 (0.5143)\n",
      "error:  0.009321939094301568 step  1021\n",
      "cost:  0.3490946874941102\n",
      "opt took 0.00min, 1021iters\n",
      "Epoch: [317][10/14.988]Time: 0.017 (0.037) Data: 0.001 (0.025) Loss: 0.4797 (0.5083)\n",
      "10-NN,s=0.1: TOP1:  67.96666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 318\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [318][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.5016 (0.5016)\n",
      "error:  0.009865962360858682 step  811\n",
      "cost:  0.3526055273304109\n",
      "opt took 0.00min,  811iters\n",
      "Epoch: [318][10/14.988]Time: 0.017 (0.033) Data: 0.001 (0.022) Loss: 0.5162 (0.5020)\n",
      "10-NN,s=0.1: TOP1:  66.96666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 319\n",
      "LSTM\n",
      "0.09000000000000001\n",
      "Epoch: [319][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4900 (0.4900)\n",
      "error:  0.009142972055761978 step  1041\n",
      "cost:  0.3849109691249712\n",
      "opt took 0.00min, 1041iters\n",
      "Epoch: [319][10/14.988]Time: 0.017 (0.028) Data: 0.001 (0.017) Loss: 0.5076 (0.5013)\n",
      "10-NN,s=0.1: TOP1:  68.2\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 320\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [320][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.4834 (0.4834)\n",
      "error:  0.009453357377296578 step  1241\n",
      "cost:  0.36141379345615976\n",
      "opt took 0.00min, 1241iters\n",
      "Epoch: [320][10/14.988]Time: 0.017 (0.037) Data: 0.001 (0.024) Loss: 0.4617 (0.4831)\n",
      "10-NN,s=0.1: TOP1:  66.8\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 321\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [321][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.4626 (0.4626)\n",
      "error:  0.009498334276205056 step  1301\n",
      "cost:  0.3396805443971945\n",
      "opt took 0.00min, 1301iters\n",
      "Epoch: [321][10/14.988]Time: 0.258 (0.035) Data: 0.244 (0.023) Loss: 0.4804 (0.4704)\n",
      "10-NN,s=0.1: TOP1:  68.4\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 322\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [322][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4535 (0.4535)\n",
      "error:  0.009446584853566087 step  831\n",
      "cost:  0.3486746105303045\n",
      "opt took 0.00min,  831iters\n",
      "Epoch: [322][10/14.988]Time: 0.139 (0.022) Data: 0.123 (0.012) Loss: 0.4938 (0.4751)\n",
      "10-NN,s=0.1: TOP1:  68.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 323\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [323][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.4767 (0.4767)\n",
      "error:  0.009929469138125535 step  1361\n",
      "cost:  0.40506606955290103\n",
      "opt took 0.00min, 1361iters\n",
      "Epoch: [323][10/14.988]Time: 0.294 (0.038) Data: 0.277 (0.026) Loss: 0.4690 (0.4861)\n",
      "10-NN,s=0.1: TOP1:  67.96666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 324\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [324][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4647 (0.4647)\n",
      "error:  0.009517317575930218 step  1171\n",
      "cost:  0.35562324053522837\n",
      "opt took 0.00min, 1171iters\n",
      "Epoch: [324][10/14.988]Time: 0.318 (0.038) Data: 0.297 (0.028) Loss: 0.4933 (0.4793)\n",
      "10-NN,s=0.1: TOP1:  67.73333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 325\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [325][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4575 (0.4575)\n",
      "error:  0.009667474708469515 step  1111\n",
      "cost:  0.3488661663538352\n",
      "opt took 0.01min, 1111iters\n",
      "Epoch: [325][10/14.988]Time: 0.392 (0.046) Data: 0.378 (0.035) Loss: 0.4652 (0.4764)\n",
      "10-NN,s=0.1: TOP1:  68.33333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 326\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [326][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4707 (0.4707)\n",
      "error:  0.009788618943041838 step  1691\n",
      "cost:  0.37160352858555323\n",
      "opt took 0.01min, 1691iters\n",
      "Epoch: [326][10/14.988]Time: 0.438 (0.050) Data: 0.418 (0.038) Loss: 0.4741 (0.4706)\n",
      "10-NN,s=0.1: TOP1:  69.03333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 327\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [327][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4879 (0.4879)\n",
      "error:  0.009734991640790391 step  1351\n",
      "cost:  0.3455696880436071\n",
      "opt took 0.00min, 1351iters\n",
      "Epoch: [327][10/14.988]Time: 0.249 (0.032) Data: 0.231 (0.022) Loss: 0.4708 (0.4733)\n",
      "10-NN,s=0.1: TOP1:  69.8\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 328\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [328][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4438 (0.4438)\n",
      "error:  0.009424406149700637 step  1321\n",
      "cost:  0.34485446821707844\n",
      "opt took 0.00min, 1321iters\n",
      "Epoch: [328][10/14.988]Time: 0.270 (0.035) Data: 0.256 (0.024) Loss: 0.4557 (0.4613)\n",
      "10-NN,s=0.1: TOP1:  69.3\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 329\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [329][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4475 (0.4475)\n",
      "error:  0.009293529102224585 step  1001\n",
      "cost:  0.38630932931124684\n",
      "opt took 0.00min, 1001iters\n",
      "Epoch: [329][10/14.988]Time: 0.253 (0.033) Data: 0.239 (0.022) Loss: 0.4536 (0.4556)\n",
      "10-NN,s=0.1: TOP1:  69.16666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 330\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [330][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4603 (0.4603)\n",
      "error:  0.009488163491216395 step  1301\n",
      "cost:  0.34685813420045436\n",
      "opt took 0.01min, 1301iters\n",
      "Epoch: [330][10/14.988]Time: 0.404 (0.047) Data: 0.390 (0.036) Loss: 0.4800 (0.4642)\n",
      "10-NN,s=0.1: TOP1:  68.13333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 331\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [331][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4721 (0.4721)\n",
      "error:  0.009594824182089456 step  1391\n",
      "cost:  0.3331225738304353\n",
      "opt took 0.00min, 1391iters\n",
      "Epoch: [331][10/14.988]Time: 0.225 (0.030) Data: 0.211 (0.020) Loss: 0.4517 (0.4683)\n",
      "10-NN,s=0.1: TOP1:  68.33333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 332\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [332][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.4381 (0.4381)\n",
      "error:  0.009992822508213428 step  1321\n",
      "cost:  0.34518952969861577\n",
      "opt took 0.00min, 1321iters\n",
      "Epoch: [332][10/14.988]Time: 0.281 (0.037) Data: 0.266 (0.025) Loss: 0.5044 (0.4591)\n",
      "10-NN,s=0.1: TOP1:  67.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 333\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [333][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4328 (0.4328)\n",
      "error:  0.009955627275852419 step  1621\n",
      "cost:  0.3982553987419383\n",
      "opt took 0.00min, 1621iters\n",
      "Epoch: [333][10/14.988]Time: 0.223 (0.030) Data: 0.208 (0.019) Loss: 0.4694 (0.4631)\n",
      "10-NN,s=0.1: TOP1:  68.73333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 334\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [334][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4554 (0.4554)\n",
      "Epoch: [334][10/14.988]Time: 0.010 (0.011) Data: 0.000 (0.001) Loss: 0.4408 (0.4586)\n",
      "error:  0.009157918457779468 step  931\n",
      "cost:  0.3518101197547122\n",
      "opt took 0.00min,  931iters\n",
      "10-NN,s=0.1: TOP1:  68.13333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 335\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [335][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4632 (0.4632)\n",
      "Epoch: [335][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.000) Loss: 0.4764 (0.4779)\n",
      "error:  0.009923193431224386 step  1001\n",
      "cost:  0.34165549219048613\n",
      "opt took 0.00min, 1001iters\n",
      "10-NN,s=0.1: TOP1:  69.13333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 336\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [336][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.4689 (0.4689)\n",
      "Epoch: [336][10/14.988]Time: 0.012 (0.013) Data: 0.001 (0.001) Loss: 0.4752 (0.4725)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.008916480132632465 step  481\n",
      "cost:  0.3683167296925553\n",
      "opt took 0.00min,  481iters\n",
      "10-NN,s=0.1: TOP1:  68.5\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 337\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [337][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4794 (0.4794)\n",
      "Epoch: [337][10/14.988]Time: 0.012 (0.011) Data: 0.001 (0.001) Loss: 0.4793 (0.4677)\n",
      "error:  0.009644612334796077 step  1721\n",
      "cost:  0.3439784759372267\n",
      "opt took 0.00min, 1721iters\n",
      "10-NN,s=0.1: TOP1:  69.06666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 338\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [338][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4914 (0.4914)\n",
      "Epoch: [338][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.4661 (0.4655)\n",
      "error:  0.009905284909882517 step  1411\n",
      "cost:  0.3375520214261208\n",
      "opt took 0.01min, 1411iters\n",
      "10-NN,s=0.1: TOP1:  68.86666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 339\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [339][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4769 (0.4769)\n",
      "Epoch: [339][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4632 (0.4573)\n",
      "error:  0.009680718506011599 step  1681\n",
      "cost:  0.3674002275298866\n",
      "opt took 0.01min, 1681iters\n",
      "10-NN,s=0.1: TOP1:  68.93333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 340\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [340][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.4526 (0.4526)\n",
      "Epoch: [340][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4576 (0.4595)\n",
      "error:  0.009632063115925904 step  1321\n",
      "cost:  0.34252694436106534\n",
      "opt took 0.00min, 1321iters\n",
      "10-NN,s=0.1: TOP1:  68.66666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 341\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [341][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4403 (0.4403)\n",
      "Epoch: [341][10/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4562 (0.4566)\n",
      "error:  0.009560985531094857 step  1081\n",
      "cost:  0.32826359905811386\n",
      "opt took 0.00min, 1081iters\n",
      "10-NN,s=0.1: TOP1:  68.5\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 342\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [342][0/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.4372 (0.4372)\n",
      "Epoch: [342][10/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4761 (0.4579)\n",
      "error:  0.00990280499578966 step  1241\n",
      "cost:  0.3491387506778324\n",
      "opt took 0.00min, 1241iters\n",
      "10-NN,s=0.1: TOP1:  68.93333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 343\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [343][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.4468 (0.4468)\n",
      "Epoch: [343][10/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4525 (0.4575)\n",
      "error:  0.009685074179894948 step  1211\n",
      "cost:  0.3963280294926881\n",
      "opt took 0.01min, 1211iters\n",
      "10-NN,s=0.1: TOP1:  69.63333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 344\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [344][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4477 (0.4477)\n",
      "Epoch: [344][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4536 (0.4546)\n",
      "error:  0.009526702813971721 step  1251\n",
      "cost:  0.348223912017971\n",
      "opt took 0.01min, 1251iters\n",
      "10-NN,s=0.1: TOP1:  69.0\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 345\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [345][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4390 (0.4390)\n",
      "Epoch: [345][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.4624 (0.4501)\n",
      "error:  0.009288460622960737 step  941\n",
      "cost:  0.3371272731957376\n",
      "opt took 0.00min,  941iters\n",
      "10-NN,s=0.1: TOP1:  69.3\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 346\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [346][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.4539 (0.4539)\n",
      "Epoch: [346][10/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4258 (0.4497)\n",
      "error:  0.009926577805111969 step  1181\n",
      "cost:  0.3667836630649473\n",
      "opt took 0.00min, 1181iters\n",
      "10-NN,s=0.1: TOP1:  69.16666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 347\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [347][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4485 (0.4485)\n",
      "Epoch: [347][10/14.988]Time: 0.010 (0.011) Data: 0.001 (0.001) Loss: 0.4561 (0.4519)\n",
      "error:  0.009744463832470296 step  1131\n",
      "cost:  0.3362927942758509\n",
      "opt took 0.00min, 1131iters\n",
      "10-NN,s=0.1: TOP1:  69.03333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 348\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [348][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.4584 (0.4584)\n",
      "Epoch: [348][10/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4511 (0.4489)\n",
      "error:  0.009768924991225836 step  1281\n",
      "cost:  0.33078383138097156\n",
      "opt took 0.00min, 1281iters\n",
      "10-NN,s=0.1: TOP1:  69.06666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 349\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [349][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4275 (0.4275)\n",
      "Epoch: [349][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4300 (0.4438)\n",
      "error:  0.009927248508632802 step  1231\n",
      "cost:  0.37466225825170246\n",
      "opt took 0.00min, 1231iters\n",
      "10-NN,s=0.1: TOP1:  68.8\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 350\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [350][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4328 (0.4328)\n",
      "Epoch: [350][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4603 (0.4564)\n",
      "error:  0.009468892255966366 step  1171\n",
      "cost:  0.3325250765024362\n",
      "opt took 0.00min, 1171iters\n",
      "10-NN,s=0.1: TOP1:  69.06666666666666\n",
      "doing PCA with 10 components ..done\n",
      "50-NN,s=0.1: TOP1:  64.06666666666666\n",
      "50-NN,s=0.5: TOP1:  64.33333333333333\n",
      "10-NN,s=0.1: TOP1:  69.16666666666667\n",
      "10-NN,s=0.5: TOP1:  69.0\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 351\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [351][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.4466 (0.4466)\n",
      "Epoch: [351][10/14.988]Time: 0.011 (0.013) Data: 0.000 (0.001) Loss: 0.4481 (0.4488)\n",
      "error:  0.009711639139138195 step  1371\n",
      "cost:  0.3239600146653094\n",
      "opt took 0.01min, 1371iters\n",
      "10-NN,s=0.1: TOP1:  69.2\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 352\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [352][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.4374 (0.4374)\n",
      "Epoch: [352][10/14.988]Time: 0.011 (0.012) Data: 0.001 (0.001) Loss: 0.4419 (0.4463)\n",
      "error:  0.009878944639989284 step  1321\n",
      "cost:  0.35483758877993593\n",
      "opt took 0.01min, 1321iters\n",
      "10-NN,s=0.1: TOP1:  69.03333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 353\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [353][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4408 (0.4408)\n",
      "Epoch: [353][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4596 (0.4499)\n",
      "error:  0.009382839213547545 step  1371\n",
      "cost:  0.39435324490682827\n",
      "opt took 0.00min, 1371iters\n",
      "10-NN,s=0.1: TOP1:  68.83333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 354\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [354][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4444 (0.4444)\n",
      "Epoch: [354][10/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4519 (0.4476)\n",
      "error:  0.00941250236140223 step  981\n",
      "cost:  0.3513249505711876\n",
      "opt took 0.00min,  981iters\n",
      "10-NN,s=0.1: TOP1:  70.0\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 355\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [355][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4351 (0.4351)\n",
      "Epoch: [355][10/14.988]Time: 0.017 (0.011) Data: 0.001 (0.001) Loss: 0.4546 (0.4381)\n",
      "error:  0.009733169473661318 step  1241\n",
      "cost:  0.3295554543067805\n",
      "opt took 0.00min, 1241iters\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 356\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [356][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4511 (0.4511)\n",
      "Epoch: [356][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.4439 (0.4429)\n",
      "error:  0.009720922212233751 step  1701\n",
      "cost:  0.3633388814101417\n",
      "opt took 0.00min, 1701iters\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 357\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [357][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4290 (0.4290)\n",
      "Epoch: [357][10/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4551 (0.4392)\n",
      "error:  0.009905424701936671 step  1281\n",
      "cost:  0.33025532182771095\n",
      "opt took 0.00min, 1281iters\n",
      "10-NN,s=0.1: TOP1:  70.0\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 358\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [358][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4860 (0.4860)\n",
      "Epoch: [358][10/14.988]Time: 0.011 (0.011) Data: 0.000 (0.001) Loss: 0.4327 (0.4425)\n",
      "error:  0.009665359617299418 step  1181\n",
      "cost:  0.3249696815365557\n",
      "opt took 0.00min, 1181iters\n",
      "10-NN,s=0.1: TOP1:  69.7\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 359\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [359][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4245 (0.4245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [359][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4203 (0.4371)\n",
      "error:  0.009995081383506688 step  1281\n",
      "cost:  0.3646993879617299\n",
      "opt took 0.00min, 1281iters\n",
      "10-NN,s=0.1: TOP1:  69.66666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 360\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [360][0/14.988]Time: 0.014 (0.014) Data: 0.001 (0.001) Loss: 0.4805 (0.4805)\n",
      "Epoch: [360][10/14.988]Time: 0.012 (0.012) Data: 0.000 (0.001) Loss: 0.4496 (0.4615)\n",
      "error:  0.009411621544060056 step  1221\n",
      "cost:  0.3294104022657716\n",
      "opt took 0.00min, 1221iters\n",
      "10-NN,s=0.1: TOP1:  69.83333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 361\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [361][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4604 (0.4604)\n",
      "Epoch: [361][10/14.988]Time: 0.010 (0.011) Data: 0.000 (0.001) Loss: 0.4725 (0.4585)\n",
      "error:  0.009978419995764543 step  1171\n",
      "cost:  0.31841756085876116\n",
      "opt took 0.00min, 1171iters\n",
      "10-NN,s=0.1: TOP1:  69.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 362\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [362][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4339 (0.4339)\n",
      "Epoch: [362][10/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4717 (0.4582)\n",
      "error:  0.009756898262565983 step  1341\n",
      "cost:  0.3443292581328629\n",
      "opt took 0.00min, 1341iters\n",
      "10-NN,s=0.1: TOP1:  69.5\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 363\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [363][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4600 (0.4600)\n",
      "Epoch: [363][10/14.988]Time: 0.010 (0.011) Data: 0.000 (0.001) Loss: 0.4513 (0.4565)\n",
      "error:  0.009337620920959577 step  941\n",
      "cost:  0.39531041009160056\n",
      "opt took 0.01min,  941iters\n",
      "10-NN,s=0.1: TOP1:  69.86666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 364\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [364][0/14.988]Time: 0.016 (0.016) Data: 0.002 (0.002) Loss: 0.4984 (0.4984)\n",
      "Epoch: [364][10/14.988]Time: 0.013 (0.013) Data: 0.001 (0.001) Loss: 0.4310 (0.4572)\n",
      "error:  0.00974643042971246 step  1521\n",
      "cost:  0.34382611365790183\n",
      "opt took 0.00min, 1521iters\n",
      "10-NN,s=0.1: TOP1:  69.2\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 365\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [365][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4313 (0.4313)\n",
      "Epoch: [365][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.000) Loss: 0.4472 (0.4503)\n",
      "error:  0.00967764498570789 step  1121\n",
      "cost:  0.3255703709819328\n",
      "opt took 0.00min, 1121iters\n",
      "10-NN,s=0.1: TOP1:  68.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 366\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [366][0/14.988]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 0.4435 (0.4435)\n",
      "Epoch: [366][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4637 (0.4528)\n",
      "error:  0.009522168019718724 step  1241\n",
      "cost:  0.36059278357730923\n",
      "opt took 0.00min, 1241iters\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 367\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [367][0/14.988]Time: 0.015 (0.015) Data: 0.001 (0.001) Loss: 0.4354 (0.4354)\n",
      "Epoch: [367][10/14.988]Time: 0.012 (0.013) Data: 0.001 (0.001) Loss: 0.4290 (0.4487)\n",
      "error:  0.009751197596891137 step  1661\n",
      "cost:  0.3312337758115841\n",
      "opt took 0.00min, 1661iters\n",
      "10-NN,s=0.1: TOP1:  68.73333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 368\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [368][0/14.988]Time: 0.014 (0.014) Data: 0.002 (0.002) Loss: 0.4524 (0.4524)\n",
      "Epoch: [368][10/14.988]Time: 0.012 (0.013) Data: 0.001 (0.001) Loss: 0.4518 (0.4418)\n",
      "error:  0.009339909661997603 step  981\n",
      "cost:  0.32555238958418875\n",
      "opt took 0.01min,  981iters\n",
      "10-NN,s=0.1: TOP1:  68.86666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 369\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [369][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.4214 (0.4214)\n",
      "Epoch: [369][10/14.988]Time: 0.011 (0.012) Data: 0.000 (0.001) Loss: 0.4444 (0.4387)\n",
      "error:  0.00941971868447089 step  1391\n",
      "cost:  0.3551850705192921\n",
      "opt took 0.00min, 1391iters\n",
      "10-NN,s=0.1: TOP1:  69.3\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 370\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [370][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4614 (0.4614)\n",
      "Epoch: [370][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4443 (0.4453)\n",
      "error:  0.009872894458703985 step  1481\n",
      "cost:  0.32724683668530646\n",
      "opt took 0.00min, 1481iters\n",
      "10-NN,s=0.1: TOP1:  69.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 371\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [371][0/14.988]Time: 0.013 (0.013) Data: 0.002 (0.002) Loss: 0.4237 (0.4237)\n",
      "Epoch: [371][10/14.988]Time: 0.011 (0.012) Data: 0.001 (0.001) Loss: 0.3982 (0.4349)\n",
      "error:  0.00959449494616127 step  1291\n",
      "cost:  0.3173249584581953\n",
      "opt took 0.01min, 1291iters\n",
      "10-NN,s=0.1: TOP1:  69.2\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 372\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [372][0/14.988]Time: 0.011 (0.011) Data: 0.001 (0.001) Loss: 0.4424 (0.4424)\n",
      "Epoch: [372][10/14.988]Time: 0.010 (0.011) Data: 0.001 (0.001) Loss: 0.4579 (0.4352)\n",
      "error:  0.009428754452956878 step  941\n",
      "cost:  0.3485074717741856\n",
      "opt took 0.00min,  941iters\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 373\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [373][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4680 (0.4680)\n",
      "Epoch: [373][10/14.988]Time: 0.010 (0.010) Data: 0.000 (0.001) Loss: 0.4250 (0.4407)\n",
      "error:  0.00957291666554061 step  631\n",
      "cost:  0.38622851516724593\n",
      "opt took 0.00min,  631iters\n",
      "10-NN,s=0.1: TOP1:  69.5\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 374\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "Epoch: [374][0/14.988]Time: 0.012 (0.012) Data: 0.001 (0.001) Loss: 0.4139 (0.4139)\n",
      "Epoch: [374][10/14.988]Time: 0.010 (0.011) Data: 0.000 (0.001) Loss: 0.4288 (0.4352)\n",
      "10-NN,s=0.1: TOP1:  69.36666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 375\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.00995465557905939 step  851\n",
      "cost:  0.32542190265534576\n",
      "opt took 0.01min,  851iters\n",
      "Epoch: [375][0/14.988]Time: 0.458 (0.458) Data: 0.442 (0.442) Loss: 0.4315 (0.4315)\n",
      "Epoch: [375][10/14.988]Time: 0.011 (0.055) Data: 0.001 (0.041) Loss: 0.4281 (0.4329)\n",
      "10-NN,s=0.1: TOP1:  69.66666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 376\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009833739090561844 step  671\n",
      "cost:  0.3631593833388426\n",
      "opt took 0.00min,  671iters\n",
      "Epoch: [376][0/14.988]Time: 0.239 (0.239) Data: 0.223 (0.223) Loss: 0.4261 (0.4261)\n",
      "Epoch: [376][10/14.988]Time: 0.011 (0.036) Data: 0.000 (0.021) Loss: 0.4410 (0.4347)\n",
      "10-NN,s=0.1: TOP1:  69.16666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 377\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009393114083428133 step  1031\n",
      "cost:  0.33205231250546313\n",
      "opt took 0.01min, 1031iters\n",
      "Epoch: [377][0/14.988]Time: 0.501 (0.501) Data: 0.484 (0.484) Loss: 0.4440 (0.4440)\n",
      "Epoch: [377][10/14.988]Time: 0.014 (0.061) Data: 0.001 (0.045) Loss: 0.4334 (0.4387)\n",
      "10-NN,s=0.1: TOP1:  69.9\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 378\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009559036913922792 step  1501\n",
      "cost:  0.32476601239628006\n",
      "opt took 0.01min, 1501iters\n",
      "Epoch: [378][0/14.988]Time: 0.603 (0.603) Data: 0.588 (0.588) Loss: 0.4389 (0.4389)\n",
      "Epoch: [378][10/14.988]Time: 0.011 (0.069) Data: 0.000 (0.054) Loss: 0.4213 (0.4304)\n",
      "10-NN,s=0.1: TOP1:  69.3\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 379\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009652243200165334 step  1561\n",
      "cost:  0.3678104676810155\n",
      "opt took 0.01min, 1561iters\n",
      "Epoch: [379][0/14.988]Time: 0.651 (0.651) Data: 0.636 (0.636) Loss: 0.4189 (0.4189)\n",
      "Epoch: [379][10/14.988]Time: 0.011 (0.073) Data: 0.000 (0.058) Loss: 0.4346 (0.4289)\n",
      "10-NN,s=0.1: TOP1:  69.2\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 380\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009759704422269588 step  1161\n",
      "cost:  0.3208666700731554\n",
      "opt took 0.00min, 1161iters\n",
      "Epoch: [380][0/14.988]Time: 0.247 (0.247) Data: 0.223 (0.223) Loss: 0.4222 (0.4222)\n",
      "Epoch: [380][10/14.988]Time: 0.011 (0.035) Data: 0.001 (0.021) Loss: 0.4308 (0.4343)\n",
      "10-NN,s=0.1: TOP1:  70.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 381\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009848306375506044 step  1031\n",
      "cost:  0.3152290080292088\n",
      "opt took 0.00min, 1031iters\n",
      "Epoch: [381][0/14.988]Time: 0.212 (0.212) Data: 0.197 (0.197) Loss: 0.4427 (0.4427)\n",
      "Epoch: [381][10/14.988]Time: 0.010 (0.033) Data: 0.000 (0.018) Loss: 0.4378 (0.4315)\n",
      "10-NN,s=0.1: TOP1:  69.13333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 382\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009904477566715864 step  1211\n",
      "cost:  0.3397551720151449\n",
      "opt took 0.00min, 1211iters\n",
      "Epoch: [382][0/14.988]Time: 0.217 (0.217) Data: 0.202 (0.202) Loss: 0.4242 (0.4242)\n",
      "Epoch: [382][10/14.988]Time: 0.011 (0.032) Data: 0.000 (0.019) Loss: 0.4283 (0.4418)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-NN,s=0.1: TOP1:  69.53333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 383\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009981997332011772 step  1221\n",
      "cost:  0.3820953604380831\n",
      "opt took 0.00min, 1221iters\n",
      "Epoch: [383][0/14.988]Time: 0.270 (0.270) Data: 0.248 (0.248) Loss: 0.4505 (0.4505)\n",
      "Epoch: [383][10/14.988]Time: 0.010 (0.037) Data: 0.000 (0.023) Loss: 0.4458 (0.4352)\n",
      "10-NN,s=0.1: TOP1:  69.4\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 384\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009464249658518553 step  1231\n",
      "cost:  0.3347801202896538\n",
      "opt took 0.00min, 1231iters\n",
      "Epoch: [384][0/14.988]Time: 0.285 (0.285) Data: 0.269 (0.269) Loss: 0.4329 (0.4329)\n",
      "Epoch: [384][10/14.988]Time: 0.011 (0.038) Data: 0.001 (0.025) Loss: 0.4268 (0.4328)\n",
      "10-NN,s=0.1: TOP1:  69.6\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 385\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.00974768091776601 step  1121\n",
      "cost:  0.3184830261821496\n",
      "opt took 0.00min, 1121iters\n",
      "Epoch: [385][0/14.988]Time: 0.242 (0.242) Data: 0.227 (0.227) Loss: 0.4243 (0.4243)\n",
      "Epoch: [385][10/14.988]Time: 0.011 (0.034) Data: 0.000 (0.021) Loss: 0.4459 (0.4323)\n",
      "10-NN,s=0.1: TOP1:  68.6\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 386\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009454343654024089 step  911\n",
      "cost:  0.36013524799286445\n",
      "opt took 0.00min,  911iters\n",
      "Epoch: [386][0/14.988]Time: 0.230 (0.230) Data: 0.215 (0.215) Loss: 0.4081 (0.4081)\n",
      "Epoch: [386][10/14.988]Time: 0.010 (0.033) Data: 0.001 (0.020) Loss: 0.4314 (0.4278)\n",
      "10-NN,s=0.1: TOP1:  68.63333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 387\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009948276929278133 step  1541\n",
      "cost:  0.33118224113373146\n",
      "opt took 0.01min, 1541iters\n",
      "Epoch: [387][0/14.988]Time: 0.428 (0.428) Data: 0.389 (0.389) Loss: 0.4600 (0.4600)\n",
      "Epoch: [387][10/14.988]Time: 0.011 (0.052) Data: 0.001 (0.036) Loss: 0.4144 (0.4318)\n",
      "10-NN,s=0.1: TOP1:  69.46666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 388\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009987904347970478 step  781\n",
      "cost:  0.3174391166441774\n",
      "opt took 0.00min,  781iters\n",
      "Epoch: [388][0/14.988]Time: 0.157 (0.157) Data: 0.145 (0.145) Loss: 0.4205 (0.4205)\n",
      "Epoch: [388][10/14.988]Time: 0.010 (0.028) Data: 0.000 (0.014) Loss: 0.4372 (0.4267)\n",
      "10-NN,s=0.1: TOP1:  69.16666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 389\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009883640448476672 step  621\n",
      "cost:  0.3661794781845944\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [389][0/14.988]Time: 0.180 (0.180) Data: 0.163 (0.163) Loss: 0.4191 (0.4191)\n",
      "Epoch: [389][10/14.988]Time: 0.010 (0.029) Data: 0.000 (0.015) Loss: 0.4333 (0.4255)\n",
      "10-NN,s=0.1: TOP1:  69.93333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 390\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.00968097458829964 step  901\n",
      "cost:  0.32509250998083405\n",
      "opt took 0.00min,  901iters\n",
      "Epoch: [390][0/14.988]Time: 0.192 (0.192) Data: 0.178 (0.178) Loss: 0.4289 (0.4289)\n",
      "Epoch: [390][10/14.988]Time: 0.010 (0.030) Data: 0.001 (0.017) Loss: 0.4250 (0.4277)\n",
      "10-NN,s=0.1: TOP1:  69.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 391\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009721587129517761 step  1181\n",
      "cost:  0.3087882835865863\n",
      "opt took 0.00min, 1181iters\n",
      "Epoch: [391][0/14.988]Time: 0.279 (0.279) Data: 0.265 (0.265) Loss: 0.4254 (0.4254)\n",
      "Epoch: [391][10/14.988]Time: 0.011 (0.037) Data: 0.001 (0.025) Loss: 0.4060 (0.4212)\n",
      "10-NN,s=0.1: TOP1:  68.76666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 392\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009664260500692912 step  1621\n",
      "cost:  0.3469427816680117\n",
      "opt took 0.00min, 1621iters\n",
      "Epoch: [392][0/14.988]Time: 0.322 (0.322) Data: 0.307 (0.307) Loss: 0.4104 (0.4104)\n",
      "Epoch: [392][10/14.988]Time: 0.010 (0.042) Data: 0.001 (0.029) Loss: 0.4275 (0.4303)\n",
      "10-NN,s=0.1: TOP1:  69.1\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 393\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009148069425020333 step  841\n",
      "cost:  0.38122257320736863\n",
      "opt took 0.00min,  841iters\n",
      "Epoch: [393][0/14.988]Time: 0.188 (0.188) Data: 0.173 (0.173) Loss: 0.4380 (0.4380)\n",
      "Epoch: [393][10/14.988]Time: 0.011 (0.029) Data: 0.000 (0.016) Loss: 0.4062 (0.4303)\n",
      "10-NN,s=0.1: TOP1:  69.23333333333333\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 394\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009923789308927566 step  1421\n",
      "cost:  0.3290616565278906\n",
      "opt took 0.01min, 1421iters\n",
      "Epoch: [394][0/14.988]Time: 0.441 (0.441) Data: 0.424 (0.424) Loss: 0.4270 (0.4270)\n",
      "Epoch: [394][10/14.988]Time: 0.011 (0.055) Data: 0.000 (0.039) Loss: 0.4175 (0.4274)\n",
      "10-NN,s=0.1: TOP1:  68.76666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 395\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009638498341420942 step  1601\n",
      "cost:  0.3188613839069284\n",
      "opt took 0.01min, 1601iters\n",
      "Epoch: [395][0/14.988]Time: 0.372 (0.372) Data: 0.356 (0.356) Loss: 0.4248 (0.4248)\n",
      "Epoch: [395][10/14.988]Time: 0.013 (0.050) Data: 0.001 (0.033) Loss: 0.4386 (0.4253)\n",
      "10-NN,s=0.1: TOP1:  69.6\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 396\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009830624419105427 step  751\n",
      "cost:  0.36048105225756477\n",
      "opt took 0.00min,  751iters\n",
      "Epoch: [396][0/14.988]Time: 0.276 (0.276) Data: 0.261 (0.261) Loss: 0.4122 (0.4122)\n",
      "Epoch: [396][10/14.988]Time: 0.012 (0.038) Data: 0.001 (0.024) Loss: 0.4162 (0.4198)\n",
      "10-NN,s=0.1: TOP1:  68.26666666666667\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 397\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009790136456182763 step  1151\n",
      "cost:  0.3268623099959316\n",
      "opt took 0.01min, 1151iters\n",
      "Epoch: [397][0/14.988]Time: 0.641 (0.641) Data: 0.623 (0.623) Loss: 0.4168 (0.4168)\n",
      "Epoch: [397][10/14.988]Time: 0.016 (0.075) Data: 0.001 (0.057) Loss: 0.4249 (0.4203)\n",
      "10-NN,s=0.1: TOP1:  69.43333333333334\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 398\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009629061956723195 step  851\n",
      "cost:  0.31238646465507647\n",
      "opt took 0.00min,  851iters\n",
      "Epoch: [398][0/14.988]Time: 0.316 (0.316) Data: 0.302 (0.302) Loss: 0.4489 (0.4489)\n",
      "Epoch: [398][10/14.988]Time: 0.011 (0.041) Data: 0.000 (0.028) Loss: 0.4286 (0.4216)\n",
      "10-NN,s=0.1: TOP1:  68.56666666666666\n",
      "best accuracy: 72.63\n",
      "\n",
      "Epoch: 399\n",
      "LSTM\n",
      "0.009000000000000003\n",
      "error:  0.009666248192222837 step  1301\n",
      "cost:  0.352719970843991\n",
      "opt took 0.00min, 1301iters\n",
      "Epoch: [399][0/14.988]Time: 0.240 (0.240) Data: 0.224 (0.224) Loss: 0.4602 (0.4602)\n",
      "Epoch: [399][10/14.988]Time: 0.010 (0.034) Data: 0.000 (0.021) Loss: 0.4407 (0.4463)\n",
      "10-NN,s=0.1: TOP1:  69.36666666666666\n",
      "best accuracy: 72.63\n",
      "doing PCA with 10 components ..done\n",
      "10-NN,s=0.1: TOP1:  73.23333333333333\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    \n",
    "    acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim)\n",
    "    feature_return_switch(model, False)\n",
    "#     writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = my_kNN(model, K=[50, 10], sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "#         for num_nn in [50, 10]:\n",
    "#             for sig in [0.1, 0.5]:\n",
    "#                 writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "#                 i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "end = time.time()\n",
    "\n",
    "checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
