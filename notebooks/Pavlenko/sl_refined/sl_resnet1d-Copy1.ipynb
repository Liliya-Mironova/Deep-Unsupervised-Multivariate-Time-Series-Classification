{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"/root/data/Multivariate_arff\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 100   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './resnet1d_exp' # experiments results dir\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 100\n",
    "lr=0.03     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 10\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters (AlexNet in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10       # number of heads\n",
    "ncl=6       # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# # (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "# CFG = {\n",
    "#     'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "#     'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    data = arff.loadarff(filepath)\n",
    "    data = pd.DataFrame(data[0])\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    return X.values, y.values\n",
    "\n",
    "def load_group(prefix, filenames): \n",
    "    loaded = []\n",
    "    for name in filenames: \n",
    "        X, y = load_file(prefix + \"/\" + name) \n",
    "        loaded.append(X)\n",
    "    # stack group so that features are the 3rd dimension \n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded, y\n",
    "\n",
    "def load_dataset_group(folder_path, ds_path, dims_num, is_train=True, label_enc=False): \n",
    "    filenames = []\n",
    "    if is_train:\n",
    "        postfix = \"_TRAIN.arff\"\n",
    "    else:\n",
    "        postfix = \"_TEST.arff\"\n",
    "    for dim_num in range(1, dims_num + 1):\n",
    "        filenames.append(ds_path + str(dim_num) + postfix)\n",
    "\n",
    "    X, y = load_group(folder_path, filenames)\n",
    "    X = torch.from_numpy(np.array(X, dtype=np.float64))\n",
    "    if label_enc:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        y = torch.from_numpy(np.array(y, dtype=np.int32))\n",
    "    else:\n",
    "        y = torch.from_numpy(np.array(y, dtype=np.int32)) - 1\n",
    "    X = X.transpose(1, 2)\n",
    "    return X, y\n",
    "\n",
    "def load_dataset(folder_path, ds_path, dims_num, label_enc=False): \n",
    "    X_train, y_train = load_dataset_group(folder_path, ds_path, dims_num, \n",
    "                                          is_train=True, label_enc=label_enc) \n",
    "    X_test, y_test = load_dataset_group(folder_path, ds_path, dims_num, \n",
    "                                        is_train=False, label_enc=label_enc)\n",
    "    X_train = F.normalize(X_train, dim=1)\n",
    "    X_test = F.normalize(X_test, dim=1)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: torch.Size([2459, 6, 36]) \n",
      "y_train.shape: torch.Size([2459])\n",
      "X_test.shape: torch.Size([2466, 6, 36]) \n",
      "y_test.shape: torch.Size([2466])\n"
     ]
    }
   ],
   "source": [
    "#ds_path = \"ERing/ERingDimension\"\n",
    "#ds_path = \"SpokenArabicDigits/SpokenArabicDigitsDimension\"\n",
    "# dims_num = 13\n",
    "# num_classes = 10\n",
    "# magic_dim = 4500\n",
    "\n",
    "ds_path = \"LSST/LSSTDimension\"\n",
    "dims_num = 6\n",
    "num_classes = 14\n",
    "magic_dim = 2304\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataset(datadir, ds_path, dims_num)\n",
    "# X_train[0], y_train\n",
    "print(\"X_train.shape:\", X_train.shape, \"\\ny_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape, \"\\ny_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2459"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2459, 6, 36])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "__all__ = ['resnetv1','resnetv1_18']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, in_channel=3, width=1, num_classes=[1000]):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        self.base = int(16 * width)\n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(in_channel, 16, kernel_size=3, padding=1, bias=False), # [100, 16, 36]\n",
    "                            nn.BatchNorm1d(16),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            self._make_layer(block, self.base, layers[0]),                   # [100, 16, 36]\n",
    "                            self._make_layer(block, self.base * 2, layers[1]),               # [100, 32, 36]\n",
    "                            self._make_layer(block, self.base * 4, layers[2]),               # [100, 64, 36]\n",
    "                            self._make_layer(block, self.base * 8, layers[3]),               # [100, 128, 36]\n",
    "                            nn.AvgPool1d(2),                                                 # [100, 128, 18]\n",
    "        ])\n",
    "    \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnetv1_18(num_classes=[1000]):\n",
    "    \"\"\"Encoder for instance discrimination and MoCo\"\"\"\n",
    "    return resnet18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "        \n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((N, ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((N, magic_dim)) # knn_dim\n",
    "    \n",
    "    for batch_idx, (data, _, _selected) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data.float())\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy()\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N + batch_idx\n",
    "        if len(optimize_times) > 0 and niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h],\n",
    "                                                     selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if True:\n",
    "#         if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "#             writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*len(trainloader.dataset))\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = resnet18(num_classes=numc, in_channel=dims_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [110.0, 101.0, 100.75, 100.49, 100.24, 99.99, 99.73, 99.48, 99.23, 98.98, 98.72, 98.47, 98.22, 97.96, 97.71, 97.46, 97.2, 96.95, 96.7, 96.44, 96.19, 95.94, 95.68, 95.43, 95.18, 94.92, 94.67, 94.42, 94.17, 93.91, 93.66, 93.41, 93.15, 92.9, 92.65, 92.39, 92.14, 91.89, 91.63, 91.38, 91.13, 90.87, 90.62, 90.37, 90.12, 89.86, 89.61, 89.36, 89.1, 88.85, 88.6, 88.34, 88.09, 87.84, 87.58, 87.33, 87.08, 86.82, 86.57, 86.32, 86.07, 85.81, 85.56, 85.31, 85.05, 84.8, 84.55, 84.29, 84.04, 83.79, 83.53, 83.28, 83.03, 82.77, 82.52, 82.27, 82.02, 81.76, 81.51, 81.26, 81.0, 80.75, 80.5, 80.24, 79.99, 79.74, 79.48, 79.23, 78.98, 78.72, 78.47, 78.22, 77.96, 77.71, 77.46, 77.21, 76.95, 76.7, 76.45, 76.19, 75.94, 75.69, 75.43, 75.18, 74.93, 74.67, 74.42, 74.17, 73.91, 73.66, 73.41, 73.16, 72.9, 72.65, 72.4, 72.14, 71.89, 71.64, 71.38, 71.13, 70.88, 70.62, 70.37, 70.12, 69.86, 69.61, 69.36, 69.11, 68.85, 68.6, 68.35, 68.09, 67.84, 67.59, 67.33, 67.08, 66.83, 66.57, 66.32, 66.07, 65.81, 65.56, 65.31, 65.06, 64.8, 64.55, 64.3, 64.04, 63.79, 63.54, 63.28, 63.03, 62.78, 62.52, 62.27, 62.02, 61.76, 61.51, 61.26, 61.01, 60.75, 60.5, 60.25, 59.99, 59.74, 59.49, 59.23, 58.98, 58.73, 58.47, 58.22, 57.97, 57.71, 57.46, 57.21, 56.95, 56.7, 56.45, 56.2, 55.94, 55.69, 55.44, 55.18, 54.93, 54.68, 54.42, 54.17, 53.92, 53.66, 53.41, 53.16, 52.9, 52.65, 52.4, 52.15, 51.89, 51.64, 51.39, 51.13, 50.88, 50.63, 50.37, 50.12, 49.87, 49.61, 49.36, 49.11, 48.85, 48.6, 48.35, 48.1, 47.84, 47.59, 47.34, 47.08, 46.83, 46.58, 46.32, 46.07, 45.82, 45.56, 45.31, 45.06, 44.8, 44.55, 44.3, 44.05, 43.79, 43.54, 43.29, 43.03, 42.78, 42.53, 42.27, 42.02, 41.77, 41.51, 41.26, 41.01, 40.75, 40.5, 40.25, 40.0, 39.74, 39.49, 39.24, 38.98, 38.73, 38.48, 38.22, 37.97, 37.72, 37.46, 37.21, 36.96, 36.7, 36.45, 36.2, 35.94, 35.69, 35.44, 35.19, 34.93, 34.68, 34.43, 34.17, 33.92, 33.67, 33.41, 33.16, 32.91, 32.65, 32.4, 32.15, 31.89, 31.64, 31.39, 31.14, 30.88, 30.63, 30.38, 30.12, 29.87, 29.62, 29.36, 29.11, 28.86, 28.6, 28.35, 28.1, 27.84, 27.59, 27.34, 27.09, 26.83, 26.58, 26.33, 26.07, 25.82, 25.57, 25.31, 25.06, 24.81, 24.55, 24.3, 24.05, 23.79, 23.54, 23.29, 23.04, 22.78, 22.53, 22.28, 22.02, 21.77, 21.52, 21.26, 21.01, 20.76, 20.5, 20.25, 20.0, 19.74, 19.49, 19.24, 18.98, 18.73, 18.48, 18.23, 17.97, 17.72, 17.47, 17.21, 16.96, 16.71, 16.45, 16.2, 15.95, 15.69, 15.44, 15.19, 14.93, 14.68, 14.43, 14.18, 13.92, 13.67, 13.42, 13.16, 12.91, 12.66, 12.4, 12.15, 11.9, 11.64, 11.39, 11.14, 10.88, 10.63, 10.38, 10.13, 9.87, 9.62, 9.37, 9.11, 8.86, 8.61, 8.35, 8.1, 7.85, 7.59, 7.34, 7.09, 6.83, 6.58, 6.33, 6.08, 5.82, 5.57, 5.32, 5.06, 4.81, 4.56, 4.3, 4.05, 3.8, 3.54, 3.29, 3.04, 2.78, 2.53, 2.28, 2.03, 1.77, 1.52, 1.27, 1.01, 0.76, 0.51, 0.25, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ResNet1D\"\n",
    "writer = SummaryWriter(f'./runs/ERing/{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(net, K, sigma=0.1, dim=128, use_pca=False):\n",
    "    net.eval()\n",
    "    # this part is ugly but made to be backwards-compatible. there was a change in cifar dataset's structure.\n",
    "    trainLabels = y_train\n",
    "    LEN = N\n",
    "    C = trainLabels.max() + 1\n",
    "\n",
    "    trainFeatures = torch.zeros((magic_dim, LEN))  # , device='cuda:0') # dim\n",
    "    normalize = Normalize()\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=False)):\n",
    "        batchSize = batch_size\n",
    "        inputs = inputs.cuda()\n",
    "        features = net(inputs.float())\n",
    "        if not use_pca:\n",
    "            features = normalize(features)\n",
    "        tmp = trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize]\n",
    "        trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu()\n",
    "        \n",
    "    if use_pca:\n",
    "        comps = 128\n",
    "        print('doing PCA with %s components'%comps, end=' ')\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=comps, whiten=False)\n",
    "        trainFeatures = pca.fit_transform(trainFeatures.numpy().T)\n",
    "        trainFeatures = torch.Tensor(trainFeatures)\n",
    "        trainFeatures = normalize(trainFeatures).t()\n",
    "        print('..done')\n",
    "    def eval_k_s(K_,sigma_):\n",
    "        total = 0\n",
    "        top1 = 0.\n",
    "        top5 = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            retrieval_one_hot = torch.zeros(K_, C)# .cuda()\n",
    "            for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=True)):\n",
    "                targets = targets # .cuda(async=True) # or without async for py3.7\n",
    "                inputs = inputs.cuda()\n",
    "                batchSize = batch_size\n",
    "                features = net(inputs)\n",
    "                if use_pca:\n",
    "                    features = pca.transform(features.cpu().numpy())\n",
    "                    features = torch.Tensor(features).cuda()\n",
    "                features = normalize(features).cpu()\n",
    "\n",
    "                dist = torch.mm(features, trainFeatures)\n",
    "\n",
    "                yd, yi = dist.topk(K_, dim=1, largest=True, sorted=True)\n",
    "                candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "                retrieval = torch.gather(candidates, 1, yi).long()\n",
    "\n",
    "                retrieval_one_hot.resize_(batchSize * K_, C).zero_()\n",
    "                retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1.)\n",
    "                \n",
    "                yd_transform = yd.clone().div_(sigma_).exp_()\n",
    "                probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C),\n",
    "                                            yd_transform.view(batchSize, -1, 1)),\n",
    "                                  1)\n",
    "                _, predictions = probs.sort(1, True)\n",
    "\n",
    "                # Find which predictions match the target\n",
    "                correct = predictions.eq(targets.data.view(-1, 1))\n",
    "\n",
    "                top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "                top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        print(f\"{K_}-NN,s={sigma_}: TOP1: \", top1 * 100. / total)\n",
    "        return top1 / total\n",
    "\n",
    "    if isinstance(K, list):\n",
    "        res = []\n",
    "        for K_ in K:\n",
    "            for sigma_ in sigma:\n",
    "                res.append(eval_k_s(K_, sigma_))\n",
    "        return res\n",
    "    else:\n",
    "        res = eval_k_s(K, sigma)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "ResNet1D\n",
      "error:  0.009233038617996403 step  31\n",
      "cost:  1.3887578024021057\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [0][0/2459]Time: 0.332 (0.332) Data: 0.259 (0.259) Loss: 1.9297 (1.9297)\n",
      "Epoch: [0][1/2459]Time: 0.045 (0.189) Data: 0.005 (0.132) Loss: 1.8533 (1.8915)\n",
      "Epoch: [0][2/2459]Time: 0.027 (0.135) Data: 0.001 (0.088) Loss: 1.9103 (1.8977)\n",
      "Epoch: [0][3/2459]Time: 0.026 (0.108) Data: 0.001 (0.066) Loss: 1.9746 (1.9169)\n",
      "Epoch: [0][4/2459]Time: 0.030 (0.092) Data: 0.001 (0.053) Loss: 1.8865 (1.9109)\n",
      "Epoch: [0][5/2459]Time: 0.026 (0.081) Data: 0.001 (0.045) Loss: 1.8449 (1.8999)\n",
      "Epoch: [0][6/2459]Time: 0.026 (0.073) Data: 0.001 (0.038) Loss: 1.9727 (1.9103)\n",
      "error:  0.002349356032679273 step  61\n",
      "cost:  1.2761258430396636\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [0][7/2459]Time: 0.264 (0.097) Data: 0.232 (0.063) Loss: 1.8981 (1.9088)\n",
      "Epoch: [0][8/2459]Time: 0.033 (0.090) Data: 0.002 (0.056) Loss: 1.9163 (1.9096)\n",
      "Epoch: [0][9/2459]Time: 0.036 (0.085) Data: 0.001 (0.050) Loss: 1.8840 (1.9070)\n",
      "Epoch: [0][10/2459]Time: 0.031 (0.080) Data: 0.001 (0.046) Loss: 1.9261 (1.9088)\n",
      "Epoch: [0][11/2459]Time: 0.025 (0.075) Data: 0.001 (0.042) Loss: 1.9058 (1.9085)\n",
      "Epoch: [0][12/2459]Time: 0.025 (0.071) Data: 0.001 (0.039) Loss: 1.9596 (1.9125)\n",
      "error:  0.003581258903361806 step  101\n",
      "cost:  1.0678210954646103\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [0][13/2459]Time: 0.288 (0.087) Data: 0.244 (0.054) Loss: 1.9205 (1.9130)\n",
      "Epoch: [0][14/2459]Time: 0.036 (0.083) Data: 0.002 (0.050) Loss: 1.8873 (1.9113)\n",
      "Epoch: [0][15/2459]Time: 0.042 (0.081) Data: 0.001 (0.047) Loss: 1.8998 (1.9106)\n",
      "Epoch: [0][16/2459]Time: 0.024 (0.077) Data: 0.001 (0.044) Loss: 1.9424 (1.9125)\n",
      "Epoch: [0][17/2459]Time: 0.025 (0.074) Data: 0.001 (0.042) Loss: 1.9195 (1.9129)\n",
      "Epoch: [0][18/2459]Time: 0.025 (0.072) Data: 0.001 (0.040) Loss: 1.8783 (1.9110)\n",
      "error:  0.005808772199771273 step  141\n",
      "cost:  0.8205727935752818\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [0][19/2459]Time: 0.328 (0.085) Data: 0.295 (0.053) Loss: 1.8732 (1.9091)\n",
      "Epoch: [0][20/2459]Time: 0.033 (0.082) Data: 0.001 (0.050) Loss: 1.8692 (1.9072)\n",
      "Epoch: [0][21/2459]Time: 0.035 (0.080) Data: 0.001 (0.048) Loss: 1.8674 (1.9054)\n",
      "Epoch: [0][22/2459]Time: 0.030 (0.078) Data: 0.002 (0.046) Loss: 1.8894 (1.9047)\n",
      "Epoch: [0][23/2459]Time: 0.028 (0.076) Data: 0.001 (0.044) Loss: 1.8988 (1.9045)\n",
      "10-NN,s=0.1: TOP1:  34.375\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 128 components ..done\n",
      "50-NN,s=0.1: TOP1:  35.208333333333336\n",
      "50-NN,s=0.5: TOP1:  34.25\n",
      "10-NN,s=0.1: TOP1:  32.125\n",
      "10-NN,s=0.5: TOP1:  32.583333333333336\n",
      "best accuracy: 34.38\n",
      "\n",
      "Epoch: 1\n",
      "ResNet1D\n",
      "error:  0.007681390203068794 step  31\n",
      "cost:  1.412157658628795\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [1][0/2459]Time: 0.248 (0.248) Data: 0.182 (0.182) Loss: 1.7276 (1.7276)\n",
      "error:  0.0004414138282144986 step  41\n",
      "cost:  1.4969555844128348\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][1/2459]Time: 0.218 (0.233) Data: 0.189 (0.185) Loss: 1.7488 (1.7382)\n",
      "error:  0.00034319071492772135 step  41\n",
      "cost:  1.421775790431264\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][2/2459]Time: 0.291 (0.252) Data: 0.202 (0.191) Loss: 1.6675 (1.7147)\n",
      "error:  0.004648542414496304 step  31\n",
      "cost:  1.3377070181177797\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [1][3/2459]Time: 0.223 (0.245) Data: 0.181 (0.188) Loss: 1.7062 (1.7126)\n",
      "error:  0.005428222536589478 step  41\n",
      "cost:  1.3265186918729004\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][4/2459]Time: 0.279 (0.252) Data: 0.243 (0.199) Loss: 1.6902 (1.7081)\n",
      "error:  0.0008101313597550508 step  51\n",
      "cost:  1.2720779071991115\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [1][5/2459]Time: 0.247 (0.251) Data: 0.206 (0.200) Loss: 1.6346 (1.6958)\n",
      "error:  0.0023119140483753853 step  51\n",
      "cost:  1.2644453482300853\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [1][6/2459]Time: 0.260 (0.252) Data: 0.225 (0.204) Loss: 1.7323 (1.7010)\n",
      "error:  0.0020080944220113306 step  61\n",
      "cost:  1.2197395763582406\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [1][7/2459]Time: 0.266 (0.254) Data: 0.222 (0.206) Loss: 1.7527 (1.7075)\n",
      "error:  0.003198767619244536 step  71\n",
      "cost:  1.1147359345321148\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [1][8/2459]Time: 0.293 (0.258) Data: 0.235 (0.209) Loss: 1.6525 (1.7014)\n",
      "error:  0.005388116962101064 step  101\n",
      "cost:  1.0836908303461734\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [1][9/2459]Time: 0.318 (0.264) Data: 0.260 (0.215) Loss: 1.6487 (1.6961)\n",
      "error:  0.00525153001455092 step  111\n",
      "cost:  1.0823143640407538\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [1][10/2459]Time: 0.346 (0.272) Data: 0.299 (0.222) Loss: 1.6445 (1.6914)\n",
      "error:  0.0052303453503231845 step  111\n",
      "cost:  0.9577144022810951\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [1][11/2459]Time: 0.325 (0.276) Data: 0.297 (0.228) Loss: 1.6869 (1.6910)\n",
      "error:  0.007320404759135202 step  121\n",
      "cost:  0.8873792121804809\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [1][12/2459]Time: 0.279 (0.276) Data: 0.224 (0.228) Loss: 1.6375 (1.6869)\n",
      "error:  0.009116544151685502 step  101\n",
      "cost:  0.8731053346313947\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [1][13/2459]Time: 0.289 (0.277) Data: 0.242 (0.229) Loss: 1.6097 (1.6814)\n",
      "error:  0.007659842275018591 step  121\n",
      "cost:  0.797714708129488\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [1][14/2459]Time: 0.250 (0.275) Data: 0.217 (0.228) Loss: 1.7019 (1.6828)\n",
      "error:  0.0055080060196170955 step  131\n",
      "cost:  0.7436526741942495\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [1][15/2459]Time: 0.277 (0.275) Data: 0.241 (0.229) Loss: 1.6656 (1.6817)\n",
      "error:  0.007427239636017546 step  151\n",
      "cost:  0.7324959092379183\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [1][16/2459]Time: 0.246 (0.274) Data: 0.213 (0.228) Loss: 1.6112 (1.6776)\n",
      "error:  0.005474117010770607 step  161\n",
      "cost:  0.7027222235725117\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [1][17/2459]Time: 0.421 (0.282) Data: 0.379 (0.236) Loss: 1.5890 (1.6726)\n",
      "error:  0.006357896937584928 step  201\n",
      "cost:  0.6488014794491774\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [1][18/2459]Time: 0.322 (0.284) Data: 0.292 (0.239) Loss: 1.6193 (1.6698)\n",
      "error:  0.008222084155095843 step  191\n",
      "cost:  0.6194016146768583\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [1][19/2459]Time: 0.300 (0.285) Data: 0.262 (0.241) Loss: 1.6680 (1.6697)\n",
      "error:  0.006721375802708818 step  191\n",
      "cost:  0.611058956914086\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [1][20/2459]Time: 0.282 (0.285) Data: 0.253 (0.241) Loss: 1.6119 (1.6670)\n",
      "error:  0.009547729810514771 step  171\n",
      "cost:  0.6008409212311204\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [1][21/2459]Time: 0.332 (0.287) Data: 0.271 (0.242) Loss: 1.5948 (1.6637)\n",
      "error:  0.007055552523181907 step  181\n",
      "cost:  0.5744191284757028\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [1][22/2459]Time: 0.300 (0.287) Data: 0.261 (0.243) Loss: 1.6765 (1.6643)\n",
      "error:  0.007233040979034877 step  141\n",
      "cost:  0.5499287801540125\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [1][23/2459]Time: 0.293 (0.288) Data: 0.258 (0.244) Loss: 1.6512 (1.6637)\n",
      "10-NN,s=0.1: TOP1:  33.041666666666664\n",
      "best accuracy: 34.38\n",
      "\n",
      "Epoch: 2\n",
      "ResNet1D\n",
      "error:  0.0011627038523909228 step  41\n",
      "cost:  1.3425635300232746\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [2][0/2459]Time: 0.245 (0.245) Data: 0.209 (0.209) Loss: 1.4227 (1.4227)\n",
      "error:  0.0036408809298498213 step  41\n",
      "cost:  1.4017241215882126\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [2][1/2459]Time: 0.273 (0.259) Data: 0.245 (0.227) Loss: 1.4377 (1.4302)\n",
      "error:  0.004198765561726714 step  51\n",
      "cost:  1.4419546689715204\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [2][2/2459]Time: 0.248 (0.255) Data: 0.217 (0.223) Loss: 1.4179 (1.4261)\n",
      "error:  0.0024230470059475318 step  51\n",
      "cost:  1.3031888437536079\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [2][3/2459]Time: 0.399 (0.291) Data: 0.359 (0.257) Loss: 1.4321 (1.4276)\n",
      "error:  0.00869916023013717 step  51\n",
      "cost:  1.2278965823439285\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [2][4/2459]Time: 0.402 (0.313) Data: 0.358 (0.277) Loss: 1.4401 (1.4301)\n",
      "error:  0.0066399221816818566 step  61\n",
      "cost:  1.2276335658146713\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [2][5/2459]Time: 0.280 (0.308) Data: 0.217 (0.267) Loss: 1.4081 (1.4264)\n",
      "error:  0.0030492741759911013 step  71\n",
      "cost:  1.2078954687473107\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [2][6/2459]Time: 0.349 (0.314) Data: 0.292 (0.271) Loss: 1.4255 (1.4263)\n",
      "error:  0.003810011889519571 step  91\n",
      "cost:  1.1874626137607394\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [2][7/2459]Time: 0.213 (0.301) Data: 0.184 (0.260) Loss: 1.4461 (1.4288)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.005936685988586099 step  101\n",
      "cost:  1.0970638845147211\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [2][8/2459]Time: 0.258 (0.296) Data: 0.226 (0.256) Loss: 1.4088 (1.4266)\n",
      "error:  0.004356519482974108 step  71\n",
      "cost:  0.9586815654145\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [2][9/2459]Time: 0.258 (0.292) Data: 0.219 (0.252) Loss: 1.4303 (1.4269)\n",
      "error:  0.008313518381654283 step  111\n",
      "cost:  0.9684723529938857\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [2][10/2459]Time: 0.277 (0.291) Data: 0.249 (0.252) Loss: 1.4249 (1.4267)\n",
      "error:  0.004966955497098358 step  131\n",
      "cost:  1.0221863550058954\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [2][11/2459]Time: 0.254 (0.288) Data: 0.226 (0.250) Loss: 1.3596 (1.4212)\n",
      "error:  0.006326925245197623 step  151\n",
      "cost:  0.9075002856469107\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [2][12/2459]Time: 0.344 (0.292) Data: 0.300 (0.254) Loss: 1.4489 (1.4233)\n",
      "error:  0.009802542560975569 step  151\n",
      "cost:  0.8150262143605131\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [2][13/2459]Time: 0.421 (0.301) Data: 0.370 (0.262) Loss: 1.3972 (1.4214)\n",
      "error:  0.005425565275148481 step  151\n",
      "cost:  0.7958892594122743\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [2][14/2459]Time: 0.443 (0.311) Data: 0.348 (0.268) Loss: 1.3701 (1.4180)\n",
      "error:  0.009050642393146568 step  131\n",
      "cost:  0.8178279460727476\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [2][15/2459]Time: 0.282 (0.309) Data: 0.235 (0.266) Loss: 1.3758 (1.4154)\n",
      "error:  0.00866063653620397 step  171\n",
      "cost:  0.785768076436248\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [2][16/2459]Time: 0.402 (0.314) Data: 0.375 (0.272) Loss: 1.4208 (1.4157)\n",
      "error:  0.006595828657071645 step  201\n",
      "cost:  0.7106145576488274\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [2][17/2459]Time: 0.461 (0.323) Data: 0.356 (0.277) Loss: 1.3731 (1.4133)\n",
      "error:  0.008651375810275197 step  221\n",
      "cost:  0.6416954787226843\n",
      "opt took 0.00min,  221iters\n",
      "Epoch: [2][18/2459]Time: 0.327 (0.323) Data: 0.270 (0.276) Loss: 1.3616 (1.4106)\n",
      "error:  0.009030919115448843 step  231\n",
      "cost:  0.6543212460156279\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [2][19/2459]Time: 0.258 (0.320) Data: 0.229 (0.274) Loss: 1.3775 (1.4089)\n",
      "error:  0.007595197283554067 step  161\n",
      "cost:  0.6305011149541583\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [2][20/2459]Time: 0.357 (0.321) Data: 0.309 (0.276) Loss: 1.3762 (1.4074)\n",
      "error:  0.007417485052486628 step  201\n",
      "cost:  0.5652016411387437\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [2][21/2459]Time: 0.285 (0.320) Data: 0.233 (0.274) Loss: 1.4107 (1.4075)\n",
      "error:  0.007362978295661904 step  231\n",
      "cost:  0.5082209356357418\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [2][22/2459]Time: 0.322 (0.320) Data: 0.280 (0.274) Loss: 1.3860 (1.4066)\n",
      "error:  0.009150373643324494 step  211\n",
      "cost:  0.5045853926671848\n",
      "opt took 0.00min,  211iters\n",
      "Epoch: [2][23/2459]Time: 0.359 (0.322) Data: 0.301 (0.275) Loss: 1.3808 (1.4055)\n",
      "10-NN,s=0.1: TOP1:  34.666666666666664\n",
      "Saving..\n",
      "best accuracy: 34.67\n",
      "\n",
      "Epoch: 3\n",
      "ResNet1D\n",
      "error:  0.0060459084810498 step  41\n",
      "cost:  1.2632664908369051\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [3][0/2459]Time: 0.227 (0.227) Data: 0.196 (0.196) Loss: 1.2342 (1.2342)\n",
      "error:  0.0075906219417353205 step  41\n",
      "cost:  1.2226750047443475\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [3][1/2459]Time: 0.318 (0.273) Data: 0.253 (0.225) Loss: 1.1879 (1.2111)\n",
      "error:  0.004606330856664731 step  41\n",
      "cost:  1.211044812187807\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [3][2/2459]Time: 0.259 (0.268) Data: 0.216 (0.222) Loss: 1.2728 (1.2317)\n",
      "error:  0.005992702593079824 step  51\n",
      "cost:  1.222003060106283\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [3][3/2459]Time: 0.410 (0.303) Data: 0.365 (0.257) Loss: 1.2334 (1.2321)\n",
      "error:  0.009231478371976576 step  51\n",
      "cost:  1.184825420175561\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [3][4/2459]Time: 0.335 (0.310) Data: 0.268 (0.260) Loss: 1.2229 (1.2303)\n",
      "error:  0.002905535668860537 step  61\n",
      "cost:  1.1108896726769648\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [3][5/2459]Time: 0.349 (0.316) Data: 0.313 (0.268) Loss: 1.2224 (1.2290)\n",
      "error:  0.008198045768807005 step  71\n",
      "cost:  1.1036260654565557\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [3][6/2459]Time: 0.279 (0.311) Data: 0.249 (0.266) Loss: 1.2602 (1.2334)\n",
      "error:  0.0030710017829084224 step  91\n",
      "cost:  1.0992143810301171\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [3][7/2459]Time: 0.253 (0.304) Data: 0.208 (0.258) Loss: 1.2736 (1.2384)\n",
      "error:  0.008848204030124318 step  81\n",
      "cost:  1.0455733030115169\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [3][8/2459]Time: 0.384 (0.313) Data: 0.352 (0.269) Loss: 1.1773 (1.2317)\n",
      "error:  0.004037302667556419 step  71\n",
      "cost:  0.9762079764179812\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [3][9/2459]Time: 0.283 (0.310) Data: 0.247 (0.267) Loss: 1.1734 (1.2258)\n",
      "error:  0.008330201471210108 step  101\n",
      "cost:  0.9201451852869252\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [3][10/2459]Time: 0.279 (0.307) Data: 0.247 (0.265) Loss: 1.2281 (1.2260)\n",
      "error:  0.008421813008779866 step  111\n",
      "cost:  0.9112518439219398\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [3][11/2459]Time: 0.249 (0.302) Data: 0.194 (0.259) Loss: 1.1885 (1.2229)\n",
      "error:  0.0051086388110268555 step  131\n",
      "cost:  0.9013251074411398\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [3][12/2459]Time: 0.234 (0.297) Data: 0.211 (0.255) Loss: 1.1781 (1.2195)\n",
      "error:  0.004535223909440944 step  141\n",
      "cost:  0.8507763569714046\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [3][13/2459]Time: 0.226 (0.292) Data: 0.195 (0.251) Loss: 1.1902 (1.2174)\n",
      "error:  0.005658094053894502 step  111\n",
      "cost:  0.7659243782554787\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [3][14/2459]Time: 0.354 (0.296) Data: 0.327 (0.256) Loss: 1.1595 (1.2135)\n",
      "error:  0.00834250923489488 step  101\n",
      "cost:  0.7288824238298041\n",
      "opt took 0.01min,  101iters\n",
      "Epoch: [3][15/2459]Time: 0.527 (0.310) Data: 0.500 (0.271) Loss: 1.1832 (1.2116)\n",
      "error:  0.006342820680099059 step  141\n",
      "cost:  0.6980010590309204\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [3][16/2459]Time: 0.330 (0.312) Data: 0.301 (0.273) Loss: 1.1543 (1.2082)\n",
      "error:  0.009186738935233696 step  131\n",
      "cost:  0.677186511647813\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [3][17/2459]Time: 0.233 (0.307) Data: 0.205 (0.269) Loss: 1.1674 (1.2060)\n",
      "error:  0.009197615821142202 step  161\n",
      "cost:  0.6378199447671291\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [3][18/2459]Time: 0.219 (0.302) Data: 0.191 (0.265) Loss: 1.1470 (1.2029)\n",
      "error:  0.007156586636528295 step  151\n",
      "cost:  0.6229324239414235\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [3][19/2459]Time: 0.237 (0.299) Data: 0.210 (0.262) Loss: 1.1947 (1.2025)\n",
      "error:  0.00841630154361439 step  171\n",
      "cost:  0.598921225098962\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [3][20/2459]Time: 0.228 (0.296) Data: 0.200 (0.259) Loss: 1.1972 (1.2022)\n",
      "error:  0.007470527683573369 step  191\n",
      "cost:  0.5718104653748226\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [3][21/2459]Time: 0.324 (0.297) Data: 0.291 (0.261) Loss: 1.1819 (1.2013)\n",
      "error:  0.008511992671060953 step  181\n",
      "cost:  0.5424915036059048\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [3][22/2459]Time: 0.276 (0.296) Data: 0.210 (0.259) Loss: 1.1658 (1.1997)\n",
      "error:  0.008011961130802825 step  191\n",
      "cost:  0.5478101575378213\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [3][23/2459]Time: 0.349 (0.298) Data: 0.283 (0.260) Loss: 1.1960 (1.1996)\n",
      "10-NN,s=0.1: TOP1:  36.208333333333336\n",
      "Saving..\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 4\n",
      "ResNet1D\n",
      "error:  0.0019991495261867387 step  61\n",
      "cost:  1.1986356820250206\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [4][0/2459]Time: 0.224 (0.224) Data: 0.192 (0.192) Loss: 1.0643 (1.0643)\n",
      "error:  0.00908256172904609 step  51\n",
      "cost:  1.1506826079265602\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [4][1/2459]Time: 0.316 (0.270) Data: 0.238 (0.215) Loss: 1.0494 (1.0569)\n",
      "error:  0.0029635063063089273 step  61\n",
      "cost:  1.1170535302312574\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [4][2/2459]Time: 0.219 (0.253) Data: 0.188 (0.206) Loss: 1.0955 (1.0698)\n",
      "error:  0.005562723070591802 step  61\n",
      "cost:  1.1455190814944003\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [4][3/2459]Time: 0.321 (0.270) Data: 0.249 (0.217) Loss: 1.0224 (1.0579)\n",
      "error:  0.0029382486364646354 step  71\n",
      "cost:  1.2221667707416635\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [4][4/2459]Time: 0.227 (0.261) Data: 0.198 (0.213) Loss: 1.0959 (1.0655)\n",
      "error:  0.005391137740208718 step  71\n",
      "cost:  1.1891926370654833\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [4][5/2459]Time: 0.274 (0.263) Data: 0.230 (0.216) Loss: 0.9992 (1.0545)\n",
      "error:  0.003489269570632114 step  81\n",
      "cost:  1.0490648655025103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt took 0.00min,   81iters\n",
      "Epoch: [4][6/2459]Time: 0.245 (0.261) Data: 0.213 (0.215) Loss: 1.0231 (1.0500)\n",
      "error:  0.008997757713394838 step  71\n",
      "cost:  0.99930465774916\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [4][7/2459]Time: 0.273 (0.262) Data: 0.237 (0.218) Loss: 1.0029 (1.0441)\n",
      "error:  0.004455771013267218 step  91\n",
      "cost:  1.0385325461155621\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [4][8/2459]Time: 0.233 (0.259) Data: 0.200 (0.216) Loss: 1.0239 (1.0419)\n",
      "error:  0.006858789826424294 step  101\n",
      "cost:  1.0506866050301535\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [4][9/2459]Time: 0.222 (0.255) Data: 0.194 (0.214) Loss: 1.0458 (1.0423)\n",
      "error:  0.005596943693829792 step  101\n",
      "cost:  0.9419463060532046\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [4][10/2459]Time: 0.336 (0.263) Data: 0.309 (0.222) Loss: 1.0465 (1.0426)\n",
      "error:  0.0032939098794758648 step  91\n",
      "cost:  0.8605009498081678\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [4][11/2459]Time: 0.238 (0.261) Data: 0.203 (0.221) Loss: 1.0647 (1.0445)\n",
      "error:  0.004176937422322613 step  91\n",
      "cost:  0.8745115582342236\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [4][12/2459]Time: 0.266 (0.261) Data: 0.237 (0.222) Loss: 1.0350 (1.0438)\n",
      "error:  0.0029971132319176474 step  101\n",
      "cost:  0.8539272781337142\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [4][13/2459]Time: 0.305 (0.264) Data: 0.231 (0.223) Loss: 1.0058 (1.0410)\n",
      "error:  0.004157994981105251 step  121\n",
      "cost:  0.7668474817512924\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [4][14/2459]Time: 0.266 (0.264) Data: 0.234 (0.223) Loss: 0.9954 (1.0380)\n",
      "error:  0.008617990068905956 step  131\n",
      "cost:  0.6901328807878077\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [4][15/2459]Time: 0.325 (0.268) Data: 0.249 (0.225) Loss: 1.0032 (1.0358)\n",
      "error:  0.0060055595919612514 step  141\n",
      "cost:  0.6829169601471762\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [4][16/2459]Time: 0.277 (0.269) Data: 0.237 (0.226) Loss: 1.0172 (1.0347)\n",
      "error:  0.006432563176756001 step  171\n",
      "cost:  0.6862478018105205\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [4][17/2459]Time: 0.457 (0.279) Data: 0.427 (0.237) Loss: 0.9805 (1.0317)\n",
      "error:  0.0065974476030881135 step  181\n",
      "cost:  0.6530033662302356\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [4][18/2459]Time: 0.232 (0.277) Data: 0.204 (0.235) Loss: 1.0606 (1.0332)\n",
      "error:  0.007054968273146889 step  191\n",
      "cost:  0.5915452529671966\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [4][19/2459]Time: 0.236 (0.275) Data: 0.208 (0.234) Loss: 1.0062 (1.0319)\n",
      "error:  0.006642844149180593 step  191\n",
      "cost:  0.5356484608603156\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [4][20/2459]Time: 0.223 (0.272) Data: 0.195 (0.232) Loss: 0.9514 (1.0281)\n",
      "error:  0.00910067975391271 step  171\n",
      "cost:  0.5064066043480023\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [4][21/2459]Time: 0.261 (0.272) Data: 0.217 (0.231) Loss: 0.9758 (1.0257)\n",
      "error:  0.006064969655310115 step  211\n",
      "cost:  0.5097377025396305\n",
      "opt took 0.00min,  211iters\n",
      "Epoch: [4][22/2459]Time: 0.269 (0.272) Data: 0.238 (0.232) Loss: 0.9879 (1.0240)\n",
      "error:  0.006181225884224872 step  201\n",
      "cost:  0.5249594403560959\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [4][23/2459]Time: 0.309 (0.273) Data: 0.280 (0.234) Loss: 1.0050 (1.0232)\n",
      "10-NN,s=0.1: TOP1:  34.625\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 5\n",
      "ResNet1D\n",
      "error:  0.006010768972807723 step  61\n",
      "cost:  1.0738902923747036\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [5][0/2459]Time: 0.510 (0.510) Data: 0.430 (0.430) Loss: 0.8539 (0.8539)\n",
      "error:  0.004219597406939113 step  61\n",
      "cost:  1.0662282061512043\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [5][1/2459]Time: 0.251 (0.380) Data: 0.221 (0.325) Loss: 0.8260 (0.8400)\n",
      "error:  0.00372939770222791 step  61\n",
      "cost:  1.0489637312363367\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [5][2/2459]Time: 0.319 (0.360) Data: 0.257 (0.303) Loss: 0.8832 (0.8544)\n",
      "error:  0.009725831130383056 step  61\n",
      "cost:  1.0621264436049838\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [5][3/2459]Time: 0.196 (0.319) Data: 0.171 (0.270) Loss: 0.9126 (0.8689)\n",
      "error:  0.002503063413269224 step  71\n",
      "cost:  1.049825561034396\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [5][4/2459]Time: 0.306 (0.316) Data: 0.264 (0.268) Loss: 0.8600 (0.8671)\n",
      "error:  0.0019935697082938963 step  71\n",
      "cost:  1.0067881522260989\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [5][5/2459]Time: 0.273 (0.309) Data: 0.229 (0.262) Loss: 0.8318 (0.8612)\n",
      "error:  0.00725570550593857 step  61\n",
      "cost:  0.988434324246005\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [5][6/2459]Time: 0.280 (0.305) Data: 0.238 (0.259) Loss: 0.8292 (0.8567)\n",
      "error:  0.009520933564582723 step  71\n",
      "cost:  0.9834423558653616\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [5][7/2459]Time: 0.246 (0.298) Data: 0.195 (0.251) Loss: 0.8410 (0.8547)\n",
      "error:  0.005237205739151873 step  81\n",
      "cost:  0.9931037570651574\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [5][8/2459]Time: 0.275 (0.295) Data: 0.237 (0.249) Loss: 0.8523 (0.8544)\n",
      "error:  0.006219324965094697 step  81\n",
      "cost:  0.9456412828812709\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [5][9/2459]Time: 0.257 (0.291) Data: 0.218 (0.246) Loss: 0.8731 (0.8563)\n",
      "error:  0.006624181580255684 step  81\n",
      "cost:  0.8938056556060926\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [5][10/2459]Time: 0.332 (0.295) Data: 0.292 (0.250) Loss: 0.8510 (0.8558)\n",
      "error:  0.00882591460988924 step  81\n",
      "cost:  0.887830057124242\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [5][11/2459]Time: 0.415 (0.305) Data: 0.351 (0.259) Loss: 0.8706 (0.8571)\n",
      "error:  0.006934812135132917 step  111\n",
      "cost:  0.8902273469620882\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [5][12/2459]Time: 0.243 (0.300) Data: 0.211 (0.255) Loss: 0.8806 (0.8589)\n",
      "error:  0.003903700001152477 step  111\n",
      "cost:  0.867784174557816\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [5][13/2459]Time: 0.337 (0.303) Data: 0.287 (0.257) Loss: 0.8889 (0.8610)\n",
      "error:  0.005889167029682785 step  101\n",
      "cost:  0.8126767635685318\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [5][14/2459]Time: 0.270 (0.301) Data: 0.246 (0.256) Loss: 0.8747 (0.8619)\n",
      "error:  0.006966423387198439 step  91\n",
      "cost:  0.7741148677988853\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [5][15/2459]Time: 0.312 (0.301) Data: 0.278 (0.258) Loss: 0.8386 (0.8605)\n",
      "error:  0.005170380676421615 step  111\n",
      "cost:  0.7588307912130157\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [5][16/2459]Time: 0.272 (0.300) Data: 0.214 (0.255) Loss: 0.8751 (0.8613)\n",
      "error:  0.008489842291602656 step  121\n",
      "cost:  0.7462526694782651\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [5][17/2459]Time: 0.289 (0.299) Data: 0.249 (0.255) Loss: 0.8566 (0.8611)\n",
      "error:  0.008255968254814228 step  141\n",
      "cost:  0.71464974139463\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [5][18/2459]Time: 0.555 (0.313) Data: 0.513 (0.268) Loss: 0.8219 (0.8590)\n",
      "error:  0.008406295021888943 step  171\n",
      "cost:  0.6854311078099662\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [5][19/2459]Time: 0.334 (0.314) Data: 0.299 (0.270) Loss: 0.8570 (0.8589)\n",
      "error:  0.009121589211247283 step  191\n",
      "cost:  0.6824855180577888\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [5][20/2459]Time: 0.282 (0.312) Data: 0.253 (0.269) Loss: 0.9091 (0.8613)\n",
      "error:  0.00890411205938324 step  201\n",
      "cost:  0.6770012052700433\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [5][21/2459]Time: 0.295 (0.311) Data: 0.246 (0.268) Loss: 0.8592 (0.8612)\n",
      "error:  0.006145552836798185 step  191\n",
      "cost:  0.6470624196670883\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [5][22/2459]Time: 0.231 (0.308) Data: 0.204 (0.265) Loss: 0.9318 (0.8643)\n",
      "error:  0.006663571730290974 step  171\n",
      "cost:  0.6175431005405008\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [5][23/2459]Time: 0.353 (0.310) Data: 0.326 (0.268) Loss: 0.8528 (0.8638)\n",
      "10-NN,s=0.1: TOP1:  34.166666666666664\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 6\n",
      "ResNet1D\n",
      "error:  0.004412049739576207 step  81\n",
      "cost:  1.0139639109334344\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [6][0/2459]Time: 0.231 (0.231) Data: 0.199 (0.199) Loss: 0.7606 (0.7606)\n",
      "error:  0.008520888371732926 step  81\n",
      "cost:  1.0408273117664557\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [6][1/2459]Time: 0.292 (0.261) Data: 0.254 (0.227) Loss: 0.7478 (0.7542)\n",
      "error:  0.003917938996116477 step  81\n",
      "cost:  1.0229390340206754\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [6][2/2459]Time: 0.243 (0.255) Data: 0.214 (0.222) Loss: 0.7433 (0.7506)\n",
      "error:  0.0038064566631605956 step  81\n",
      "cost:  1.0320871751910194\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [6][3/2459]Time: 0.301 (0.267) Data: 0.273 (0.235) Loss: 0.7412 (0.7482)\n",
      "error:  0.0029710419295641 step  91\n",
      "cost:  1.0487325385677755\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [6][4/2459]Time: 0.220 (0.257) Data: 0.193 (0.226) Loss: 0.7653 (0.7516)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.004755004547699282 step  91\n",
      "cost:  0.9834487317187092\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [6][5/2459]Time: 0.249 (0.256) Data: 0.222 (0.226) Loss: 0.7701 (0.7547)\n",
      "error:  0.0035226190621376885 step  81\n",
      "cost:  0.9447458100014119\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [6][6/2459]Time: 0.243 (0.254) Data: 0.188 (0.220) Loss: 0.7581 (0.7552)\n",
      "error:  0.005213686955913066 step  81\n",
      "cost:  1.0165045437873264\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [6][7/2459]Time: 0.243 (0.253) Data: 0.207 (0.219) Loss: 0.7291 (0.7519)\n",
      "error:  0.007349195795170793 step  81\n",
      "cost:  1.0204574451763175\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [6][8/2459]Time: 0.279 (0.256) Data: 0.226 (0.219) Loss: 0.7637 (0.7532)\n",
      "error:  0.005329195152325217 step  91\n",
      "cost:  0.9136222752699101\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [6][9/2459]Time: 0.293 (0.259) Data: 0.257 (0.223) Loss: 0.7684 (0.7548)\n",
      "error:  0.008256718692560616 step  91\n",
      "cost:  0.8994790733731991\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [6][10/2459]Time: 0.346 (0.267) Data: 0.277 (0.228) Loss: 0.7408 (0.7535)\n",
      "error:  0.009648170267244405 step  101\n",
      "cost:  0.9277430968369563\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [6][11/2459]Time: 0.284 (0.269) Data: 0.251 (0.230) Loss: 0.7605 (0.7541)\n",
      "error:  0.005399901741936275 step  101\n",
      "cost:  0.9006432453564884\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [6][12/2459]Time: 0.260 (0.268) Data: 0.228 (0.230) Loss: 0.7382 (0.7529)\n",
      "error:  0.00770598573141279 step  111\n",
      "cost:  0.8994183851469383\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [6][13/2459]Time: 0.256 (0.267) Data: 0.222 (0.229) Loss: 0.7344 (0.7515)\n",
      "error:  0.00445029134721886 step  101\n",
      "cost:  0.848147041859418\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [6][14/2459]Time: 0.244 (0.266) Data: 0.215 (0.228) Loss: 0.7152 (0.7491)\n",
      "error:  0.00387543775156185 step  111\n",
      "cost:  0.7847620913849893\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [6][15/2459]Time: 0.305 (0.268) Data: 0.231 (0.228) Loss: 0.7174 (0.7471)\n",
      "error:  0.005419924470157489 step  121\n",
      "cost:  0.8466081445128717\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [6][16/2459]Time: 0.276 (0.269) Data: 0.240 (0.229) Loss: 0.7725 (0.7486)\n",
      "error:  0.006481160114178208 step  111\n",
      "cost:  0.8129965091925061\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [6][17/2459]Time: 0.419 (0.277) Data: 0.371 (0.237) Loss: 0.7942 (0.7512)\n",
      "error:  0.0049280994791371935 step  141\n",
      "cost:  0.7196444660002453\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [6][18/2459]Time: 0.267 (0.276) Data: 0.230 (0.237) Loss: 0.7831 (0.7528)\n",
      "error:  0.008526050481099445 step  141\n",
      "cost:  0.7136358436733166\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [6][19/2459]Time: 0.284 (0.277) Data: 0.207 (0.235) Loss: 0.7525 (0.7528)\n",
      "error:  0.009255850756711315 step  161\n",
      "cost:  0.7570619454336447\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [6][20/2459]Time: 0.241 (0.275) Data: 0.201 (0.233) Loss: 0.7101 (0.7508)\n",
      "error:  0.005866006563610693 step  171\n",
      "cost:  0.7537123410398459\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [6][21/2459]Time: 0.324 (0.277) Data: 0.296 (0.236) Loss: 0.8104 (0.7535)\n",
      "error:  0.005719304220229593 step  161\n",
      "cost:  0.6348096134186354\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [6][22/2459]Time: 0.263 (0.277) Data: 0.234 (0.236) Loss: 0.7066 (0.7515)\n",
      "error:  0.00817795927516507 step  171\n",
      "cost:  0.5921384568567096\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [6][23/2459]Time: 0.262 (0.276) Data: 0.230 (0.236) Loss: 0.7254 (0.7504)\n",
      "10-NN,s=0.1: TOP1:  35.958333333333336\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 7\n",
      "ResNet1D\n",
      "error:  0.007643468668026454 step  81\n",
      "cost:  0.9375673898944799\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [7][0/2459]Time: 0.221 (0.221) Data: 0.194 (0.194) Loss: 0.6321 (0.6321)\n",
      "error:  0.0027735235755275145 step  81\n",
      "cost:  0.9124859315971472\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [7][1/2459]Time: 0.222 (0.222) Data: 0.194 (0.194) Loss: 0.6656 (0.6488)\n",
      "error:  0.005013889405829586 step  81\n",
      "cost:  0.9221355274872381\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [7][2/2459]Time: 0.312 (0.252) Data: 0.266 (0.218) Loss: 0.7180 (0.6719)\n",
      "error:  0.004945349279757871 step  91\n",
      "cost:  0.9288274439776089\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [7][3/2459]Time: 0.221 (0.244) Data: 0.192 (0.212) Loss: 0.6819 (0.6744)\n",
      "error:  0.0037830557728476544 step  101\n",
      "cost:  0.9095692416467483\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [7][4/2459]Time: 0.272 (0.250) Data: 0.229 (0.215) Loss: 0.5840 (0.6563)\n",
      "error:  0.00852473688561517 step  91\n",
      "cost:  0.8872499741179923\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [7][5/2459]Time: 0.247 (0.249) Data: 0.218 (0.216) Loss: 0.6439 (0.6542)\n",
      "error:  0.0049825798863954995 step  91\n",
      "cost:  0.8568074235194819\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [7][6/2459]Time: 0.261 (0.251) Data: 0.212 (0.215) Loss: 0.6982 (0.6605)\n",
      "error:  0.00802886414331272 step  91\n",
      "cost:  0.8741053093305832\n",
      "opt took 0.00min,   91iters\n",
      "Epoch: [7][7/2459]Time: 0.237 (0.249) Data: 0.198 (0.213) Loss: 0.6571 (0.6601)\n",
      "error:  0.0035580030906273707 step  111\n",
      "cost:  0.9337556971006874\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [7][8/2459]Time: 0.295 (0.254) Data: 0.218 (0.214) Loss: 0.6302 (0.6568)\n",
      "error:  0.003692605221350953 step  111\n",
      "cost:  0.8600415153151515\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [7][9/2459]Time: 0.246 (0.253) Data: 0.217 (0.214) Loss: 0.6124 (0.6523)\n",
      "error:  0.004215523080738137 step  111\n",
      "cost:  0.8026282655881338\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [7][10/2459]Time: 0.292 (0.257) Data: 0.210 (0.213) Loss: 0.6614 (0.6532)\n",
      "error:  0.008390467912824717 step  131\n",
      "cost:  0.8331091871411102\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [7][11/2459]Time: 0.282 (0.259) Data: 0.236 (0.215) Loss: 0.6147 (0.6500)\n",
      "error:  0.004328480878083352 step  121\n",
      "cost:  0.8250301102253088\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [7][12/2459]Time: 0.295 (0.262) Data: 0.259 (0.219) Loss: 0.6375 (0.6490)\n",
      "error:  0.006074690546751804 step  131\n",
      "cost:  0.8032925062488098\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [7][13/2459]Time: 0.254 (0.261) Data: 0.222 (0.219) Loss: 0.6785 (0.6511)\n",
      "error:  0.00406756552079568 step  121\n",
      "cost:  0.8061226317402981\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [7][14/2459]Time: 0.264 (0.261) Data: 0.231 (0.220) Loss: 0.6690 (0.6523)\n",
      "error:  0.007646646092553833 step  141\n",
      "cost:  0.7735493029662138\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [7][15/2459]Time: 0.220 (0.259) Data: 0.192 (0.218) Loss: 0.5565 (0.6463)\n",
      "error:  0.007134857804535044 step  151\n",
      "cost:  0.7693316746902722\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [7][16/2459]Time: 0.392 (0.267) Data: 0.334 (0.225) Loss: 0.6017 (0.6437)\n",
      "error:  0.008384577725120024 step  171\n",
      "cost:  0.7561509447113418\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [7][17/2459]Time: 0.274 (0.267) Data: 0.245 (0.226) Loss: 0.6160 (0.6421)\n",
      "error:  0.006298099920580502 step  181\n",
      "cost:  0.678303175962183\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [7][18/2459]Time: 0.465 (0.277) Data: 0.407 (0.236) Loss: 0.5957 (0.6397)\n",
      "error:  0.007693524190419132 step  171\n",
      "cost:  0.6503589916361934\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [7][19/2459]Time: 0.264 (0.277) Data: 0.236 (0.236) Loss: 0.5934 (0.6374)\n",
      "error:  0.005800543610774533 step  191\n",
      "cost:  0.6472564341845131\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [7][20/2459]Time: 0.338 (0.280) Data: 0.275 (0.237) Loss: 0.5912 (0.6352)\n",
      "error:  0.0088408293488218 step  181\n",
      "cost:  0.6189163634194332\n",
      "opt took 0.00min,  181iters\n",
      "Epoch: [7][21/2459]Time: 0.243 (0.278) Data: 0.214 (0.236) Loss: 0.6001 (0.6336)\n",
      "error:  0.008117113818419619 step  221\n",
      "cost:  0.5761010672303193\n",
      "opt took 0.00min,  221iters\n",
      "Epoch: [7][22/2459]Time: 0.278 (0.278) Data: 0.244 (0.237) Loss: 0.6243 (0.6332)\n",
      "error:  0.007120110685029268 step  211\n",
      "cost:  0.5629810203416411\n",
      "opt took 0.00min,  211iters\n",
      "Epoch: [7][23/2459]Time: 0.255 (0.277) Data: 0.219 (0.236) Loss: 0.5950 (0.6316)\n",
      "10-NN,s=0.1: TOP1:  35.5\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 8\n",
      "ResNet1D\n",
      "error:  0.007938709359863516 step  101\n",
      "cost:  0.8620441724570309\n",
      "opt took 0.00min,  101iters\n",
      "Epoch: [8][0/2459]Time: 0.321 (0.321) Data: 0.281 (0.281) Loss: 0.4566 (0.4566)\n",
      "error:  0.008694060642908896 step  111\n",
      "cost:  0.8565787024403478\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [8][1/2459]Time: 0.238 (0.279) Data: 0.210 (0.246) Loss: 0.4705 (0.4636)\n",
      "error:  0.004921153659966482 step  121\n",
      "cost:  0.845735270780162\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [8][2/2459]Time: 0.242 (0.267) Data: 0.210 (0.234) Loss: 0.5005 (0.4759)\n",
      "error:  0.005567597129327639 step  101\n",
      "cost:  0.8229996183695419\n",
      "opt took 0.00min,  101iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][3/2459]Time: 0.271 (0.268) Data: 0.220 (0.230) Loss: 0.5023 (0.4825)\n",
      "error:  0.005201425937310211 step  81\n",
      "cost:  0.7946014116208222\n",
      "opt took 0.00min,   81iters\n",
      "Epoch: [8][4/2459]Time: 0.257 (0.266) Data: 0.233 (0.231) Loss: 0.5724 (0.5005)\n",
      "error:  0.008034367313097035 step  111\n",
      "cost:  0.8100195269284365\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [8][5/2459]Time: 0.227 (0.259) Data: 0.199 (0.226) Loss: 0.5122 (0.5024)\n",
      "error:  0.007153300921307948 step  121\n",
      "cost:  0.8418743265353698\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [8][6/2459]Time: 0.322 (0.268) Data: 0.281 (0.233) Loss: 0.5110 (0.5036)\n",
      "error:  0.006050751035810542 step  121\n",
      "cost:  0.8537294611242168\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [8][7/2459]Time: 0.282 (0.270) Data: 0.244 (0.235) Loss: 0.5513 (0.5096)\n",
      "error:  0.003943946528377706 step  121\n",
      "cost:  0.7971371605609985\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [8][8/2459]Time: 0.287 (0.272) Data: 0.245 (0.236) Loss: 0.5357 (0.5125)\n",
      "error:  0.007113244186013334 step  121\n",
      "cost:  0.7716834320187184\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [8][9/2459]Time: 0.567 (0.301) Data: 0.516 (0.264) Loss: 0.4803 (0.5093)\n",
      "error:  0.005879422728434447 step  121\n",
      "cost:  0.8435954013174497\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [8][10/2459]Time: 0.339 (0.305) Data: 0.265 (0.264) Loss: 0.4964 (0.5081)\n",
      "error:  0.0060449247125973216 step  131\n",
      "cost:  0.8784668045362115\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [8][11/2459]Time: 0.352 (0.309) Data: 0.302 (0.267) Loss: 0.5532 (0.5119)\n",
      "error:  0.004379232477698225 step  141\n",
      "cost:  0.795892688016319\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [8][12/2459]Time: 0.464 (0.321) Data: 0.421 (0.279) Loss: 0.5225 (0.5127)\n",
      "error:  0.005534304075485497 step  121\n",
      "cost:  0.759725226192458\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [8][13/2459]Time: 0.448 (0.330) Data: 0.406 (0.288) Loss: 0.5650 (0.5164)\n",
      "error:  0.006546357920315771 step  141\n",
      "cost:  0.7755222507662693\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [8][14/2459]Time: 0.387 (0.334) Data: 0.355 (0.293) Loss: 0.5236 (0.5169)\n",
      "error:  0.009186249848236128 step  141\n",
      "cost:  0.854536985746183\n",
      "opt took 0.01min,  141iters\n",
      "Epoch: [8][15/2459]Time: 0.629 (0.352) Data: 0.593 (0.311) Loss: 0.5347 (0.5180)\n",
      "error:  0.005281498446328081 step  151\n",
      "cost:  0.8613461102148166\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [8][16/2459]Time: 0.288 (0.348) Data: 0.247 (0.307) Loss: 0.5402 (0.5193)\n",
      "error:  0.007096979580771934 step  131\n",
      "cost:  0.712918357236093\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [8][17/2459]Time: 0.341 (0.348) Data: 0.299 (0.307) Loss: 0.5499 (0.5210)\n",
      "error:  0.008163867901116006 step  141\n",
      "cost:  0.7059344235993563\n",
      "opt took 0.01min,  141iters\n",
      "Epoch: [8][18/2459]Time: 0.798 (0.372) Data: 0.703 (0.328) Loss: 0.5372 (0.5219)\n",
      "error:  0.006596784900388064 step  161\n",
      "cost:  0.7614673094892501\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [8][19/2459]Time: 0.298 (0.368) Data: 0.260 (0.324) Loss: 0.5224 (0.5219)\n",
      "error:  0.007325350047876822 step  171\n",
      "cost:  0.7155658477446676\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [8][20/2459]Time: 0.451 (0.372) Data: 0.400 (0.328) Loss: 0.5377 (0.5226)\n",
      "error:  0.006378508519072845 step  161\n",
      "cost:  0.6580049590284553\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [8][21/2459]Time: 0.430 (0.374) Data: 0.392 (0.331) Loss: 0.6107 (0.5267)\n",
      "error:  0.007572368583346312 step  161\n",
      "cost:  0.6444111561382967\n",
      "opt took 0.01min,  161iters\n",
      "Epoch: [8][22/2459]Time: 0.791 (0.393) Data: 0.761 (0.350) Loss: 0.5221 (0.5265)\n",
      "error:  0.005968716483367942 step  171\n",
      "cost:  0.6434854662844383\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [8][23/2459]Time: 0.438 (0.394) Data: 0.362 (0.350) Loss: 0.6018 (0.5296)\n",
      "10-NN,s=0.1: TOP1:  34.75\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 9\n",
      "ResNet1D\n",
      "error:  0.005224915401011221 step  111\n",
      "cost:  0.9113783940671354\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [9][0/2459]Time: 0.291 (0.291) Data: 0.268 (0.268) Loss: 0.3993 (0.3993)\n",
      "error:  0.005447330562022179 step  121\n",
      "cost:  0.9102806788171892\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [9][1/2459]Time: 0.225 (0.258) Data: 0.195 (0.231) Loss: 0.4532 (0.4262)\n",
      "error:  0.004951548716516818 step  111\n",
      "cost:  0.8048964826104548\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [9][2/2459]Time: 0.324 (0.280) Data: 0.283 (0.249) Loss: 0.4225 (0.4250)\n",
      "error:  0.00634451950897863 step  121\n",
      "cost:  0.785672251761219\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [9][3/2459]Time: 0.231 (0.268) Data: 0.203 (0.237) Loss: 0.4620 (0.4342)\n",
      "error:  0.006544260766367271 step  121\n",
      "cost:  0.9079229460522488\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [9][4/2459]Time: 0.321 (0.278) Data: 0.283 (0.246) Loss: 0.4472 (0.4368)\n",
      "error:  0.007724478339125707 step  131\n",
      "cost:  0.8508693952125709\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [9][5/2459]Time: 0.230 (0.270) Data: 0.201 (0.239) Loss: 0.4619 (0.4410)\n",
      "error:  0.007322339154492097 step  121\n",
      "cost:  0.7651894929922132\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [9][6/2459]Time: 0.259 (0.269) Data: 0.216 (0.236) Loss: 0.4811 (0.4468)\n",
      "error:  0.006251076568922098 step  121\n",
      "cost:  0.7733206538949186\n",
      "opt took 0.00min,  121iters\n",
      "Epoch: [9][7/2459]Time: 0.357 (0.280) Data: 0.316 (0.246) Loss: 0.4192 (0.4433)\n",
      "error:  0.005159690412157136 step  131\n",
      "cost:  0.8462675363746769\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [9][8/2459]Time: 0.303 (0.282) Data: 0.273 (0.249) Loss: 0.4667 (0.4459)\n",
      "error:  0.007171547165267467 step  131\n",
      "cost:  0.8225586407295842\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [9][9/2459]Time: 0.547 (0.309) Data: 0.481 (0.272) Loss: 0.4987 (0.4512)\n",
      "error:  0.005716304102079572 step  121\n",
      "cost:  0.7390709015285346\n",
      "opt took 0.01min,  121iters\n",
      "Epoch: [9][10/2459]Time: 0.846 (0.358) Data: 0.790 (0.319) Loss: 0.4370 (0.4499)\n",
      "error:  0.008001220272777276 step  111\n",
      "cost:  0.703462245739076\n",
      "opt took 0.00min,  111iters\n",
      "Epoch: [9][11/2459]Time: 0.252 (0.349) Data: 0.222 (0.311) Loss: 0.4154 (0.4470)\n",
      "error:  0.007651369558076326 step  141\n",
      "cost:  0.7728054687504134\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [9][12/2459]Time: 0.318 (0.346) Data: 0.280 (0.309) Loss: 0.4858 (0.4500)\n",
      "error:  0.007104536122940264 step  141\n",
      "cost:  0.8025506772863858\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [9][13/2459]Time: 0.305 (0.343) Data: 0.244 (0.304) Loss: 0.4548 (0.4503)\n",
      "error:  0.008261914817935367 step  131\n",
      "cost:  0.728837744862056\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [9][14/2459]Time: 0.237 (0.336) Data: 0.209 (0.298) Loss: 0.4156 (0.4480)\n",
      "error:  0.007276988372229343 step  131\n",
      "cost:  0.6931007086031639\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [9][15/2459]Time: 0.340 (0.337) Data: 0.244 (0.294) Loss: 0.5030 (0.4515)\n",
      "error:  0.007134792159547487 step  151\n",
      "cost:  0.731328116298299\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [9][16/2459]Time: 0.449 (0.343) Data: 0.379 (0.299) Loss: 0.4202 (0.4496)\n",
      "error:  0.00835285717874934 step  151\n",
      "cost:  0.7873501862377099\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [9][17/2459]Time: 0.242 (0.338) Data: 0.213 (0.294) Loss: 0.4948 (0.4521)\n",
      "error:  0.009386615871053716 step  151\n",
      "cost:  0.7437495661175808\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [9][18/2459]Time: 0.354 (0.338) Data: 0.319 (0.296) Loss: 0.4695 (0.4531)\n",
      "error:  0.004566529720437917 step  141\n",
      "cost:  0.6645360014726418\n",
      "opt took 0.00min,  141iters\n",
      "Epoch: [9][19/2459]Time: 0.282 (0.336) Data: 0.242 (0.293) Loss: 0.4851 (0.4547)\n",
      "error:  0.006389003136672455 step  131\n",
      "cost:  0.6409872895676212\n",
      "opt took 0.00min,  131iters\n",
      "Epoch: [9][20/2459]Time: 0.425 (0.340) Data: 0.379 (0.297) Loss: 0.4546 (0.4547)\n",
      "error:  0.005680283240041217 step  151\n",
      "cost:  0.6864732274442772\n",
      "opt took 0.00min,  151iters\n",
      "Epoch: [9][21/2459]Time: 0.239 (0.335) Data: 0.199 (0.293) Loss: 0.4699 (0.4553)\n",
      "error:  0.004782417269515782 step  161\n",
      "cost:  0.743579064783962\n",
      "opt took 0.00min,  161iters\n",
      "Epoch: [9][22/2459]Time: 0.251 (0.332) Data: 0.221 (0.290) Loss: 0.4979 (0.4572)\n",
      "error:  0.005227564278925012 step  171\n",
      "cost:  0.7186013642244734\n",
      "opt took 0.00min,  171iters\n",
      "Epoch: [9][23/2459]Time: 0.231 (0.327) Data: 0.201 (0.286) Loss: 0.4171 (0.4555)\n",
      "10-NN,s=0.1: TOP1:  35.875\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 10\n",
      "ResNet1D\n",
      "error:  0.00920694113627174 step  621\n",
      "cost:  0.2539407985590562\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [10][0/2459]Time: 0.532 (0.532) Data: 0.494 (0.494) Loss: 0.3889 (0.3889)\n",
      "error:  0.008783631504777789 step  591\n",
      "cost:  0.253313899189726\n",
      "opt took 0.01min,  591iters\n",
      "Epoch: [10][1/2459]Time: 0.884 (0.708) Data: 0.831 (0.663) Loss: 0.3906 (0.3897)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009059753338954812 step  421\n",
      "cost:  0.2538927867403954\n",
      "opt took 0.00min,  421iters\n",
      "Epoch: [10][2/2459]Time: 0.460 (0.625) Data: 0.421 (0.582) Loss: 0.4272 (0.4022)\n",
      "error:  0.00834516760358861 step  351\n",
      "cost:  0.26049631117048794\n",
      "opt took 0.00min,  351iters\n",
      "Epoch: [10][3/2459]Time: 0.502 (0.594) Data: 0.460 (0.552) Loss: 0.4380 (0.4112)\n",
      "error:  0.008786111784765516 step  561\n",
      "cost:  0.261746784950808\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [10][4/2459]Time: 0.472 (0.570) Data: 0.409 (0.523) Loss: 0.4058 (0.4101)\n",
      "error:  0.009408713400304802 step  461\n",
      "cost:  0.2614113445244886\n",
      "opt took 0.00min,  461iters\n",
      "Epoch: [10][5/2459]Time: 0.354 (0.534) Data: 0.310 (0.488) Loss: 0.3763 (0.4045)\n",
      "error:  0.00929643125775137 step  441\n",
      "cost:  0.26004961461618237\n",
      "opt took 0.01min,  441iters\n",
      "Epoch: [10][6/2459]Time: 0.838 (0.577) Data: 0.784 (0.530) Loss: 0.4295 (0.4080)\n",
      "error:  0.009007748644344282 step  521\n",
      "cost:  0.24868546818041054\n",
      "opt took 0.00min,  521iters\n",
      "Epoch: [10][7/2459]Time: 0.355 (0.549) Data: 0.288 (0.500) Loss: 0.3878 (0.4055)\n",
      "error:  0.008624963449049727 step  431\n",
      "cost:  0.24944072799252193\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [10][8/2459]Time: 0.250 (0.516) Data: 0.222 (0.469) Loss: 0.4519 (0.4107)\n",
      "error:  0.00933641439523003 step  521\n",
      "cost:  0.25077583685679394\n",
      "opt took 0.00min,  521iters\n",
      "Epoch: [10][9/2459]Time: 0.271 (0.492) Data: 0.242 (0.446) Loss: 0.4185 (0.4114)\n",
      "error:  0.009593508445049914 step  571\n",
      "cost:  0.25885663704900214\n",
      "opt took 0.00min,  571iters\n",
      "Epoch: [10][10/2459]Time: 0.560 (0.498) Data: 0.491 (0.450) Loss: 0.4089 (0.4112)\n",
      "error:  0.009171484671293362 step  681\n",
      "cost:  0.26469942306485506\n",
      "opt took 0.00min,  681iters\n",
      "Epoch: [10][11/2459]Time: 0.545 (0.502) Data: 0.507 (0.455) Loss: 0.4114 (0.4112)\n",
      "error:  0.009546057932266483 step  1071\n",
      "cost:  0.26258501625103237\n",
      "opt took 0.01min, 1071iters\n",
      "Epoch: [10][12/2459]Time: 0.888 (0.532) Data: 0.842 (0.485) Loss: 0.3792 (0.4088)\n",
      "error:  0.009854700905780223 step  761\n",
      "cost:  0.25117812338326867\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [10][13/2459]Time: 0.423 (0.524) Data: 0.351 (0.475) Loss: 0.4026 (0.4083)\n",
      "error:  0.009985240288771013 step  891\n",
      "cost:  0.23951386204923464\n",
      "opt took 0.01min,  891iters\n",
      "Epoch: [10][14/2459]Time: 0.619 (0.530) Data: 0.574 (0.482) Loss: 0.4012 (0.4079)\n",
      "error:  0.008966857046288967 step  571\n",
      "cost:  0.2318533853815741\n",
      "opt took 0.00min,  571iters\n",
      "Epoch: [10][15/2459]Time: 0.477 (0.527) Data: 0.430 (0.479) Loss: 0.3761 (0.4059)\n",
      "error:  0.009367661847524245 step  531\n",
      "cost:  0.23382984633646248\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [10][16/2459]Time: 0.480 (0.524) Data: 0.436 (0.476) Loss: 0.3656 (0.4035)\n",
      "error:  0.008957767690791152 step  571\n",
      "cost:  0.24944152121300722\n",
      "opt took 0.01min,  571iters\n",
      "Epoch: [10][17/2459]Time: 0.656 (0.531) Data: 0.618 (0.484) Loss: 0.3965 (0.4031)\n",
      "error:  0.00950462035900923 step  611\n",
      "cost:  0.24415964020535247\n",
      "opt took 0.01min,  611iters\n",
      "Epoch: [10][18/2459]Time: 0.599 (0.535) Data: 0.543 (0.487) Loss: 0.4051 (0.4032)\n",
      "error:  0.008638767148562376 step  361\n",
      "cost:  0.2344356117363159\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [10][19/2459]Time: 0.410 (0.529) Data: 0.354 (0.480) Loss: 0.3888 (0.4025)\n",
      "error:  0.007953004019614407 step  221\n",
      "cost:  0.2276917166258458\n",
      "opt took 0.00min,  221iters\n",
      "Epoch: [10][20/2459]Time: 0.526 (0.529) Data: 0.466 (0.480) Loss: 0.4057 (0.4026)\n",
      "error:  0.008726754829412764 step  431\n",
      "cost:  0.22560669544482073\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [10][21/2459]Time: 0.378 (0.522) Data: 0.301 (0.472) Loss: 0.3985 (0.4025)\n",
      "error:  0.009727109212128049 step  1011\n",
      "cost:  0.22937253063411653\n",
      "opt took 0.01min, 1011iters\n",
      "Epoch: [10][22/2459]Time: 0.767 (0.532) Data: 0.740 (0.483) Loss: 0.4323 (0.4038)\n",
      "error:  0.009392409857844952 step  851\n",
      "cost:  0.23938691007588012\n",
      "opt took 0.00min,  851iters\n",
      "Epoch: [10][23/2459]Time: 0.296 (0.523) Data: 0.250 (0.473) Loss: 0.4280 (0.4048)\n",
      "10-NN,s=0.1: TOP1:  35.791666666666664\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 11\n",
      "ResNet1D\n",
      "error:  0.009902102184342731 step  561\n",
      "cost:  0.19395476196791098\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [11][0/2459]Time: 0.494 (0.494) Data: 0.407 (0.407) Loss: 0.3552 (0.3552)\n",
      "error:  0.009991515316234878 step  651\n",
      "cost:  0.20127684620496625\n",
      "opt took 0.01min,  651iters\n",
      "Epoch: [11][1/2459]Time: 0.819 (0.657) Data: 0.771 (0.589) Loss: 0.3041 (0.3297)\n",
      "error:  0.009203730284758405 step  771\n",
      "cost:  0.1970703976600253\n",
      "opt took 0.00min,  771iters\n",
      "Epoch: [11][2/2459]Time: 0.555 (0.623) Data: 0.485 (0.554) Loss: 0.3364 (0.3319)\n",
      "error:  0.00929367203808884 step  431\n",
      "cost:  0.19098170326295047\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [11][3/2459]Time: 0.635 (0.626) Data: 0.598 (0.565) Loss: 0.4327 (0.3571)\n",
      "error:  0.00883310955598271 step  481\n",
      "cost:  0.18825178468370865\n",
      "opt took 0.00min,  481iters\n",
      "Epoch: [11][4/2459]Time: 0.416 (0.584) Data: 0.394 (0.531) Loss: 0.3200 (0.3497)\n",
      "error:  0.009049399425987925 step  591\n",
      "cost:  0.18476282786566497\n",
      "opt took 0.00min,  591iters\n",
      "Epoch: [11][5/2459]Time: 0.423 (0.557) Data: 0.381 (0.506) Loss: 0.3641 (0.3521)\n",
      "error:  0.008396635264472985 step  481\n",
      "cost:  0.19015171325636415\n",
      "opt took 0.00min,  481iters\n",
      "Epoch: [11][6/2459]Time: 0.564 (0.558) Data: 0.489 (0.503) Loss: 0.3358 (0.3498)\n",
      "error:  0.00959232858391279 step  461\n",
      "cost:  0.18813199770345326\n",
      "opt took 0.00min,  461iters\n",
      "Epoch: [11][7/2459]Time: 0.523 (0.554) Data: 0.468 (0.499) Loss: 0.3398 (0.3485)\n",
      "error:  0.008659268601273085 step  401\n",
      "cost:  0.1930415335781572\n",
      "opt took 0.00min,  401iters\n",
      "Epoch: [11][8/2459]Time: 0.242 (0.519) Data: 0.213 (0.467) Loss: 0.3117 (0.3444)\n",
      "error:  0.008742679776679307 step  441\n",
      "cost:  0.19127735195902548\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [11][9/2459]Time: 0.455 (0.513) Data: 0.406 (0.461) Loss: 0.3466 (0.3446)\n",
      "error:  0.008911901858397075 step  501\n",
      "cost:  0.1900908408980465\n",
      "opt took 0.01min,  501iters\n",
      "Epoch: [11][10/2459]Time: 0.614 (0.522) Data: 0.576 (0.471) Loss: 0.3660 (0.3466)\n",
      "error:  0.009400697812459557 step  491\n",
      "cost:  0.18773222849530652\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [11][11/2459]Time: 0.384 (0.510) Data: 0.315 (0.458) Loss: 0.3546 (0.3473)\n",
      "error:  0.009795370034977102 step  441\n",
      "cost:  0.1889105861202044\n",
      "opt took 0.01min,  441iters\n",
      "Epoch: [11][12/2459]Time: 0.661 (0.522) Data: 0.631 (0.472) Loss: 0.3733 (0.3493)\n",
      "error:  0.008830030786248111 step  441\n",
      "cost:  0.18916235888207414\n",
      "opt took 0.01min,  441iters\n",
      "Epoch: [11][13/2459]Time: 0.684 (0.534) Data: 0.639 (0.484) Loss: 0.3223 (0.3473)\n",
      "error:  0.008817187086940792 step  441\n",
      "cost:  0.1828681029103806\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [11][14/2459]Time: 0.415 (0.526) Data: 0.379 (0.477) Loss: 0.3899 (0.3502)\n",
      "error:  0.009736213607438371 step  271\n",
      "cost:  0.17851187185861309\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [11][15/2459]Time: 0.390 (0.517) Data: 0.327 (0.467) Loss: 0.3707 (0.3515)\n",
      "error:  0.009115056743485894 step  581\n",
      "cost:  0.1767927533766524\n",
      "opt took 0.01min,  581iters\n",
      "Epoch: [11][16/2459]Time: 0.664 (0.526) Data: 0.624 (0.477) Loss: 0.3342 (0.3504)\n",
      "error:  0.009138565446003888 step  611\n",
      "cost:  0.17796411186707417\n",
      "opt took 0.00min,  611iters\n",
      "Epoch: [11][17/2459]Time: 0.303 (0.514) Data: 0.275 (0.465) Loss: 0.3733 (0.3517)\n",
      "error:  0.009687760028242676 step  451\n",
      "cost:  0.17831823375528616\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [11][18/2459]Time: 0.334 (0.504) Data: 0.294 (0.456) Loss: 0.3297 (0.3505)\n",
      "error:  0.009442379827898084 step  571\n",
      "cost:  0.18043872264299074\n",
      "opt took 0.00min,  571iters\n",
      "Epoch: [11][19/2459]Time: 0.280 (0.493) Data: 0.251 (0.446) Loss: 0.3635 (0.3512)\n",
      "error:  0.009972625929139278 step  661\n",
      "cost:  0.18621864236677702\n",
      "opt took 0.00min,  661iters\n",
      "Epoch: [11][20/2459]Time: 0.280 (0.483) Data: 0.252 (0.437) Loss: 0.3572 (0.3515)\n",
      "error:  0.009178832081119914 step  521\n",
      "cost:  0.19539376568790276\n",
      "opt took 0.00min,  521iters\n",
      "Epoch: [11][21/2459]Time: 0.354 (0.477) Data: 0.284 (0.430) Loss: 0.3320 (0.3506)\n",
      "error:  0.008937880147541377 step  381\n",
      "cost:  0.19399782477574123\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [11][22/2459]Time: 0.406 (0.474) Data: 0.363 (0.427) Loss: 0.3596 (0.3510)\n",
      "error:  0.008303989750431895 step  401\n",
      "cost:  0.1912553397909367\n",
      "opt took 0.00min,  401iters\n",
      "Epoch: [11][23/2459]Time: 0.339 (0.468) Data: 0.299 (0.422) Loss: 0.3444 (0.3507)\n",
      "10-NN,s=0.1: TOP1:  35.583333333333336\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 12\n",
      "ResNet1D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.009902684696318387 step  701\n",
      "cost:  0.18564568937164855\n",
      "opt took 0.01min,  701iters\n",
      "Epoch: [12][0/2459]Time: 0.727 (0.727) Data: 0.688 (0.688) Loss: 0.3261 (0.3261)\n",
      "error:  0.00982441627565156 step  861\n",
      "cost:  0.18109637010317794\n",
      "opt took 0.01min,  861iters\n",
      "Epoch: [12][1/2459]Time: 0.612 (0.669) Data: 0.555 (0.621) Loss: 0.2736 (0.2998)\n",
      "error:  0.009385973709764128 step  931\n",
      "cost:  0.17441269459864206\n",
      "opt took 0.00min,  931iters\n",
      "Epoch: [12][2/2459]Time: 0.585 (0.641) Data: 0.522 (0.588) Loss: 0.2808 (0.2935)\n",
      "error:  0.009153853861750605 step  761\n",
      "cost:  0.17824531988697045\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [12][3/2459]Time: 0.428 (0.588) Data: 0.400 (0.541) Loss: 0.3489 (0.3073)\n",
      "error:  0.009942316549257924 step  611\n",
      "cost:  0.17545675058796756\n",
      "opt took 0.00min,  611iters\n",
      "Epoch: [12][4/2459]Time: 0.542 (0.579) Data: 0.514 (0.536) Loss: 0.2904 (0.3040)\n",
      "error:  0.009484231277990252 step  501\n",
      "cost:  0.17574662691213017\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [12][5/2459]Time: 0.488 (0.564) Data: 0.449 (0.521) Loss: 0.3013 (0.3035)\n",
      "error:  0.009314963191944803 step  961\n",
      "cost:  0.1770527068176007\n",
      "opt took 0.00min,  961iters\n",
      "Epoch: [12][6/2459]Time: 0.503 (0.555) Data: 0.476 (0.515) Loss: 0.4213 (0.3203)\n",
      "error:  0.009907210495845531 step  681\n",
      "cost:  0.17845243342289632\n",
      "opt took 0.01min,  681iters\n",
      "Epoch: [12][7/2459]Time: 0.596 (0.560) Data: 0.568 (0.522) Loss: 0.3663 (0.3261)\n",
      "error:  0.009555720972259651 step  851\n",
      "cost:  0.18194646831008998\n",
      "opt took 0.01min,  851iters\n",
      "Epoch: [12][8/2459]Time: 1.089 (0.619) Data: 1.050 (0.580) Loss: 0.3247 (0.3259)\n",
      "error:  0.00945858109310127 step  911\n",
      "cost:  0.18039988985157293\n",
      "opt took 0.01min,  911iters\n",
      "Epoch: [12][9/2459]Time: 0.854 (0.642) Data: 0.804 (0.603) Loss: 0.3127 (0.3246)\n",
      "error:  0.00965002532232051 step  1231\n",
      "cost:  0.18322431556488886\n",
      "opt took 0.00min, 1231iters\n",
      "Epoch: [12][10/2459]Time: 0.394 (0.620) Data: 0.355 (0.580) Loss: 0.3093 (0.3232)\n",
      "error:  0.009192144137208258 step  901\n",
      "cost:  0.18058621917667053\n",
      "opt took 0.00min,  901iters\n",
      "Epoch: [12][11/2459]Time: 0.476 (0.608) Data: 0.434 (0.568) Loss: 0.2851 (0.3200)\n",
      "error:  0.00954012233467616 step  701\n",
      "cost:  0.17578684878742187\n",
      "opt took 0.00min,  701iters\n",
      "Epoch: [12][12/2459]Time: 0.440 (0.595) Data: 0.402 (0.555) Loss: 0.3183 (0.3199)\n",
      "error:  0.00881891790848599 step  551\n",
      "cost:  0.17224498593311527\n",
      "opt took 0.01min,  551iters\n",
      "Epoch: [12][13/2459]Time: 0.777 (0.608) Data: 0.722 (0.567) Loss: 0.3426 (0.3215)\n",
      "error:  0.009231086460916527 step  381\n",
      "cost:  0.1672448400897658\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [12][14/2459]Time: 0.395 (0.594) Data: 0.367 (0.554) Loss: 0.3246 (0.3217)\n",
      "error:  0.00989150576064235 step  381\n",
      "cost:  0.17006077006094006\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [12][15/2459]Time: 0.347 (0.578) Data: 0.296 (0.538) Loss: 0.3728 (0.3249)\n",
      "error:  0.009213838874455371 step  661\n",
      "cost:  0.1736804504323604\n",
      "opt took 0.01min,  661iters\n",
      "Epoch: [12][16/2459]Time: 0.630 (0.581) Data: 0.571 (0.540) Loss: 0.3298 (0.3252)\n",
      "error:  0.009145919279591741 step  721\n",
      "cost:  0.17561673593091234\n",
      "opt took 0.01min,  721iters\n",
      "Epoch: [12][17/2459]Time: 0.702 (0.588) Data: 0.661 (0.546) Loss: 0.2988 (0.3237)\n",
      "error:  0.00962179140819952 step  681\n",
      "cost:  0.1813709419113077\n",
      "opt took 0.00min,  681iters\n",
      "Epoch: [12][18/2459]Time: 0.355 (0.576) Data: 0.327 (0.535) Loss: 0.3146 (0.3233)\n",
      "error:  0.00963809939225413 step  911\n",
      "cost:  0.1778171271278391\n",
      "opt took 0.00min,  911iters\n",
      "Epoch: [12][19/2459]Time: 0.710 (0.583) Data: 0.664 (0.541) Loss: 0.3355 (0.3239)\n",
      "error:  0.009583244811131042 step  781\n",
      "cost:  0.17387908800916108\n",
      "opt took 0.00min,  781iters\n",
      "Epoch: [12][20/2459]Time: 0.485 (0.578) Data: 0.442 (0.537) Loss: 0.3490 (0.3251)\n",
      "error:  0.009729725266166778 step  581\n",
      "cost:  0.17166762053255102\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [12][21/2459]Time: 0.428 (0.571) Data: 0.394 (0.530) Loss: 0.3421 (0.3259)\n",
      "error:  0.009082301104351154 step  491\n",
      "cost:  0.16308495794746392\n",
      "opt took 0.00min,  491iters\n",
      "Epoch: [12][22/2459]Time: 0.345 (0.561) Data: 0.316 (0.521) Loss: 0.3135 (0.3253)\n",
      "error:  0.009564831915071625 step  681\n",
      "cost:  0.16430566304082742\n",
      "opt took 0.00min,  681iters\n",
      "Epoch: [12][23/2459]Time: 0.399 (0.555) Data: 0.357 (0.514) Loss: 0.3383 (0.3259)\n",
      "10-NN,s=0.1: TOP1:  36.041666666666664\n",
      "best accuracy: 36.21\n",
      "\n",
      "Epoch: 13\n",
      "ResNet1D\n",
      "error:  0.009753995334459198 step  741\n",
      "cost:  0.1800773933293588\n",
      "opt took 0.00min,  741iters\n",
      "Epoch: [13][0/2459]Time: 0.356 (0.356) Data: 0.328 (0.328) Loss: 0.2573 (0.2573)\n",
      "error:  0.009451205449028777 step  1161\n",
      "cost:  0.18459219129531165\n",
      "opt took 0.01min, 1161iters\n",
      "Epoch: [13][1/2459]Time: 1.063 (0.709) Data: 1.024 (0.676) Loss: 0.2969 (0.2771)\n",
      "error:  0.00975553298136933 step  881\n",
      "cost:  0.1878222641620223\n",
      "opt took 0.00min,  881iters\n",
      "Epoch: [13][2/2459]Time: 0.490 (0.636) Data: 0.463 (0.605) Loss: 0.2475 (0.2672)\n",
      "error:  0.009036214102733253 step  781\n",
      "cost:  0.18734943032158638\n",
      "opt took 0.00min,  781iters\n",
      "Epoch: [13][3/2459]Time: 0.288 (0.549) Data: 0.260 (0.519) Loss: 0.3053 (0.2767)\n",
      "error:  0.00985673806945464 step  1071\n",
      "cost:  0.18338107376770724\n",
      "opt took 0.00min, 1071iters\n",
      "Epoch: [13][4/2459]Time: 0.463 (0.532) Data: 0.435 (0.502) Loss: 0.2461 (0.2706)\n",
      "error:  0.009790065763630507 step  831\n",
      "cost:  0.17855452836503483\n",
      "opt took 0.01min,  831iters\n",
      "Epoch: [13][5/2459]Time: 0.903 (0.594) Data: 0.830 (0.557) Loss: 0.2578 (0.2685)\n",
      "error:  0.008672408770941442 step  361\n",
      "cost:  0.17129375872332045\n",
      "opt took 0.01min,  361iters\n",
      "Epoch: [13][6/2459]Time: 0.781 (0.620) Data: 0.737 (0.582) Loss: 0.2893 (0.2714)\n",
      "error:  0.009746658763464033 step  641\n",
      "cost:  0.16513371902030188\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [13][7/2459]Time: 0.442 (0.598) Data: 0.412 (0.561) Loss: 0.2938 (0.2742)\n",
      "error:  0.009184718390281121 step  411\n",
      "cost:  0.1737590782568766\n",
      "opt took 0.01min,  411iters\n",
      "Epoch: [13][8/2459]Time: 0.789 (0.619) Data: 0.747 (0.582) Loss: 0.2976 (0.2768)\n",
      "error:  0.009834606944362556 step  721\n",
      "cost:  0.18591469463808866\n",
      "opt took 0.00min,  721iters\n",
      "Epoch: [13][9/2459]Time: 0.383 (0.596) Data: 0.355 (0.559) Loss: 0.2893 (0.2781)\n",
      "error:  0.009568658894996918 step  821\n",
      "cost:  0.19265974122389906\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [13][10/2459]Time: 0.465 (0.584) Data: 0.438 (0.548) Loss: 0.2648 (0.2769)\n",
      "error:  0.009436947158856124 step  1081\n",
      "cost:  0.18791680952354375\n",
      "opt took 0.01min, 1081iters\n",
      "Epoch: [13][11/2459]Time: 0.838 (0.605) Data: 0.797 (0.569) Loss: 0.2952 (0.2784)\n",
      "error:  0.009250272448697028 step  621\n",
      "cost:  0.1851480012088823\n",
      "opt took 0.01min,  621iters\n",
      "Epoch: [13][12/2459]Time: 0.607 (0.605) Data: 0.553 (0.568) Loss: 0.2630 (0.2772)\n",
      "error:  0.009106390959571109 step  721\n",
      "cost:  0.17549354004051304\n",
      "opt took 0.00min,  721iters\n",
      "Epoch: [13][13/2459]Time: 0.539 (0.600) Data: 0.506 (0.563) Loss: 0.2864 (0.2779)\n",
      "error:  0.009408732457447777 step  551\n",
      "cost:  0.168346426578046\n",
      "opt took 0.00min,  551iters\n",
      "Epoch: [13][14/2459]Time: 0.472 (0.592) Data: 0.432 (0.554) Loss: 0.2957 (0.2791)\n",
      "error:  0.009769272253686623 step  911\n",
      "cost:  0.17019170088948782\n",
      "opt took 0.01min,  911iters\n",
      "Epoch: [13][15/2459]Time: 0.793 (0.604) Data: 0.734 (0.566) Loss: 0.2787 (0.2790)\n",
      "error:  0.0094315716388228 step  1161\n",
      "cost:  0.17109300678448042\n",
      "opt took 0.01min, 1161iters\n",
      "Epoch: [13][16/2459]Time: 1.033 (0.630) Data: 0.950 (0.588) Loss: 0.2608 (0.2780)\n",
      "error:  0.009516504203962883 step  1451\n",
      "cost:  0.17284060211312408\n",
      "opt took 0.00min, 1451iters\n",
      "Epoch: [13][17/2459]Time: 0.511 (0.623) Data: 0.478 (0.582) Loss: 0.2708 (0.2776)\n",
      "error:  0.009864377194771956 step  1201\n",
      "cost:  0.1811385955845988\n",
      "opt took 0.01min, 1201iters\n",
      "Epoch: [13][18/2459]Time: 0.619 (0.623) Data: 0.547 (0.580) Loss: 0.2868 (0.2781)\n",
      "error:  0.00954102366216747 step  1261\n",
      "cost:  0.17507902488414184\n",
      "opt took 0.01min, 1261iters\n",
      "Epoch: [13][19/2459]Time: 0.607 (0.622) Data: 0.576 (0.580) Loss: 0.2714 (0.2777)\n",
      "error:  0.009783817352575608 step  951\n",
      "cost:  0.1732064460454417\n",
      "opt took 0.01min,  951iters\n",
      "Epoch: [13][20/2459]Time: 0.614 (0.622) Data: 0.562 (0.579) Loss: 0.2657 (0.2771)\n",
      "error:  0.009193312196612013 step  551\n",
      "cost:  0.16903363025645068\n",
      "opt took 0.00min,  551iters\n",
      "Epoch: [13][21/2459]Time: 0.323 (0.608) Data: 0.294 (0.566) Loss: 0.2831 (0.2774)\n",
      "error:  0.00979476438674498 step  471\n",
      "cost:  0.17491978733796787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt took 0.00min,  471iters\n",
      "Epoch: [13][22/2459]Time: 0.434 (0.600) Data: 0.395 (0.559) Loss: 0.3472 (0.2805)\n",
      "error:  0.009364897863807564 step  691\n",
      "cost:  0.1683071986255244\n",
      "opt took 0.00min,  691iters\n",
      "Epoch: [13][23/2459]Time: 0.350 (0.590) Data: 0.273 (0.547) Loss: 0.3073 (0.2816)\n",
      "10-NN,s=0.1: TOP1:  36.458333333333336\n",
      "Saving..\n",
      "best accuracy: 36.46\n",
      "\n",
      "Epoch: 14\n",
      "ResNet1D\n",
      "error:  0.009632543458180431 step  841\n",
      "cost:  0.15159144868361296\n",
      "opt took 0.00min,  841iters\n",
      "Epoch: [14][0/2459]Time: 0.399 (0.399) Data: 0.324 (0.324) Loss: 0.2312 (0.2312)\n",
      "error:  0.009751416057215856 step  651\n",
      "cost:  0.16009593488768775\n",
      "opt took 0.02min,  651iters\n",
      "Epoch: [14][1/2459]Time: 1.211 (0.805) Data: 1.184 (0.754) Loss: 0.2370 (0.2341)\n",
      "error:  0.009777427605144329 step  641\n",
      "cost:  0.16388368115226265\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [14][2/2459]Time: 0.439 (0.683) Data: 0.397 (0.635) Loss: 0.2656 (0.2446)\n",
      "error:  0.009769990355174119 step  761\n",
      "cost:  0.16597898112600548\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [14][3/2459]Time: 0.400 (0.612) Data: 0.339 (0.561) Loss: 0.2580 (0.2480)\n",
      "error:  0.009040668788908413 step  701\n",
      "cost:  0.16301991861273488\n",
      "opt took 0.01min,  701iters\n",
      "Epoch: [14][4/2459]Time: 0.657 (0.621) Data: 0.615 (0.572) Loss: 0.2673 (0.2518)\n",
      "error:  0.00942196405973339 step  701\n",
      "cost:  0.15881430995204474\n",
      "opt took 0.00min,  701iters\n",
      "Epoch: [14][5/2459]Time: 0.472 (0.596) Data: 0.436 (0.549) Loss: 0.2095 (0.2448)\n",
      "error:  0.00990077290502267 step  981\n",
      "cost:  0.15215112068191372\n",
      "opt took 0.00min,  981iters\n",
      "Epoch: [14][6/2459]Time: 0.455 (0.576) Data: 0.427 (0.532) Loss: 0.2296 (0.2426)\n",
      "error:  0.009019470934741647 step  401\n",
      "cost:  0.15474834879558758\n",
      "opt took 0.00min,  401iters\n",
      "Epoch: [14][7/2459]Time: 0.281 (0.539) Data: 0.226 (0.494) Loss: 0.2779 (0.2470)\n",
      "error:  0.009843340515204746 step  721\n",
      "cost:  0.1645004518551119\n",
      "opt took 0.00min,  721iters\n",
      "Epoch: [14][8/2459]Time: 0.309 (0.514) Data: 0.279 (0.470) Loss: 0.3204 (0.2552)\n",
      "error:  0.009240226533702933 step  841\n",
      "cost:  0.16669984161103715\n",
      "opt took 0.00min,  841iters\n",
      "Epoch: [14][9/2459]Time: 0.398 (0.502) Data: 0.317 (0.454) Loss: 0.2428 (0.2539)\n",
      "error:  0.009472917565918504 step  911\n",
      "cost:  0.16561662991764273\n",
      "opt took 0.00min,  911iters\n",
      "Epoch: [14][10/2459]Time: 0.305 (0.484) Data: 0.273 (0.438) Loss: 0.2735 (0.2557)\n",
      "error:  0.009785502837292737 step  1221\n",
      "cost:  0.16415844996519352\n",
      "opt took 0.00min, 1221iters\n",
      "Epoch: [14][11/2459]Time: 0.478 (0.484) Data: 0.449 (0.439) Loss: 0.2679 (0.2567)\n",
      "error:  0.009215998189159746 step  881\n",
      "cost:  0.1597352765436488\n",
      "opt took 0.00min,  881iters\n",
      "Epoch: [14][12/2459]Time: 0.405 (0.478) Data: 0.363 (0.433) Loss: 0.2669 (0.2575)\n",
      "error:  0.008986405151076737 step  671\n",
      "cost:  0.15503128010327227\n",
      "opt took 0.00min,  671iters\n",
      "Epoch: [14][13/2459]Time: 0.437 (0.475) Data: 0.374 (0.429) Loss: 0.2397 (0.2562)\n",
      "error:  0.00886742792296702 step  531\n",
      "cost:  0.15673966550244975\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [14][14/2459]Time: 0.349 (0.466) Data: 0.309 (0.421) Loss: 0.2512 (0.2559)\n",
      "error:  0.009356542175383176 step  611\n",
      "cost:  0.16079928472376426\n",
      "opt took 0.00min,  611iters\n",
      "Epoch: [14][15/2459]Time: 0.454 (0.466) Data: 0.412 (0.420) Loss: 0.2110 (0.2531)\n",
      "error:  0.00990644460277701 step  641\n",
      "cost:  0.16173608919679855\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [14][16/2459]Time: 0.437 (0.464) Data: 0.400 (0.419) Loss: 0.2561 (0.2533)\n",
      "error:  0.009771952421000707 step  821\n",
      "cost:  0.1643876962881403\n",
      "opt took 0.00min,  821iters\n",
      "Epoch: [14][17/2459]Time: 0.477 (0.465) Data: 0.444 (0.420) Loss: 0.2565 (0.2535)\n",
      "error:  0.009333488086354036 step  341\n",
      "cost:  0.16633078033784202\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [14][18/2459]Time: 0.431 (0.463) Data: 0.357 (0.417) Loss: 0.2471 (0.2531)\n",
      "error:  0.009615823041118077 step  851\n",
      "cost:  0.15805414758288722\n",
      "opt took 0.00min,  851iters\n",
      "Epoch: [14][19/2459]Time: 0.385 (0.459) Data: 0.358 (0.414) Loss: 0.2353 (0.2522)\n",
      "error:  0.009687456194795607 step  751\n",
      "cost:  0.16105365950969608\n",
      "opt took 0.00min,  751iters\n",
      "Epoch: [14][20/2459]Time: 0.294 (0.451) Data: 0.266 (0.407) Loss: 0.2280 (0.2511)\n",
      "error:  0.009597714885999253 step  1081\n",
      "cost:  0.16780374904301645\n",
      "opt took 0.00min, 1081iters\n",
      "Epoch: [14][21/2459]Time: 0.483 (0.453) Data: 0.454 (0.409) Loss: 0.2425 (0.2507)\n",
      "error:  0.00941381094825089 step  1281\n",
      "cost:  0.16306334828802738\n",
      "opt took 0.00min, 1281iters\n",
      "Epoch: [14][22/2459]Time: 0.450 (0.453) Data: 0.417 (0.410) Loss: 0.2347 (0.2500)\n",
      "error:  0.009643362470382 step  1521\n",
      "cost:  0.16279996630860427\n",
      "opt took 0.00min, 1521iters\n",
      "Epoch: [14][23/2459]Time: 0.510 (0.455) Data: 0.481 (0.413) Loss: 0.2616 (0.2505)\n",
      "10-NN,s=0.1: TOP1:  35.291666666666664\n",
      "best accuracy: 36.46\n",
      "\n",
      "Epoch: 15\n",
      "ResNet1D\n",
      "error:  0.009235429824022479 step  671\n",
      "cost:  0.17213713012600398\n",
      "opt took 0.00min,  671iters\n",
      "Epoch: [15][0/2459]Time: 0.301 (0.301) Data: 0.271 (0.271) Loss: 0.2303 (0.2303)\n",
      "error:  0.00988042964625635 step  1711\n",
      "cost:  0.17121947785235206\n",
      "opt took 0.00min, 1711iters\n",
      "Epoch: [15][1/2459]Time: 0.501 (0.401) Data: 0.459 (0.365) Loss: 0.2223 (0.2263)\n",
      "error:  0.00918116007631109 step  681\n",
      "cost:  0.1752300378630703\n",
      "opt took 0.00min,  681iters\n",
      "Epoch: [15][2/2459]Time: 0.373 (0.392) Data: 0.327 (0.352) Loss: 0.2174 (0.2234)\n",
      "error:  0.009690183830372456 step  1091\n",
      "cost:  0.17672040132850106\n",
      "opt took 0.01min, 1091iters\n",
      "Epoch: [15][3/2459]Time: 0.595 (0.443) Data: 0.566 (0.406) Loss: 0.2703 (0.2351)\n",
      "error:  0.008668128991369484 step  381\n",
      "cost:  0.17596452248855207\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [15][4/2459]Time: 0.245 (0.403) Data: 0.215 (0.367) Loss: 0.1911 (0.2263)\n",
      "error:  0.00932764899430627 step  331\n",
      "cost:  0.17367126424998586\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [15][5/2459]Time: 0.300 (0.386) Data: 0.224 (0.343) Loss: 0.2262 (0.2263)\n",
      "error:  0.00971277686735783 step  441\n",
      "cost:  0.18117852905569448\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [15][6/2459]Time: 0.241 (0.365) Data: 0.213 (0.325) Loss: 0.2326 (0.2272)\n",
      "error:  0.009082223362378739 step  321\n",
      "cost:  0.17558132295319046\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [15][7/2459]Time: 0.643 (0.400) Data: 0.613 (0.361) Loss: 0.2342 (0.2281)\n",
      "error:  0.009903193845751224 step  351\n",
      "cost:  0.16763657242057606\n",
      "opt took 0.00min,  351iters\n",
      "Epoch: [15][8/2459]Time: 0.385 (0.398) Data: 0.339 (0.358) Loss: 0.1824 (0.2230)\n",
      "error:  0.009524725500017817 step  741\n",
      "cost:  0.17249162237501378\n",
      "opt took 0.01min,  741iters\n",
      "Epoch: [15][9/2459]Time: 0.659 (0.424) Data: 0.611 (0.384) Loss: 0.2057 (0.2213)\n",
      "error:  0.009574132437762617 step  741\n",
      "cost:  0.17248851821164946\n",
      "opt took 0.00min,  741iters\n",
      "Epoch: [15][10/2459]Time: 0.443 (0.426) Data: 0.416 (0.387) Loss: 0.2039 (0.2197)\n",
      "error:  0.009558564397453284 step  621\n",
      "cost:  0.17286349329545483\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [15][11/2459]Time: 0.415 (0.425) Data: 0.349 (0.383) Loss: 0.2211 (0.2198)\n",
      "error:  0.009855019032006918 step  591\n",
      "cost:  0.1721252658793201\n",
      "opt took 0.00min,  591iters\n",
      "Epoch: [15][12/2459]Time: 0.412 (0.424) Data: 0.357 (0.381) Loss: 0.2294 (0.2205)\n",
      "error:  0.009998921763884283 step  1281\n",
      "cost:  0.17139236996364543\n",
      "opt took 0.00min, 1281iters\n",
      "Epoch: [15][13/2459]Time: 0.443 (0.426) Data: 0.415 (0.384) Loss: 0.2409 (0.2220)\n",
      "error:  0.00985676084306586 step  1591\n",
      "cost:  0.16846859056766197\n",
      "opt took 0.00min, 1591iters\n",
      "Epoch: [15][14/2459]Time: 0.399 (0.424) Data: 0.369 (0.383) Loss: 0.2130 (0.2214)\n",
      "error:  0.008533762292455171 step  421\n",
      "cost:  0.1673283091373517\n",
      "opt took 0.00min,  421iters\n",
      "Epoch: [15][15/2459]Time: 0.317 (0.417) Data: 0.252 (0.375) Loss: 0.2752 (0.2248)\n",
      "error:  0.009114361375101354 step  491\n",
      "cost:  0.17193261463710865\n",
      "opt took 0.01min,  491iters\n",
      "Epoch: [15][16/2459]Time: 0.743 (0.436) Data: 0.699 (0.394) Loss: 0.2427 (0.2258)\n",
      "error:  0.008285474632119372 step  311\n",
      "cost:  0.17776367256231107\n",
      "opt took 0.00min,  311iters\n",
      "Epoch: [15][17/2459]Time: 0.261 (0.427) Data: 0.233 (0.385) Loss: 0.2338 (0.2263)\n",
      "error:  0.008717245508741533 step  281\n",
      "cost:  0.18442177182542524\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [15][18/2459]Time: 0.363 (0.423) Data: 0.308 (0.381) Loss: 0.2598 (0.2280)\n",
      "error:  0.009273390250361335 step  501\n",
      "cost:  0.17968637520284422\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [15][19/2459]Time: 0.530 (0.429) Data: 0.500 (0.387) Loss: 0.2207 (0.2277)\n",
      "error:  0.009311935944414262 step  461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  0.17948380758799562\n",
      "opt took 0.00min,  461iters\n",
      "Epoch: [15][20/2459]Time: 0.361 (0.425) Data: 0.316 (0.383) Loss: 0.2548 (0.2290)\n",
      "error:  0.009028435432200443 step  301\n",
      "cost:  0.1809573282544537\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [15][21/2459]Time: 0.279 (0.419) Data: 0.237 (0.377) Loss: 0.2123 (0.2282)\n",
      "error:  0.00955421127465983 step  441\n",
      "cost:  0.17458664054305248\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [15][22/2459]Time: 0.521 (0.423) Data: 0.494 (0.382) Loss: 0.2041 (0.2271)\n",
      "error:  0.009580976012403042 step  881\n",
      "cost:  0.17701471658735826\n",
      "opt took 0.01min,  881iters\n",
      "Epoch: [15][23/2459]Time: 1.112 (0.452) Data: 1.064 (0.410) Loss: 0.2770 (0.2292)\n",
      "10-NN,s=0.1: TOP1:  35.708333333333336\n",
      "best accuracy: 36.46\n",
      "\n",
      "Epoch: 16\n",
      "ResNet1D\n",
      "error:  0.009843221756787757 step  641\n",
      "cost:  0.17910370238236087\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [16][0/2459]Time: 0.334 (0.334) Data: 0.294 (0.294) Loss: 0.1803 (0.1803)\n",
      "error:  0.009832465743447205 step  621\n",
      "cost:  0.18150319887782249\n",
      "opt took 0.00min,  621iters\n",
      "Epoch: [16][1/2459]Time: 0.431 (0.382) Data: 0.402 (0.348) Loss: 0.2019 (0.1911)\n",
      "error:  0.008913377591402871 step  631\n",
      "cost:  0.19045959154645972\n",
      "opt took 0.00min,  631iters\n",
      "Epoch: [16][2/2459]Time: 0.289 (0.351) Data: 0.261 (0.319) Loss: 0.2091 (0.1971)\n",
      "error:  0.009173629262798744 step  641\n",
      "cost:  0.19556613347882804\n",
      "opt took 0.01min,  641iters\n",
      "Epoch: [16][3/2459]Time: 0.599 (0.413) Data: 0.557 (0.379) Loss: 0.2026 (0.1985)\n",
      "error:  0.009164584263758102 step  521\n",
      "cost:  0.18659312349329277\n",
      "opt took 0.00min,  521iters\n",
      "Epoch: [16][4/2459]Time: 0.402 (0.411) Data: 0.372 (0.377) Loss: 0.2122 (0.2012)\n",
      "error:  0.009382551731584532 step  561\n",
      "cost:  0.17542332031775834\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [16][5/2459]Time: 0.258 (0.385) Data: 0.231 (0.353) Loss: 0.2447 (0.2085)\n",
      "error:  0.008941030985693876 step  301\n",
      "cost:  0.16830121716575214\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [16][6/2459]Time: 0.374 (0.384) Data: 0.331 (0.350) Loss: 0.2246 (0.2108)\n",
      "error:  0.009964120393683995 step  451\n",
      "cost:  0.1729805602386662\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [16][7/2459]Time: 0.323 (0.376) Data: 0.260 (0.339) Loss: 0.1731 (0.2061)\n",
      "error:  0.00969721949966662 step  1011\n",
      "cost:  0.17914809916912694\n",
      "opt took 0.00min, 1011iters\n",
      "Epoch: [16][8/2459]Time: 0.292 (0.367) Data: 0.264 (0.330) Loss: 0.1892 (0.2042)\n",
      "error:  0.009382793762781505 step  971\n",
      "cost:  0.1907689587417161\n",
      "opt took 0.01min,  971iters\n",
      "Epoch: [16][9/2459]Time: 0.598 (0.390) Data: 0.547 (0.352) Loss: 0.2140 (0.2052)\n",
      "error:  0.009908957914785521 step  1021\n",
      "cost:  0.18435939032773516\n",
      "opt took 0.01min, 1021iters\n",
      "Epoch: [16][10/2459]Time: 0.759 (0.423) Data: 0.680 (0.382) Loss: 0.1840 (0.2033)\n",
      "error:  0.009176354381184582 step  671\n",
      "cost:  0.17675651252503766\n",
      "opt took 0.00min,  671iters\n",
      "Epoch: [16][11/2459]Time: 0.507 (0.430) Data: 0.450 (0.387) Loss: 0.2151 (0.2042)\n",
      "error:  0.009851458511100475 step  971\n",
      "cost:  0.17400026609920444\n",
      "opt took 0.01min,  971iters\n",
      "Epoch: [16][12/2459]Time: 0.626 (0.445) Data: 0.584 (0.403) Loss: 0.2084 (0.2046)\n",
      "error:  0.009577550956160086 step  1101\n",
      "cost:  0.17062078784747628\n",
      "opt took 0.00min, 1101iters\n",
      "Epoch: [16][13/2459]Time: 0.514 (0.450) Data: 0.475 (0.408) Loss: 0.2474 (0.2076)\n",
      "error:  0.009850242351037597 step  951\n",
      "cost:  0.17562707185124465\n",
      "opt took 0.01min,  951iters\n",
      "Epoch: [16][14/2459]Time: 0.645 (0.463) Data: 0.607 (0.421) Loss: 0.2042 (0.2074)\n",
      "error:  0.009600807347667106 step  361\n",
      "cost:  0.17580009833139135\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [16][15/2459]Time: 0.422 (0.461) Data: 0.382 (0.419) Loss: 0.1964 (0.2067)\n",
      "error:  0.009843054286738484 step  791\n",
      "cost:  0.1759549746463295\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [16][16/2459]Time: 0.443 (0.460) Data: 0.415 (0.418) Loss: 0.1967 (0.2061)\n",
      "error:  0.009870977227208977 step  1291\n",
      "cost:  0.17740344836995123\n",
      "opt took 0.00min, 1291iters\n",
      "Epoch: [16][17/2459]Time: 0.399 (0.456) Data: 0.361 (0.415) Loss: 0.2165 (0.2067)\n",
      "error:  0.00971906045546711 step  1601\n",
      "cost:  0.17355214991833268\n",
      "opt took 0.02min, 1601iters\n",
      "Epoch: [16][18/2459]Time: 1.152 (0.493) Data: 1.113 (0.452) Loss: 0.2598 (0.2095)\n",
      "error:  0.00932335430088893 step  691\n",
      "cost:  0.17113229626200135\n",
      "opt took 0.00min,  691iters\n",
      "Epoch: [16][19/2459]Time: 0.550 (0.496) Data: 0.510 (0.455) Loss: 0.1958 (0.2088)\n",
      "error:  0.009760841210113336 step  1221\n",
      "cost:  0.176155837481289\n",
      "opt took 0.01min, 1221iters\n",
      "Epoch: [16][20/2459]Time: 0.708 (0.506) Data: 0.665 (0.465) Loss: 0.2097 (0.2089)\n",
      "error:  0.009588599085913252 step  691\n",
      "cost:  0.1779231462250251\n",
      "opt took 0.01min,  691iters\n",
      "Epoch: [16][21/2459]Time: 0.562 (0.508) Data: 0.524 (0.468) Loss: 0.2387 (0.2102)\n",
      "error:  0.009169163639344169 step  521\n",
      "cost:  0.17488413707929656\n",
      "opt took 0.01min,  521iters\n",
      "Epoch: [16][22/2459]Time: 0.558 (0.511) Data: 0.531 (0.470) Loss: 0.2165 (0.2105)\n",
      "error:  0.009558727140208334 step  531\n",
      "cost:  0.1737443661798084\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [16][23/2459]Time: 0.521 (0.511) Data: 0.458 (0.470) Loss: 0.2417 (0.2118)\n",
      "10-NN,s=0.1: TOP1:  35.958333333333336\n",
      "best accuracy: 36.46\n",
      "\n",
      "Epoch: 17\n",
      "ResNet1D\n",
      "error:  0.009417419885419442 step  811\n",
      "cost:  0.18302180791921321\n",
      "opt took 0.00min,  811iters\n",
      "Epoch: [17][0/2459]Time: 0.398 (0.398) Data: 0.367 (0.367) Loss: 0.1868 (0.1868)\n",
      "error:  0.00985092427341372 step  861\n",
      "cost:  0.17816591301822812\n",
      "opt took 0.00min,  861iters\n",
      "Epoch: [17][1/2459]Time: 0.387 (0.393) Data: 0.327 (0.347) Loss: 0.2312 (0.2090)\n",
      "error:  0.008946803370615264 step  321\n",
      "cost:  0.17396995589838682\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [17][2/2459]Time: 0.253 (0.346) Data: 0.224 (0.306) Loss: 0.1941 (0.2040)\n",
      "error:  0.009822327295867739 step  441\n",
      "cost:  0.17650274350407583\n",
      "opt took 0.00min,  441iters\n",
      "Epoch: [17][3/2459]Time: 0.273 (0.328) Data: 0.219 (0.284) Loss: 0.2098 (0.2055)\n",
      "error:  0.009686191271857325 step  271\n",
      "cost:  0.1792237450968463\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [17][4/2459]Time: 0.409 (0.344) Data: 0.360 (0.299) Loss: 0.2253 (0.2094)\n",
      "error:  0.009506262672467125 step  611\n",
      "cost:  0.18618511691785056\n",
      "opt took 0.00min,  611iters\n",
      "Epoch: [17][5/2459]Time: 0.543 (0.377) Data: 0.516 (0.336) Loss: 0.1743 (0.2036)\n",
      "error:  0.009394544135558736 step  341\n",
      "cost:  0.18267627489811816\n",
      "opt took 0.01min,  341iters\n",
      "Epoch: [17][6/2459]Time: 0.553 (0.402) Data: 0.514 (0.361) Loss: 0.1963 (0.2025)\n",
      "error:  0.009374940867784898 step  361\n",
      "cost:  0.17503305738090172\n",
      "opt took 0.00min,  361iters\n",
      "Epoch: [17][7/2459]Time: 0.387 (0.400) Data: 0.359 (0.361) Loss: 0.2226 (0.2050)\n",
      "error:  0.009818904814529295 step  721\n",
      "cost:  0.16950975896438342\n",
      "opt took 0.00min,  721iters\n",
      "Epoch: [17][8/2459]Time: 0.505 (0.412) Data: 0.460 (0.372) Loss: 0.1981 (0.2043)\n",
      "error:  0.009175605316498636 step  581\n",
      "cost:  0.17166749524921834\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [17][9/2459]Time: 0.462 (0.417) Data: 0.353 (0.370) Loss: 0.1957 (0.2034)\n",
      "error:  0.00912448441998015 step  551\n",
      "cost:  0.17461660359226872\n",
      "opt took 0.00min,  551iters\n",
      "Epoch: [17][10/2459]Time: 0.274 (0.404) Data: 0.245 (0.359) Loss: 0.1899 (0.2022)\n",
      "error:  0.009641669630014338 step  761\n",
      "cost:  0.173672118513446\n",
      "opt took 0.00min,  761iters\n",
      "Epoch: [17][11/2459]Time: 0.307 (0.396) Data: 0.278 (0.352) Loss: 0.2002 (0.2020)\n",
      "error:  0.009235263200325639 step  741\n",
      "cost:  0.16676071553630625\n",
      "opt took 0.00min,  741iters\n",
      "Epoch: [17][12/2459]Time: 0.462 (0.401) Data: 0.399 (0.355) Loss: 0.1854 (0.2007)\n",
      "Epoch: [17][13/2459]Time: 0.074 (0.378) Data: 0.002 (0.330) Loss: 0.1911 (0.2001)\n",
      "Epoch: [17][14/2459]Time: 0.036 (0.355) Data: 0.001 (0.308) Loss: 0.2006 (0.2001)\n",
      "Epoch: [17][15/2459]Time: 0.035 (0.335) Data: 0.001 (0.289) Loss: 0.2297 (0.2019)\n",
      "Epoch: [17][16/2459]Time: 0.029 (0.317) Data: 0.001 (0.272) Loss: 0.1838 (0.2009)\n",
      "Epoch: [17][17/2459]Time: 0.028 (0.301) Data: 0.001 (0.257) Loss: 0.1736 (0.1994)\n",
      "Epoch: [17][18/2459]Time: 0.030 (0.287) Data: 0.001 (0.244) Loss: 0.2090 (0.1999)\n",
      "Epoch: [17][19/2459]Time: 0.026 (0.274) Data: 0.001 (0.231) Loss: 0.1994 (0.1998)\n",
      "Epoch: [17][20/2459]Time: 0.026 (0.262) Data: 0.001 (0.221) Loss: 0.2115 (0.2004)\n",
      "Epoch: [17][21/2459]Time: 0.024 (0.251) Data: 0.001 (0.211) Loss: 0.1979 (0.2003)\n",
      "Epoch: [17][22/2459]Time: 0.044 (0.242) Data: 0.001 (0.201) Loss: 0.2330 (0.2017)\n",
      "Epoch: [17][23/2459]Time: 0.024 (0.233) Data: 0.001 (0.193) Loss: 0.2046 (0.2018)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-NN,s=0.1: TOP1:  35.833333333333336\n",
      "best accuracy: 36.46\n",
      "\n",
      "Epoch: 18\n",
      "ResNet1D\n",
      "Epoch: [18][0/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.1749 (0.1749)\n",
      "Epoch: [18][1/2459]Time: 0.044 (0.035) Data: 0.001 (0.001) Loss: 0.1864 (0.1807)\n",
      "Epoch: [18][2/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.1873 (0.1829)\n",
      "Epoch: [18][3/2459]Time: 0.068 (0.040) Data: 0.001 (0.001) Loss: 0.2352 (0.1960)\n",
      "Epoch: [18][4/2459]Time: 0.075 (0.047) Data: 0.001 (0.001) Loss: 0.1815 (0.1931)\n",
      "Epoch: [18][5/2459]Time: 0.026 (0.044) Data: 0.001 (0.001) Loss: 0.1815 (0.1911)\n",
      "Epoch: [18][6/2459]Time: 0.025 (0.041) Data: 0.001 (0.001) Loss: 0.1632 (0.1871)\n",
      "Epoch: [18][7/2459]Time: 0.025 (0.039) Data: 0.001 (0.001) Loss: 0.1493 (0.1824)\n",
      "Epoch: [18][8/2459]Time: 0.024 (0.037) Data: 0.001 (0.001) Loss: 0.1799 (0.1821)\n",
      "Epoch: [18][9/2459]Time: 0.023 (0.036) Data: 0.001 (0.001) Loss: 0.1897 (0.1829)\n",
      "Epoch: [18][10/2459]Time: 0.023 (0.035) Data: 0.001 (0.001) Loss: 0.1812 (0.1827)\n",
      "Epoch: [18][11/2459]Time: 0.023 (0.034) Data: 0.001 (0.001) Loss: 0.1375 (0.1790)\n",
      "Epoch: [18][12/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.1655 (0.1779)\n",
      "Epoch: [18][13/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.1434 (0.1755)\n",
      "Epoch: [18][14/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.1695 (0.1751)\n",
      "Epoch: [18][15/2459]Time: 0.025 (0.031) Data: 0.001 (0.001) Loss: 0.1779 (0.1752)\n",
      "Epoch: [18][16/2459]Time: 0.046 (0.032) Data: 0.001 (0.001) Loss: 0.1833 (0.1757)\n",
      "Epoch: [18][17/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.1497 (0.1743)\n",
      "Epoch: [18][18/2459]Time: 0.065 (0.033) Data: 0.001 (0.001) Loss: 0.1946 (0.1753)\n",
      "Epoch: [18][19/2459]Time: 0.074 (0.035) Data: 0.001 (0.001) Loss: 0.1708 (0.1751)\n",
      "Epoch: [18][20/2459]Time: 0.036 (0.035) Data: 0.002 (0.001) Loss: 0.1567 (0.1742)\n",
      "Epoch: [18][21/2459]Time: 0.035 (0.035) Data: 0.001 (0.001) Loss: 0.1793 (0.1745)\n",
      "Epoch: [18][22/2459]Time: 0.037 (0.036) Data: 0.001 (0.001) Loss: 0.1656 (0.1741)\n",
      "Epoch: [18][23/2459]Time: 0.035 (0.036) Data: 0.001 (0.001) Loss: 0.1361 (0.1725)\n",
      "10-NN,s=0.1: TOP1:  35.75\n",
      "best accuracy: 36.46\n",
      "\n",
      "Epoch: 19\n",
      "ResNet1D\n",
      "Epoch: [19][0/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.1066 (0.1066)\n",
      "Epoch: [19][1/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.1147 (0.1107)\n",
      "Epoch: [19][2/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.1254 (0.1156)\n",
      "Epoch: [19][3/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.1322 (0.1197)\n",
      "Epoch: [19][4/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.1152 (0.1188)\n",
      "Epoch: [19][5/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.1406 (0.1224)\n",
      "Epoch: [19][6/2459]Time: 0.053 (0.028) Data: 0.001 (0.001) Loss: 0.1354 (0.1243)\n",
      "Epoch: [19][7/2459]Time: 0.026 (0.028) Data: 0.001 (0.001) Loss: 0.1796 (0.1312)\n",
      "Epoch: [19][8/2459]Time: 0.035 (0.029) Data: 0.001 (0.001) Loss: 0.1132 (0.1292)\n",
      "Epoch: [19][9/2459]Time: 0.058 (0.032) Data: 0.001 (0.001) Loss: 0.1681 (0.1331)\n",
      "Epoch: [19][10/2459]Time: 0.068 (0.035) Data: 0.001 (0.001) Loss: 0.1334 (0.1331)\n",
      "Epoch: [19][11/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.1322 (0.1330)\n",
      "Epoch: [19][12/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.1545 (0.1347)\n",
      "Epoch: [19][13/2459]Time: 0.025 (0.033) Data: 0.001 (0.001) Loss: 0.1246 (0.1340)\n",
      "Epoch: [19][14/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.1566 (0.1355)\n",
      "Epoch: [19][15/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.1245 (0.1348)\n",
      "Epoch: [19][16/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.1284 (0.1344)\n",
      "Epoch: [19][17/2459]Time: 0.024 (0.031) Data: 0.001 (0.001) Loss: 0.1366 (0.1345)\n",
      "Epoch: [19][18/2459]Time: 0.024 (0.031) Data: 0.001 (0.001) Loss: 0.1551 (0.1356)\n",
      "Epoch: [19][19/2459]Time: 0.024 (0.030) Data: 0.001 (0.001) Loss: 0.1606 (0.1369)\n",
      "Epoch: [19][20/2459]Time: 0.049 (0.031) Data: 0.001 (0.001) Loss: 0.1395 (0.1370)\n",
      "Epoch: [19][21/2459]Time: 0.024 (0.031) Data: 0.001 (0.001) Loss: 0.1709 (0.1385)\n",
      "Epoch: [19][22/2459]Time: 0.069 (0.033) Data: 0.001 (0.001) Loss: 0.1523 (0.1391)\n",
      "Epoch: [19][23/2459]Time: 0.076 (0.034) Data: 0.001 (0.001) Loss: 0.1675 (0.1403)\n",
      "10-NN,s=0.1: TOP1:  35.458333333333336\n",
      "best accuracy: 36.46\n",
      "\n",
      "Epoch: 20\n",
      "ResNet1D\n",
      "Epoch: [20][0/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.1304 (0.1304)\n",
      "Epoch: [20][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.1077 (0.1190)\n",
      "Epoch: [20][2/2459]Time: 0.065 (0.037) Data: 0.001 (0.001) Loss: 0.1120 (0.1167)\n",
      "Epoch: [20][3/2459]Time: 0.075 (0.047) Data: 0.001 (0.001) Loss: 0.1144 (0.1161)\n",
      "Epoch: [20][4/2459]Time: 0.027 (0.043) Data: 0.001 (0.001) Loss: 0.1186 (0.1166)\n",
      "Epoch: [20][5/2459]Time: 0.026 (0.040) Data: 0.001 (0.001) Loss: 0.1148 (0.1163)\n",
      "Epoch: [20][6/2459]Time: 0.025 (0.038) Data: 0.001 (0.001) Loss: 0.1127 (0.1158)\n",
      "Epoch: [20][7/2459]Time: 0.023 (0.036) Data: 0.001 (0.001) Loss: 0.1112 (0.1152)\n",
      "Epoch: [20][8/2459]Time: 0.023 (0.035) Data: 0.001 (0.001) Loss: 0.1216 (0.1159)\n",
      "Epoch: [20][9/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.1063 (0.1150)\n",
      "Epoch: [20][10/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.1026 (0.1138)\n",
      "Epoch: [20][11/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.1350 (0.1156)\n",
      "Epoch: [20][12/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.1028 (0.1146)\n",
      "Epoch: [20][13/2459]Time: 0.031 (0.031) Data: 0.001 (0.001) Loss: 0.1055 (0.1140)\n",
      "Epoch: [20][14/2459]Time: 0.039 (0.032) Data: 0.001 (0.001) Loss: 0.1030 (0.1132)\n",
      "Epoch: [20][15/2459]Time: 0.026 (0.031) Data: 0.001 (0.001) Loss: 0.1144 (0.1133)\n",
      "Epoch: [20][16/2459]Time: 0.066 (0.033) Data: 0.001 (0.001) Loss: 0.1065 (0.1129)\n",
      "Epoch: [20][17/2459]Time: 0.087 (0.036) Data: 0.002 (0.001) Loss: 0.1077 (0.1126)\n",
      "Epoch: [20][18/2459]Time: 0.044 (0.037) Data: 0.002 (0.001) Loss: 0.0986 (0.1119)\n",
      "Epoch: [20][19/2459]Time: 0.047 (0.037) Data: 0.001 (0.001) Loss: 0.1287 (0.1127)\n",
      "Epoch: [20][20/2459]Time: 0.030 (0.037) Data: 0.001 (0.001) Loss: 0.1080 (0.1125)\n",
      "Epoch: [20][21/2459]Time: 0.030 (0.037) Data: 0.001 (0.001) Loss: 0.1100 (0.1124)\n",
      "Epoch: [20][22/2459]Time: 0.030 (0.036) Data: 0.001 (0.001) Loss: 0.1247 (0.1129)\n",
      "Epoch: [20][23/2459]Time: 0.030 (0.036) Data: 0.001 (0.001) Loss: 0.1031 (0.1125)\n",
      "10-NN,s=0.1: TOP1:  37.125\n",
      "Saving..\n",
      "best accuracy: 37.12\n",
      "\n",
      "Epoch: 21\n",
      "ResNet1D\n",
      "Epoch: [21][0/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0859 (0.0859)\n",
      "Epoch: [21][1/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0898 (0.0879)\n",
      "Epoch: [21][2/2459]Time: 0.026 (0.024) Data: 0.001 (0.001) Loss: 0.0800 (0.0852)\n",
      "Epoch: [21][3/2459]Time: 0.044 (0.029) Data: 0.001 (0.001) Loss: 0.1093 (0.0912)\n",
      "Epoch: [21][4/2459]Time: 0.024 (0.028) Data: 0.001 (0.001) Loss: 0.0994 (0.0929)\n",
      "Epoch: [21][5/2459]Time: 0.067 (0.035) Data: 0.001 (0.001) Loss: 0.0836 (0.0913)\n",
      "Epoch: [21][6/2459]Time: 0.075 (0.040) Data: 0.001 (0.001) Loss: 0.0752 (0.0890)\n",
      "Epoch: [21][7/2459]Time: 0.025 (0.038) Data: 0.001 (0.001) Loss: 0.0962 (0.0899)\n",
      "Epoch: [21][8/2459]Time: 0.024 (0.037) Data: 0.001 (0.001) Loss: 0.1070 (0.0918)\n",
      "Epoch: [21][9/2459]Time: 0.023 (0.035) Data: 0.001 (0.001) Loss: 0.0850 (0.0912)\n",
      "Epoch: [21][10/2459]Time: 0.023 (0.034) Data: 0.001 (0.001) Loss: 0.0928 (0.0913)\n",
      "Epoch: [21][11/2459]Time: 0.022 (0.033) Data: 0.001 (0.001) Loss: 0.0844 (0.0907)\n",
      "Epoch: [21][12/2459]Time: 0.022 (0.032) Data: 0.001 (0.001) Loss: 0.0961 (0.0911)\n",
      "Epoch: [21][13/2459]Time: 0.022 (0.032) Data: 0.001 (0.001) Loss: 0.0880 (0.0909)\n",
      "Epoch: [21][14/2459]Time: 0.022 (0.031) Data: 0.001 (0.001) Loss: 0.0905 (0.0909)\n",
      "Epoch: [21][15/2459]Time: 0.022 (0.031) Data: 0.001 (0.001) Loss: 0.0930 (0.0910)\n",
      "Epoch: [21][16/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.1064 (0.0919)\n",
      "Epoch: [21][17/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0898 (0.0918)\n",
      "Epoch: [21][18/2459]Time: 0.048 (0.031) Data: 0.001 (0.001) Loss: 0.1009 (0.0923)\n",
      "Epoch: [21][19/2459]Time: 0.025 (0.030) Data: 0.001 (0.001) Loss: 0.0981 (0.0926)\n",
      "Epoch: [21][20/2459]Time: 0.067 (0.032) Data: 0.001 (0.001) Loss: 0.0837 (0.0922)\n",
      "Epoch: [21][21/2459]Time: 0.075 (0.034) Data: 0.001 (0.001) Loss: 0.0892 (0.0920)\n",
      "Epoch: [21][22/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0930 (0.0921)\n",
      "Epoch: [21][23/2459]Time: 0.025 (0.033) Data: 0.001 (0.001) Loss: 0.1036 (0.0925)\n",
      "10-NN,s=0.1: TOP1:  35.666666666666664\n",
      "best accuracy: 37.12\n",
      "\n",
      "Epoch: 22\n",
      "ResNet1D\n",
      "Epoch: [22][0/2459]Time: 0.040 (0.040) Data: 0.001 (0.001) Loss: 0.0617 (0.0617)\n",
      "Epoch: [22][1/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0666 (0.0642)\n",
      "Epoch: [22][2/2459]Time: 0.023 (0.028) Data: 0.001 (0.001) Loss: 0.0701 (0.0661)\n",
      "Epoch: [22][3/2459]Time: 0.023 (0.027) Data: 0.001 (0.001) Loss: 0.0748 (0.0683)\n",
      "Epoch: [22][4/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0603 (0.0667)\n",
      "Epoch: [22][5/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0806 (0.0690)\n",
      "Epoch: [22][6/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0649 (0.0684)\n",
      "Epoch: [22][7/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0720 (0.0689)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][8/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0883 (0.0710)\n",
      "Epoch: [22][9/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.1212 (0.0760)\n",
      "Epoch: [22][10/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0756 (0.0760)\n",
      "Epoch: [22][11/2459]Time: 0.046 (0.026) Data: 0.001 (0.001) Loss: 0.0887 (0.0771)\n",
      "Epoch: [22][12/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0823 (0.0775)\n",
      "Epoch: [22][13/2459]Time: 0.065 (0.029) Data: 0.001 (0.001) Loss: 0.0833 (0.0779)\n",
      "Epoch: [22][14/2459]Time: 0.076 (0.032) Data: 0.001 (0.001) Loss: 0.0810 (0.0781)\n",
      "Epoch: [22][15/2459]Time: 0.024 (0.031) Data: 0.001 (0.001) Loss: 0.0742 (0.0778)\n",
      "Epoch: [22][16/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0739 (0.0776)\n",
      "Epoch: [22][17/2459]Time: 0.022 (0.030) Data: 0.001 (0.001) Loss: 0.0764 (0.0775)\n",
      "Epoch: [22][18/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0857 (0.0780)\n",
      "Epoch: [22][19/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0971 (0.0789)\n",
      "Epoch: [22][20/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0847 (0.0792)\n",
      "Epoch: [22][21/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0936 (0.0799)\n",
      "Epoch: [22][22/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0934 (0.0804)\n",
      "Epoch: [22][23/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0814 (0.0805)\n",
      "10-NN,s=0.1: TOP1:  36.333333333333336\n",
      "best accuracy: 37.12\n",
      "\n",
      "Epoch: 23\n",
      "ResNet1D\n",
      "Epoch: [23][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0630 (0.0630)\n",
      "Epoch: [23][1/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0886 (0.0758)\n",
      "Epoch: [23][2/2459]Time: 0.024 (0.023) Data: 0.001 (0.001) Loss: 0.0740 (0.0752)\n",
      "Epoch: [23][3/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0807 (0.0766)\n",
      "Epoch: [23][4/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0670 (0.0747)\n",
      "Epoch: [23][5/2459]Time: 0.025 (0.023) Data: 0.001 (0.001) Loss: 0.0611 (0.0724)\n",
      "Epoch: [23][6/2459]Time: 0.043 (0.026) Data: 0.001 (0.001) Loss: 0.0602 (0.0706)\n",
      "Epoch: [23][7/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0683 (0.0704)\n",
      "Epoch: [23][8/2459]Time: 0.067 (0.031) Data: 0.001 (0.001) Loss: 0.0709 (0.0704)\n",
      "Epoch: [23][9/2459]Time: 0.075 (0.035) Data: 0.001 (0.001) Loss: 0.0758 (0.0710)\n",
      "Epoch: [23][10/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.0761 (0.0714)\n",
      "Epoch: [23][11/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0603 (0.0705)\n",
      "Epoch: [23][12/2459]Time: 0.025 (0.033) Data: 0.001 (0.001) Loss: 0.0861 (0.0717)\n",
      "Epoch: [23][13/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.0801 (0.0723)\n",
      "Epoch: [23][14/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.0645 (0.0718)\n",
      "Epoch: [23][15/2459]Time: 0.025 (0.031) Data: 0.001 (0.001) Loss: 0.0648 (0.0713)\n",
      "Epoch: [23][16/2459]Time: 0.024 (0.031) Data: 0.001 (0.001) Loss: 0.0536 (0.0703)\n",
      "Epoch: [23][17/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0602 (0.0697)\n",
      "Epoch: [23][18/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0777 (0.0701)\n",
      "Epoch: [23][19/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0747 (0.0704)\n",
      "Epoch: [23][20/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0743 (0.0706)\n",
      "Epoch: [23][21/2459]Time: 0.040 (0.030) Data: 0.001 (0.001) Loss: 0.0673 (0.0704)\n",
      "Epoch: [23][22/2459]Time: 0.026 (0.030) Data: 0.001 (0.001) Loss: 0.0746 (0.0706)\n",
      "Epoch: [23][23/2459]Time: 0.064 (0.031) Data: 0.001 (0.001) Loss: 0.0661 (0.0704)\n",
      "10-NN,s=0.1: TOP1:  36.416666666666664\n",
      "best accuracy: 37.12\n",
      "\n",
      "Epoch: 24\n",
      "ResNet1D\n",
      "Epoch: [24][0/2459]Time: 0.061 (0.061) Data: 0.001 (0.001) Loss: 0.0559 (0.0559)\n",
      "Epoch: [24][1/2459]Time: 0.074 (0.068) Data: 0.001 (0.001) Loss: 0.0698 (0.0628)\n",
      "Epoch: [24][2/2459]Time: 0.025 (0.054) Data: 0.001 (0.001) Loss: 0.0534 (0.0597)\n",
      "Epoch: [24][3/2459]Time: 0.024 (0.046) Data: 0.001 (0.001) Loss: 0.0657 (0.0612)\n",
      "Epoch: [24][4/2459]Time: 0.023 (0.041) Data: 0.001 (0.001) Loss: 0.0529 (0.0595)\n",
      "Epoch: [24][5/2459]Time: 0.023 (0.038) Data: 0.001 (0.001) Loss: 0.0514 (0.0582)\n",
      "Epoch: [24][6/2459]Time: 0.022 (0.036) Data: 0.001 (0.001) Loss: 0.0559 (0.0579)\n",
      "Epoch: [24][7/2459]Time: 0.022 (0.034) Data: 0.001 (0.001) Loss: 0.0610 (0.0583)\n",
      "Epoch: [24][8/2459]Time: 0.022 (0.033) Data: 0.001 (0.001) Loss: 0.0577 (0.0582)\n",
      "Epoch: [24][9/2459]Time: 0.022 (0.032) Data: 0.001 (0.001) Loss: 0.0632 (0.0587)\n",
      "Epoch: [24][10/2459]Time: 0.022 (0.031) Data: 0.001 (0.001) Loss: 0.0726 (0.0600)\n",
      "Epoch: [24][11/2459]Time: 0.022 (0.030) Data: 0.001 (0.001) Loss: 0.0533 (0.0594)\n",
      "Epoch: [24][12/2459]Time: 0.027 (0.030) Data: 0.001 (0.001) Loss: 0.0605 (0.0595)\n",
      "Epoch: [24][13/2459]Time: 0.042 (0.031) Data: 0.001 (0.001) Loss: 0.0693 (0.0602)\n",
      "Epoch: [24][14/2459]Time: 0.025 (0.031) Data: 0.001 (0.001) Loss: 0.0566 (0.0599)\n",
      "Epoch: [24][15/2459]Time: 0.065 (0.033) Data: 0.001 (0.001) Loss: 0.0611 (0.0600)\n",
      "Epoch: [24][16/2459]Time: 0.075 (0.035) Data: 0.001 (0.001) Loss: 0.0906 (0.0618)\n",
      "Epoch: [24][17/2459]Time: 0.026 (0.035) Data: 0.001 (0.001) Loss: 0.0646 (0.0620)\n",
      "Epoch: [24][18/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0533 (0.0615)\n",
      "Epoch: [24][19/2459]Time: 0.025 (0.034) Data: 0.001 (0.001) Loss: 0.0737 (0.0621)\n",
      "Epoch: [24][20/2459]Time: 0.024 (0.033) Data: 0.001 (0.001) Loss: 0.0629 (0.0622)\n",
      "Epoch: [24][21/2459]Time: 0.024 (0.033) Data: 0.001 (0.001) Loss: 0.0655 (0.0623)\n",
      "Epoch: [24][22/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.0721 (0.0627)\n",
      "Epoch: [24][23/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.0524 (0.0623)\n",
      "10-NN,s=0.1: TOP1:  37.125\n",
      "best accuracy: 37.12\n",
      "\n",
      "Epoch: 25\n",
      "ResNet1D\n",
      "Epoch: [25][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0540 (0.0540)\n",
      "Epoch: [25][1/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0501 (0.0520)\n",
      "Epoch: [25][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0574 (0.0538)\n",
      "Epoch: [25][3/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0557 (0.0543)\n",
      "Epoch: [25][4/2459]Time: 0.027 (0.024) Data: 0.001 (0.001) Loss: 0.0680 (0.0570)\n",
      "Epoch: [25][5/2459]Time: 0.041 (0.026) Data: 0.001 (0.001) Loss: 0.0600 (0.0575)\n",
      "Epoch: [25][6/2459]Time: 0.022 (0.026) Data: 0.001 (0.001) Loss: 0.0475 (0.0561)\n",
      "Epoch: [25][7/2459]Time: 0.069 (0.031) Data: 0.001 (0.001) Loss: 0.0562 (0.0561)\n",
      "Epoch: [25][8/2459]Time: 0.075 (0.036) Data: 0.001 (0.001) Loss: 0.0596 (0.0565)\n",
      "Epoch: [25][9/2459]Time: 0.026 (0.035) Data: 0.001 (0.001) Loss: 0.0558 (0.0564)\n",
      "Epoch: [25][10/2459]Time: 0.025 (0.034) Data: 0.001 (0.001) Loss: 0.0408 (0.0550)\n",
      "Epoch: [25][11/2459]Time: 0.025 (0.033) Data: 0.001 (0.001) Loss: 0.0516 (0.0547)\n",
      "Epoch: [25][12/2459]Time: 0.024 (0.033) Data: 0.001 (0.001) Loss: 0.0586 (0.0550)\n",
      "Epoch: [25][13/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0637 (0.0556)\n",
      "Epoch: [25][14/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0519 (0.0554)\n",
      "Epoch: [25][15/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0688 (0.0562)\n",
      "Epoch: [25][16/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0466 (0.0556)\n",
      "Epoch: [25][17/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0582 (0.0558)\n",
      "Epoch: [25][18/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0569 (0.0558)\n",
      "Epoch: [25][19/2459]Time: 0.046 (0.031) Data: 0.001 (0.001) Loss: 0.0502 (0.0556)\n",
      "Epoch: [25][20/2459]Time: 0.025 (0.030) Data: 0.001 (0.001) Loss: 0.0512 (0.0554)\n",
      "Epoch: [25][21/2459]Time: 0.067 (0.032) Data: 0.001 (0.001) Loss: 0.0745 (0.0562)\n",
      "Epoch: [25][22/2459]Time: 0.075 (0.034) Data: 0.001 (0.001) Loss: 0.0483 (0.0559)\n",
      "Epoch: [25][23/2459]Time: 0.025 (0.033) Data: 0.001 (0.001) Loss: 0.0817 (0.0570)\n",
      "10-NN,s=0.1: TOP1:  36.333333333333336\n",
      "best accuracy: 37.12\n",
      "\n",
      "Epoch: 26\n",
      "ResNet1D\n",
      "Epoch: [26][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0682 (0.0682)\n",
      "Epoch: [26][1/2459]Time: 0.030 (0.027) Data: 0.001 (0.001) Loss: 0.0469 (0.0576)\n",
      "Epoch: [26][2/2459]Time: 0.041 (0.031) Data: 0.001 (0.001) Loss: 0.0591 (0.0581)\n",
      "Epoch: [26][3/2459]Time: 0.022 (0.029) Data: 0.001 (0.001) Loss: 0.0553 (0.0574)\n",
      "Epoch: [26][4/2459]Time: 0.066 (0.037) Data: 0.001 (0.001) Loss: 0.0727 (0.0604)\n",
      "Epoch: [26][5/2459]Time: 0.075 (0.043) Data: 0.001 (0.001) Loss: 0.0502 (0.0587)\n",
      "Epoch: [26][6/2459]Time: 0.026 (0.041) Data: 0.001 (0.001) Loss: 0.0567 (0.0584)\n",
      "Epoch: [26][7/2459]Time: 0.027 (0.039) Data: 0.001 (0.001) Loss: 0.0510 (0.0575)\n",
      "Epoch: [26][8/2459]Time: 0.029 (0.038) Data: 0.002 (0.001) Loss: 0.0579 (0.0576)\n",
      "Epoch: [26][9/2459]Time: 0.030 (0.037) Data: 0.001 (0.001) Loss: 0.0536 (0.0572)\n",
      "Epoch: [26][10/2459]Time: 0.032 (0.037) Data: 0.002 (0.001) Loss: 0.0487 (0.0564)\n",
      "Epoch: [26][11/2459]Time: 0.028 (0.036) Data: 0.001 (0.001) Loss: 0.0540 (0.0562)\n",
      "Epoch: [26][12/2459]Time: 0.024 (0.035) Data: 0.001 (0.001) Loss: 0.0523 (0.0559)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][13/2459]Time: 0.028 (0.034) Data: 0.001 (0.001) Loss: 0.0448 (0.0551)\n",
      "Epoch: [26][14/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.0454 (0.0545)\n",
      "Epoch: [26][15/2459]Time: 0.048 (0.035) Data: 0.001 (0.001) Loss: 0.0410 (0.0536)\n",
      "Epoch: [26][16/2459]Time: 0.028 (0.034) Data: 0.001 (0.001) Loss: 0.0590 (0.0539)\n",
      "Epoch: [26][17/2459]Time: 0.063 (0.036) Data: 0.001 (0.001) Loss: 0.0449 (0.0534)\n",
      "Epoch: [26][18/2459]Time: 0.075 (0.038) Data: 0.001 (0.001) Loss: 0.0484 (0.0532)\n",
      "Epoch: [26][19/2459]Time: 0.033 (0.038) Data: 0.001 (0.001) Loss: 0.0437 (0.0527)\n",
      "Epoch: [26][20/2459]Time: 0.027 (0.037) Data: 0.001 (0.001) Loss: 0.0430 (0.0522)\n",
      "Epoch: [26][21/2459]Time: 0.027 (0.037) Data: 0.001 (0.001) Loss: 0.0496 (0.0521)\n",
      "Epoch: [26][22/2459]Time: 0.026 (0.036) Data: 0.001 (0.001) Loss: 0.0547 (0.0522)\n",
      "Epoch: [26][23/2459]Time: 0.024 (0.036) Data: 0.001 (0.001) Loss: 0.0522 (0.0522)\n",
      "10-NN,s=0.1: TOP1:  35.416666666666664\n",
      "best accuracy: 37.12\n",
      "\n",
      "Epoch: 27\n",
      "ResNet1D\n",
      "Epoch: [27][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0565 (0.0565)\n",
      "Epoch: [27][1/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0403 (0.0484)\n",
      "Epoch: [27][2/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0580 (0.0516)\n",
      "Epoch: [27][3/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0455 (0.0501)\n",
      "Epoch: [27][4/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0428 (0.0486)\n",
      "Epoch: [27][5/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0445 (0.0479)\n",
      "Epoch: [27][6/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0417 (0.0470)\n",
      "Epoch: [27][7/2459]Time: 0.023 (0.022) Data: 0.001 (0.001) Loss: 0.0495 (0.0473)\n",
      "Epoch: [27][8/2459]Time: 0.024 (0.023) Data: 0.001 (0.001) Loss: 0.0461 (0.0472)\n",
      "Epoch: [27][9/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0577 (0.0482)\n",
      "Epoch: [27][10/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0407 (0.0476)\n",
      "Epoch: [27][11/2459]Time: 0.033 (0.023) Data: 0.001 (0.001) Loss: 0.0371 (0.0467)\n",
      "Epoch: [27][12/2459]Time: 0.034 (0.024) Data: 0.001 (0.001) Loss: 0.0524 (0.0471)\n",
      "Epoch: [27][13/2459]Time: 0.023 (0.024) Data: 0.002 (0.001) Loss: 0.0399 (0.0466)\n",
      "Epoch: [27][14/2459]Time: 0.067 (0.027) Data: 0.001 (0.001) Loss: 0.0442 (0.0465)\n",
      "Epoch: [27][15/2459]Time: 0.076 (0.030) Data: 0.001 (0.001) Loss: 0.0445 (0.0463)\n",
      "Epoch: [27][16/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0496 (0.0465)\n",
      "Epoch: [27][17/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0588 (0.0472)\n",
      "Epoch: [27][18/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0483 (0.0473)\n",
      "Epoch: [27][19/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0600 (0.0479)\n",
      "Epoch: [27][20/2459]Time: 0.025 (0.029) Data: 0.001 (0.001) Loss: 0.0375 (0.0474)\n",
      "Epoch: [27][21/2459]Time: 0.032 (0.029) Data: 0.001 (0.001) Loss: 0.0448 (0.0473)\n",
      "Epoch: [27][22/2459]Time: 0.033 (0.029) Data: 0.001 (0.001) Loss: 0.0542 (0.0476)\n",
      "Epoch: [27][23/2459]Time: 0.031 (0.029) Data: 0.001 (0.001) Loss: 0.0459 (0.0475)\n",
      "10-NN,s=0.1: TOP1:  37.166666666666664\n",
      "Saving..\n",
      "best accuracy: 37.17\n",
      "\n",
      "Epoch: 28\n",
      "ResNet1D\n",
      "Epoch: [28][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0393 (0.0393)\n",
      "Epoch: [28][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0446 (0.0420)\n",
      "Epoch: [28][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0562 (0.0467)\n",
      "Epoch: [28][3/2459]Time: 0.025 (0.024) Data: 0.001 (0.001) Loss: 0.0361 (0.0441)\n",
      "Epoch: [28][4/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0329 (0.0418)\n",
      "Epoch: [28][5/2459]Time: 0.027 (0.024) Data: 0.001 (0.001) Loss: 0.0368 (0.0410)\n",
      "Epoch: [28][6/2459]Time: 0.025 (0.024) Data: 0.001 (0.001) Loss: 0.0465 (0.0418)\n",
      "Epoch: [28][7/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0454 (0.0422)\n",
      "Epoch: [28][8/2459]Time: 0.051 (0.027) Data: 0.001 (0.001) Loss: 0.0388 (0.0418)\n",
      "Epoch: [28][9/2459]Time: 0.026 (0.027) Data: 0.001 (0.001) Loss: 0.0392 (0.0416)\n",
      "Epoch: [28][10/2459]Time: 0.068 (0.031) Data: 0.001 (0.001) Loss: 0.0440 (0.0418)\n",
      "Epoch: [28][11/2459]Time: 0.076 (0.034) Data: 0.001 (0.001) Loss: 0.0422 (0.0418)\n",
      "Epoch: [28][12/2459]Time: 0.023 (0.034) Data: 0.001 (0.001) Loss: 0.0382 (0.0416)\n",
      "Epoch: [28][13/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0430 (0.0417)\n",
      "Epoch: [28][14/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.0482 (0.0421)\n",
      "Epoch: [28][15/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.0398 (0.0419)\n",
      "Epoch: [28][16/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0356 (0.0416)\n",
      "Epoch: [28][17/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0336 (0.0411)\n",
      "Epoch: [28][18/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0349 (0.0408)\n",
      "Epoch: [28][19/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0477 (0.0411)\n",
      "Epoch: [28][20/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0460 (0.0414)\n",
      "Epoch: [28][21/2459]Time: 0.025 (0.029) Data: 0.001 (0.001) Loss: 0.0426 (0.0414)\n",
      "Epoch: [28][22/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0519 (0.0419)\n",
      "Epoch: [28][23/2459]Time: 0.046 (0.030) Data: 0.001 (0.001) Loss: 0.0371 (0.0417)\n",
      "10-NN,s=0.1: TOP1:  36.708333333333336\n",
      "best accuracy: 37.17\n",
      "\n",
      "Epoch: 29\n",
      "ResNet1D\n",
      "Epoch: [29][0/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0415 (0.0415)\n",
      "Epoch: [29][1/2459]Time: 0.031 (0.028) Data: 0.001 (0.001) Loss: 0.0357 (0.0386)\n",
      "Epoch: [29][2/2459]Time: 0.036 (0.031) Data: 0.001 (0.001) Loss: 0.0333 (0.0368)\n",
      "Epoch: [29][3/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0418 (0.0381)\n",
      "Epoch: [29][4/2459]Time: 0.067 (0.037) Data: 0.001 (0.001) Loss: 0.0393 (0.0383)\n",
      "Epoch: [29][5/2459]Time: 0.075 (0.043) Data: 0.001 (0.001) Loss: 0.0299 (0.0369)\n",
      "Epoch: [29][6/2459]Time: 0.027 (0.041) Data: 0.001 (0.001) Loss: 0.0326 (0.0363)\n",
      "Epoch: [29][7/2459]Time: 0.025 (0.039) Data: 0.001 (0.001) Loss: 0.0324 (0.0358)\n",
      "Epoch: [29][8/2459]Time: 0.025 (0.037) Data: 0.001 (0.001) Loss: 0.0323 (0.0354)\n",
      "Epoch: [29][9/2459]Time: 0.025 (0.036) Data: 0.001 (0.001) Loss: 0.0404 (0.0359)\n",
      "Epoch: [29][10/2459]Time: 0.024 (0.035) Data: 0.001 (0.001) Loss: 0.0294 (0.0353)\n",
      "Epoch: [29][11/2459]Time: 0.034 (0.035) Data: 0.001 (0.001) Loss: 0.0312 (0.0350)\n",
      "Epoch: [29][12/2459]Time: 0.031 (0.035) Data: 0.001 (0.001) Loss: 0.0521 (0.0363)\n",
      "Epoch: [29][13/2459]Time: 0.029 (0.034) Data: 0.001 (0.001) Loss: 0.0358 (0.0363)\n",
      "Epoch: [29][14/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.0365 (0.0363)\n",
      "Epoch: [29][15/2459]Time: 0.051 (0.035) Data: 0.001 (0.001) Loss: 0.0434 (0.0367)\n",
      "Epoch: [29][16/2459]Time: 0.028 (0.034) Data: 0.001 (0.001) Loss: 0.0515 (0.0376)\n",
      "Epoch: [29][17/2459]Time: 0.067 (0.036) Data: 0.001 (0.001) Loss: 0.0351 (0.0374)\n",
      "Epoch: [29][18/2459]Time: 0.075 (0.038) Data: 0.001 (0.001) Loss: 0.0466 (0.0379)\n",
      "Epoch: [29][19/2459]Time: 0.027 (0.038) Data: 0.001 (0.001) Loss: 0.0405 (0.0381)\n",
      "Epoch: [29][20/2459]Time: 0.026 (0.037) Data: 0.001 (0.001) Loss: 0.0432 (0.0383)\n",
      "Epoch: [29][21/2459]Time: 0.026 (0.037) Data: 0.001 (0.001) Loss: 0.0437 (0.0385)\n",
      "Epoch: [29][22/2459]Time: 0.025 (0.036) Data: 0.001 (0.001) Loss: 0.0380 (0.0385)\n",
      "Epoch: [29][23/2459]Time: 0.024 (0.036) Data: 0.001 (0.001) Loss: 0.0371 (0.0385)\n",
      "10-NN,s=0.1: TOP1:  35.75\n",
      "best accuracy: 37.17\n",
      "\n",
      "Epoch: 30\n",
      "ResNet1D\n",
      "Epoch: [30][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0341 (0.0341)\n",
      "Epoch: [30][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0269 (0.0305)\n",
      "Epoch: [30][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0315 (0.0309)\n",
      "Epoch: [30][3/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0582 (0.0377)\n",
      "Epoch: [30][4/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0380 (0.0378)\n",
      "Epoch: [30][5/2459]Time: 0.035 (0.025) Data: 0.001 (0.001) Loss: 0.0345 (0.0372)\n",
      "Epoch: [30][6/2459]Time: 0.036 (0.026) Data: 0.001 (0.001) Loss: 0.0525 (0.0394)\n",
      "Epoch: [30][7/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0370 (0.0391)\n",
      "Epoch: [30][8/2459]Time: 0.066 (0.031) Data: 0.001 (0.001) Loss: 0.0434 (0.0396)\n",
      "Epoch: [30][9/2459]Time: 0.075 (0.035) Data: 0.001 (0.001) Loss: 0.0369 (0.0393)\n",
      "Epoch: [30][10/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.0454 (0.0399)\n",
      "Epoch: [30][11/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.0325 (0.0392)\n",
      "Epoch: [30][12/2459]Time: 0.034 (0.034) Data: 0.002 (0.001) Loss: 0.0420 (0.0395)\n",
      "Epoch: [30][13/2459]Time: 0.029 (0.033) Data: 0.001 (0.001) Loss: 0.0357 (0.0392)\n",
      "Epoch: [30][14/2459]Time: 0.026 (0.033) Data: 0.001 (0.001) Loss: 0.0334 (0.0388)\n",
      "Epoch: [30][15/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.0389 (0.0388)\n",
      "Epoch: [30][16/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0318 (0.0384)\n",
      "Epoch: [30][17/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0364 (0.0383)\n",
      "Epoch: [30][18/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0301 (0.0378)\n",
      "Epoch: [30][19/2459]Time: 0.027 (0.031) Data: 0.001 (0.001) Loss: 0.0300 (0.0374)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][20/2459]Time: 0.045 (0.031) Data: 0.001 (0.001) Loss: 0.0454 (0.0378)\n",
      "Epoch: [30][21/2459]Time: 0.026 (0.031) Data: 0.001 (0.001) Loss: 0.0397 (0.0379)\n",
      "Epoch: [30][22/2459]Time: 0.064 (0.033) Data: 0.001 (0.001) Loss: 0.0330 (0.0377)\n",
      "Epoch: [30][23/2459]Time: 0.074 (0.034) Data: 0.001 (0.001) Loss: 0.0445 (0.0380)\n",
      "10-NN,s=0.1: TOP1:  36.833333333333336\n",
      "best accuracy: 37.17\n",
      "\n",
      "Epoch: 31\n",
      "ResNet1D\n",
      "Epoch: [31][0/2459]Time: 0.042 (0.042) Data: 0.001 (0.001) Loss: 0.0358 (0.0358)\n",
      "Epoch: [31][1/2459]Time: 0.065 (0.054) Data: 0.001 (0.001) Loss: 0.0294 (0.0326)\n",
      "Epoch: [31][2/2459]Time: 0.026 (0.044) Data: 0.001 (0.001) Loss: 0.0435 (0.0362)\n",
      "Epoch: [31][3/2459]Time: 0.026 (0.040) Data: 0.001 (0.001) Loss: 0.0265 (0.0338)\n",
      "Epoch: [31][4/2459]Time: 0.026 (0.037) Data: 0.001 (0.001) Loss: 0.0344 (0.0339)\n",
      "Epoch: [31][5/2459]Time: 0.025 (0.035) Data: 0.001 (0.001) Loss: 0.0293 (0.0332)\n",
      "Epoch: [31][6/2459]Time: 0.025 (0.034) Data: 0.001 (0.001) Loss: 0.0508 (0.0357)\n",
      "Epoch: [31][7/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0309 (0.0351)\n",
      "Epoch: [31][8/2459]Time: 0.024 (0.031) Data: 0.001 (0.001) Loss: 0.0283 (0.0343)\n",
      "Epoch: [31][9/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0338 (0.0343)\n",
      "Epoch: [31][10/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0378 (0.0346)\n",
      "Epoch: [31][11/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0338 (0.0345)\n",
      "Epoch: [31][12/2459]Time: 0.062 (0.032) Data: 0.001 (0.001) Loss: 0.0279 (0.0340)\n",
      "Epoch: [31][13/2459]Time: 0.056 (0.034) Data: 0.003 (0.001) Loss: 0.0321 (0.0339)\n",
      "Epoch: [31][14/2459]Time: 0.116 (0.040) Data: 0.002 (0.001) Loss: 0.0270 (0.0334)\n",
      "Epoch: [31][15/2459]Time: 0.028 (0.039) Data: 0.001 (0.001) Loss: 0.0330 (0.0334)\n",
      "Epoch: [31][16/2459]Time: 0.029 (0.038) Data: 0.001 (0.001) Loss: 0.0369 (0.0336)\n",
      "Epoch: [31][17/2459]Time: 0.029 (0.038) Data: 0.001 (0.001) Loss: 0.0436 (0.0342)\n",
      "Epoch: [31][18/2459]Time: 0.028 (0.037) Data: 0.001 (0.001) Loss: 0.0526 (0.0351)\n",
      "Epoch: [31][19/2459]Time: 0.029 (0.037) Data: 0.001 (0.001) Loss: 0.0326 (0.0350)\n",
      "Epoch: [31][20/2459]Time: 0.028 (0.036) Data: 0.001 (0.001) Loss: 0.0350 (0.0350)\n",
      "Epoch: [31][21/2459]Time: 0.027 (0.036) Data: 0.001 (0.001) Loss: 0.0386 (0.0352)\n",
      "Epoch: [31][22/2459]Time: 0.022 (0.035) Data: 0.001 (0.001) Loss: 0.0339 (0.0351)\n",
      "Epoch: [31][23/2459]Time: 0.024 (0.035) Data: 0.001 (0.001) Loss: 0.0341 (0.0351)\n",
      "10-NN,s=0.1: TOP1:  37.208333333333336\n",
      "Saving..\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 32\n",
      "ResNet1D\n",
      "Epoch: [32][0/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0286 (0.0286)\n",
      "Epoch: [32][1/2459]Time: 0.023 (0.022) Data: 0.001 (0.001) Loss: 0.0394 (0.0340)\n",
      "Epoch: [32][2/2459]Time: 0.043 (0.029) Data: 0.001 (0.001) Loss: 0.0306 (0.0329)\n",
      "Epoch: [32][3/2459]Time: 0.030 (0.029) Data: 0.001 (0.001) Loss: 0.0295 (0.0320)\n",
      "Epoch: [32][4/2459]Time: 0.060 (0.036) Data: 0.001 (0.001) Loss: 0.0263 (0.0309)\n",
      "Epoch: [32][5/2459]Time: 0.070 (0.041) Data: 0.001 (0.001) Loss: 0.0341 (0.0314)\n",
      "Epoch: [32][6/2459]Time: 0.041 (0.041) Data: 0.001 (0.001) Loss: 0.0333 (0.0317)\n",
      "Epoch: [32][7/2459]Time: 0.031 (0.040) Data: 0.001 (0.001) Loss: 0.0312 (0.0316)\n",
      "Epoch: [32][8/2459]Time: 0.030 (0.039) Data: 0.001 (0.001) Loss: 0.0318 (0.0317)\n",
      "Epoch: [32][9/2459]Time: 0.029 (0.038) Data: 0.001 (0.001) Loss: 0.0298 (0.0315)\n",
      "Epoch: [32][10/2459]Time: 0.039 (0.038) Data: 0.001 (0.001) Loss: 0.0289 (0.0312)\n",
      "Epoch: [32][11/2459]Time: 0.030 (0.037) Data: 0.001 (0.001) Loss: 0.0312 (0.0312)\n",
      "Epoch: [32][12/2459]Time: 0.050 (0.038) Data: 0.001 (0.001) Loss: 0.0315 (0.0313)\n",
      "Epoch: [32][13/2459]Time: 0.070 (0.041) Data: 0.001 (0.001) Loss: 0.0280 (0.0310)\n",
      "Epoch: [32][14/2459]Time: 0.026 (0.040) Data: 0.001 (0.001) Loss: 0.0320 (0.0311)\n",
      "Epoch: [32][15/2459]Time: 0.026 (0.039) Data: 0.001 (0.001) Loss: 0.0327 (0.0312)\n",
      "Epoch: [32][16/2459]Time: 0.025 (0.038) Data: 0.001 (0.001) Loss: 0.0353 (0.0314)\n",
      "Epoch: [32][17/2459]Time: 0.026 (0.037) Data: 0.001 (0.001) Loss: 0.0311 (0.0314)\n",
      "Epoch: [32][18/2459]Time: 0.035 (0.037) Data: 0.001 (0.001) Loss: 0.0321 (0.0315)\n",
      "Epoch: [32][19/2459]Time: 0.025 (0.037) Data: 0.001 (0.001) Loss: 0.0321 (0.0315)\n",
      "Epoch: [32][20/2459]Time: 0.023 (0.036) Data: 0.001 (0.001) Loss: 0.0271 (0.0313)\n",
      "Epoch: [32][21/2459]Time: 0.027 (0.035) Data: 0.001 (0.001) Loss: 0.0309 (0.0313)\n",
      "Epoch: [32][22/2459]Time: 0.030 (0.035) Data: 0.001 (0.001) Loss: 0.0348 (0.0314)\n",
      "Epoch: [32][23/2459]Time: 0.031 (0.035) Data: 0.001 (0.001) Loss: 0.0321 (0.0314)\n",
      "10-NN,s=0.1: TOP1:  36.166666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 33\n",
      "ResNet1D\n",
      "Epoch: [33][0/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0205 (0.0205)\n",
      "Epoch: [33][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0252 (0.0229)\n",
      "Epoch: [33][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0267 (0.0242)\n",
      "Epoch: [33][3/2459]Time: 0.025 (0.023) Data: 0.001 (0.001) Loss: 0.0316 (0.0260)\n",
      "Epoch: [33][4/2459]Time: 0.028 (0.024) Data: 0.001 (0.001) Loss: 0.0282 (0.0264)\n",
      "Epoch: [33][5/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0260 (0.0264)\n",
      "Epoch: [33][6/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0332 (0.0274)\n",
      "Epoch: [33][7/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0288 (0.0275)\n",
      "Epoch: [33][8/2459]Time: 0.025 (0.024) Data: 0.001 (0.001) Loss: 0.0243 (0.0272)\n",
      "Epoch: [33][9/2459]Time: 0.029 (0.025) Data: 0.001 (0.001) Loss: 0.0365 (0.0281)\n",
      "Epoch: [33][10/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0253 (0.0279)\n",
      "Epoch: [33][11/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0266 (0.0277)\n",
      "Epoch: [33][12/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0313 (0.0280)\n",
      "Epoch: [33][13/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0277 (0.0280)\n",
      "Epoch: [33][14/2459]Time: 0.025 (0.024) Data: 0.001 (0.001) Loss: 0.0333 (0.0283)\n",
      "Epoch: [33][15/2459]Time: 0.035 (0.025) Data: 0.001 (0.001) Loss: 0.0312 (0.0285)\n",
      "Epoch: [33][16/2459]Time: 0.026 (0.025) Data: 0.001 (0.001) Loss: 0.0347 (0.0289)\n",
      "Epoch: [33][17/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0311 (0.0290)\n",
      "Epoch: [33][18/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0389 (0.0295)\n",
      "Epoch: [33][19/2459]Time: 0.026 (0.025) Data: 0.001 (0.001) Loss: 0.0270 (0.0294)\n",
      "Epoch: [33][20/2459]Time: 0.034 (0.025) Data: 0.001 (0.001) Loss: 0.0269 (0.0293)\n",
      "Epoch: [33][21/2459]Time: 0.027 (0.025) Data: 0.001 (0.001) Loss: 0.0265 (0.0292)\n",
      "Epoch: [33][22/2459]Time: 0.031 (0.026) Data: 0.001 (0.001) Loss: 0.0285 (0.0291)\n",
      "Epoch: [33][23/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0262 (0.0290)\n",
      "10-NN,s=0.1: TOP1:  36.5\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 34\n",
      "ResNet1D\n",
      "Epoch: [34][0/2459]Time: 0.034 (0.034) Data: 0.001 (0.001) Loss: 0.0244 (0.0244)\n",
      "Epoch: [34][1/2459]Time: 0.032 (0.033) Data: 0.002 (0.001) Loss: 0.0272 (0.0258)\n",
      "Epoch: [34][2/2459]Time: 0.032 (0.033) Data: 0.001 (0.001) Loss: 0.0223 (0.0246)\n",
      "Epoch: [34][3/2459]Time: 0.030 (0.032) Data: 0.002 (0.001) Loss: 0.0215 (0.0238)\n",
      "Epoch: [34][4/2459]Time: 0.026 (0.031) Data: 0.001 (0.001) Loss: 0.0228 (0.0236)\n",
      "Epoch: [34][5/2459]Time: 0.028 (0.030) Data: 0.001 (0.001) Loss: 0.0263 (0.0241)\n",
      "Epoch: [34][6/2459]Time: 0.039 (0.032) Data: 0.002 (0.001) Loss: 0.0252 (0.0242)\n",
      "Epoch: [34][7/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0283 (0.0247)\n",
      "Epoch: [34][8/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0244 (0.0247)\n",
      "Epoch: [34][9/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0235 (0.0246)\n",
      "Epoch: [34][10/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0339 (0.0254)\n",
      "Epoch: [34][11/2459]Time: 0.028 (0.029) Data: 0.001 (0.001) Loss: 0.0241 (0.0253)\n",
      "Epoch: [34][12/2459]Time: 0.034 (0.029) Data: 0.002 (0.001) Loss: 0.0211 (0.0250)\n",
      "Epoch: [34][13/2459]Time: 0.030 (0.029) Data: 0.001 (0.001) Loss: 0.0265 (0.0251)\n",
      "Epoch: [34][14/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0288 (0.0253)\n",
      "Epoch: [34][15/2459]Time: 0.024 (0.028) Data: 0.001 (0.001) Loss: 0.0245 (0.0253)\n",
      "Epoch: [34][16/2459]Time: 0.024 (0.028) Data: 0.001 (0.001) Loss: 0.0253 (0.0253)\n",
      "Epoch: [34][17/2459]Time: 0.036 (0.029) Data: 0.001 (0.001) Loss: 0.0276 (0.0254)\n",
      "Epoch: [34][18/2459]Time: 0.038 (0.029) Data: 0.001 (0.001) Loss: 0.0226 (0.0253)\n",
      "Epoch: [34][19/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0215 (0.0251)\n",
      "Epoch: [34][20/2459]Time: 0.028 (0.029) Data: 0.001 (0.001) Loss: 0.0312 (0.0254)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][21/2459]Time: 0.034 (0.029) Data: 0.001 (0.001) Loss: 0.0286 (0.0255)\n",
      "Epoch: [34][22/2459]Time: 0.043 (0.030) Data: 0.001 (0.001) Loss: 0.0262 (0.0256)\n",
      "Epoch: [34][23/2459]Time: 0.034 (0.030) Data: 0.002 (0.001) Loss: 0.0243 (0.0255)\n",
      "10-NN,s=0.1: TOP1:  37.0\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 35\n",
      "ResNet1D\n",
      "Epoch: [35][0/2459]Time: 0.023 (0.023) Data: 0.002 (0.002) Loss: 0.0210 (0.0210)\n",
      "Epoch: [35][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0376 (0.0293)\n",
      "Epoch: [35][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0231 (0.0272)\n",
      "Epoch: [35][3/2459]Time: 0.032 (0.025) Data: 0.001 (0.001) Loss: 0.0273 (0.0272)\n",
      "Epoch: [35][4/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0300 (0.0278)\n",
      "Epoch: [35][5/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0247 (0.0273)\n",
      "Epoch: [35][6/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0495 (0.0305)\n",
      "Epoch: [35][7/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0247 (0.0297)\n",
      "Epoch: [35][8/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0315 (0.0299)\n",
      "Epoch: [35][9/2459]Time: 0.027 (0.024) Data: 0.001 (0.001) Loss: 0.0202 (0.0290)\n",
      "Epoch: [35][10/2459]Time: 0.026 (0.024) Data: 0.001 (0.001) Loss: 0.0216 (0.0283)\n",
      "Epoch: [35][11/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0238 (0.0279)\n",
      "Epoch: [35][12/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0231 (0.0275)\n",
      "Epoch: [35][13/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0330 (0.0279)\n",
      "Epoch: [35][14/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0294 (0.0280)\n",
      "Epoch: [35][15/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0301 (0.0282)\n",
      "Epoch: [35][16/2459]Time: 0.035 (0.024) Data: 0.001 (0.001) Loss: 0.0283 (0.0282)\n",
      "Epoch: [35][17/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0414 (0.0289)\n",
      "Epoch: [35][18/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0342 (0.0292)\n",
      "Epoch: [35][19/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0264 (0.0290)\n",
      "Epoch: [35][20/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0269 (0.0289)\n",
      "Epoch: [35][21/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0303 (0.0290)\n",
      "Epoch: [35][22/2459]Time: 0.031 (0.024) Data: 0.001 (0.001) Loss: 0.0266 (0.0289)\n",
      "Epoch: [35][23/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0341 (0.0291)\n",
      "10-NN,s=0.1: TOP1:  36.833333333333336\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 36\n",
      "ResNet1D\n",
      "Epoch: [36][0/2459]Time: 0.034 (0.034) Data: 0.002 (0.002) Loss: 0.0249 (0.0249)\n",
      "Epoch: [36][1/2459]Time: 0.027 (0.031) Data: 0.001 (0.001) Loss: 0.0241 (0.0245)\n",
      "Epoch: [36][2/2459]Time: 0.026 (0.029) Data: 0.001 (0.001) Loss: 0.0212 (0.0234)\n",
      "Epoch: [36][3/2459]Time: 0.026 (0.028) Data: 0.001 (0.001) Loss: 0.0286 (0.0247)\n",
      "Epoch: [36][4/2459]Time: 0.026 (0.028) Data: 0.002 (0.001) Loss: 0.0282 (0.0254)\n",
      "Epoch: [36][5/2459]Time: 0.028 (0.028) Data: 0.001 (0.001) Loss: 0.0261 (0.0255)\n",
      "Epoch: [36][6/2459]Time: 0.026 (0.028) Data: 0.001 (0.001) Loss: 0.0264 (0.0256)\n",
      "Epoch: [36][7/2459]Time: 0.023 (0.027) Data: 0.001 (0.001) Loss: 0.0235 (0.0254)\n",
      "Epoch: [36][8/2459]Time: 0.023 (0.027) Data: 0.001 (0.001) Loss: 0.0273 (0.0256)\n",
      "Epoch: [36][9/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0243 (0.0255)\n",
      "Epoch: [36][10/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0296 (0.0258)\n",
      "Epoch: [36][11/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0245 (0.0257)\n",
      "Epoch: [36][12/2459]Time: 0.030 (0.026) Data: 0.001 (0.001) Loss: 0.0279 (0.0259)\n",
      "Epoch: [36][13/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0274 (0.0260)\n",
      "Epoch: [36][14/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0263 (0.0260)\n",
      "Epoch: [36][15/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0264 (0.0260)\n",
      "Epoch: [36][16/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0340 (0.0265)\n",
      "Epoch: [36][17/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0342 (0.0269)\n",
      "Epoch: [36][18/2459]Time: 0.031 (0.026) Data: 0.001 (0.001) Loss: 0.0242 (0.0268)\n",
      "Epoch: [36][19/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0248 (0.0267)\n",
      "Epoch: [36][20/2459]Time: 0.024 (0.026) Data: 0.002 (0.001) Loss: 0.0306 (0.0269)\n",
      "Epoch: [36][21/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0344 (0.0272)\n",
      "Epoch: [36][22/2459]Time: 0.024 (0.025) Data: 0.002 (0.001) Loss: 0.0275 (0.0272)\n",
      "Epoch: [36][23/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0296 (0.0273)\n",
      "10-NN,s=0.1: TOP1:  36.791666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 37\n",
      "ResNet1D\n",
      "Epoch: [37][0/2459]Time: 0.096 (0.096) Data: 0.002 (0.002) Loss: 0.0208 (0.0208)\n",
      "Epoch: [37][1/2459]Time: 0.069 (0.082) Data: 0.014 (0.008) Loss: 0.0222 (0.0215)\n",
      "Epoch: [37][2/2459]Time: 0.057 (0.074) Data: 0.004 (0.006) Loss: 0.0212 (0.0214)\n",
      "Epoch: [37][3/2459]Time: 0.036 (0.065) Data: 0.001 (0.005) Loss: 0.0254 (0.0224)\n",
      "Epoch: [37][4/2459]Time: 0.027 (0.057) Data: 0.001 (0.004) Loss: 0.0226 (0.0224)\n",
      "Epoch: [37][5/2459]Time: 0.023 (0.051) Data: 0.001 (0.004) Loss: 0.0204 (0.0221)\n",
      "Epoch: [37][6/2459]Time: 0.023 (0.047) Data: 0.001 (0.003) Loss: 0.0298 (0.0232)\n",
      "Epoch: [37][7/2459]Time: 0.023 (0.044) Data: 0.001 (0.003) Loss: 0.0206 (0.0229)\n",
      "Epoch: [37][8/2459]Time: 0.023 (0.042) Data: 0.002 (0.003) Loss: 0.0221 (0.0228)\n",
      "Epoch: [37][9/2459]Time: 0.036 (0.041) Data: 0.001 (0.003) Loss: 0.0312 (0.0236)\n",
      "Epoch: [37][10/2459]Time: 0.023 (0.040) Data: 0.001 (0.003) Loss: 0.0187 (0.0232)\n",
      "Epoch: [37][11/2459]Time: 0.024 (0.038) Data: 0.001 (0.002) Loss: 0.0234 (0.0232)\n",
      "Epoch: [37][12/2459]Time: 0.023 (0.037) Data: 0.001 (0.002) Loss: 0.0226 (0.0232)\n",
      "Epoch: [37][13/2459]Time: 0.023 (0.036) Data: 0.001 (0.002) Loss: 0.0252 (0.0233)\n",
      "Epoch: [37][14/2459]Time: 0.022 (0.035) Data: 0.001 (0.002) Loss: 0.0232 (0.0233)\n",
      "Epoch: [37][15/2459]Time: 0.029 (0.035) Data: 0.001 (0.002) Loss: 0.0208 (0.0231)\n",
      "Epoch: [37][16/2459]Time: 0.024 (0.034) Data: 0.001 (0.002) Loss: 0.0204 (0.0230)\n",
      "Epoch: [37][17/2459]Time: 0.023 (0.034) Data: 0.001 (0.002) Loss: 0.0214 (0.0229)\n",
      "Epoch: [37][18/2459]Time: 0.023 (0.033) Data: 0.001 (0.002) Loss: 0.0231 (0.0229)\n",
      "Epoch: [37][19/2459]Time: 0.024 (0.033) Data: 0.001 (0.002) Loss: 0.0310 (0.0233)\n",
      "Epoch: [37][20/2459]Time: 0.023 (0.032) Data: 0.001 (0.002) Loss: 0.0227 (0.0233)\n",
      "Epoch: [37][21/2459]Time: 0.024 (0.032) Data: 0.001 (0.002) Loss: 0.0243 (0.0233)\n",
      "Epoch: [37][22/2459]Time: 0.028 (0.032) Data: 0.001 (0.002) Loss: 0.0311 (0.0237)\n",
      "Epoch: [37][23/2459]Time: 0.023 (0.031) Data: 0.001 (0.002) Loss: 0.0250 (0.0237)\n",
      "10-NN,s=0.1: TOP1:  36.5\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 38\n",
      "ResNet1D\n",
      "Epoch: [38][0/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0285 (0.0285)\n",
      "Epoch: [38][1/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0192 (0.0238)\n",
      "Epoch: [38][2/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0207 (0.0228)\n",
      "Epoch: [38][3/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0200 (0.0221)\n",
      "Epoch: [38][4/2459]Time: 0.033 (0.025) Data: 0.001 (0.001) Loss: 0.0202 (0.0217)\n",
      "Epoch: [38][5/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0233 (0.0220)\n",
      "Epoch: [38][6/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0247 (0.0224)\n",
      "Epoch: [38][7/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0203 (0.0221)\n",
      "Epoch: [38][8/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0231 (0.0222)\n",
      "Epoch: [38][9/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0253 (0.0225)\n",
      "Epoch: [38][10/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0230 (0.0226)\n",
      "Epoch: [38][11/2459]Time: 0.026 (0.024) Data: 0.001 (0.001) Loss: 0.0209 (0.0224)\n",
      "Epoch: [38][12/2459]Time: 0.026 (0.025) Data: 0.001 (0.001) Loss: 0.0209 (0.0223)\n",
      "Epoch: [38][13/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0181 (0.0220)\n",
      "Epoch: [38][14/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0224 (0.0220)\n",
      "Epoch: [38][15/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0238 (0.0222)\n",
      "Epoch: [38][16/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0351 (0.0229)\n",
      "Epoch: [38][17/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0220 (0.0229)\n",
      "Epoch: [38][18/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0199 (0.0227)\n",
      "Epoch: [38][19/2459]Time: 0.030 (0.025) Data: 0.001 (0.001) Loss: 0.0385 (0.0235)\n",
      "Epoch: [38][20/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0229 (0.0235)\n",
      "Epoch: [38][21/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0216 (0.0234)\n",
      "Epoch: [38][22/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0606 (0.0250)\n",
      "Epoch: [38][23/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0199 (0.0248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-NN,s=0.1: TOP1:  36.708333333333336\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 39\n",
      "ResNet1D\n",
      "Epoch: [39][0/2459]Time: 0.023 (0.023) Data: 0.002 (0.002) Loss: 0.0243 (0.0243)\n",
      "Epoch: [39][1/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0215 (0.0229)\n",
      "Epoch: [39][2/2459]Time: 0.030 (0.025) Data: 0.001 (0.001) Loss: 0.0209 (0.0222)\n",
      "Epoch: [39][3/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0243 (0.0228)\n",
      "Epoch: [39][4/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0218 (0.0226)\n",
      "Epoch: [39][5/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0229 (0.0226)\n",
      "Epoch: [39][6/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0266 (0.0232)\n",
      "Epoch: [39][7/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0239 (0.0233)\n",
      "Epoch: [39][8/2459]Time: 0.034 (0.025) Data: 0.001 (0.001) Loss: 0.0228 (0.0232)\n",
      "Epoch: [39][9/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0207 (0.0230)\n",
      "Epoch: [39][10/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0306 (0.0237)\n",
      "Epoch: [39][11/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0190 (0.0233)\n",
      "Epoch: [39][12/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0197 (0.0230)\n",
      "Epoch: [39][13/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0254 (0.0232)\n",
      "Epoch: [39][14/2459]Time: 0.029 (0.024) Data: 0.001 (0.001) Loss: 0.0226 (0.0231)\n",
      "Epoch: [39][15/2459]Time: 0.026 (0.024) Data: 0.001 (0.001) Loss: 0.0267 (0.0234)\n",
      "Epoch: [39][16/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0227 (0.0233)\n",
      "Epoch: [39][17/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0215 (0.0232)\n",
      "Epoch: [39][18/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0279 (0.0235)\n",
      "Epoch: [39][19/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0254 (0.0236)\n",
      "Epoch: [39][20/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0204 (0.0234)\n",
      "Epoch: [39][21/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0209 (0.0233)\n",
      "Epoch: [39][22/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0211 (0.0232)\n",
      "Epoch: [39][23/2459]Time: 0.036 (0.024) Data: 0.001 (0.001) Loss: 0.0233 (0.0232)\n",
      "10-NN,s=0.1: TOP1:  36.5\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 40\n",
      "ResNet1D\n",
      "Epoch: [40][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0184 (0.0184)\n",
      "Epoch: [40][1/2459]Time: 0.029 (0.026) Data: 0.001 (0.001) Loss: 0.0173 (0.0178)\n",
      "Epoch: [40][2/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0210 (0.0189)\n",
      "Epoch: [40][3/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0177 (0.0186)\n",
      "Epoch: [40][4/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0206 (0.0190)\n",
      "Epoch: [40][5/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0188 (0.0190)\n",
      "Epoch: [40][6/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0182 (0.0188)\n",
      "Epoch: [40][7/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0199 (0.0190)\n",
      "Epoch: [40][8/2459]Time: 0.039 (0.025) Data: 0.001 (0.001) Loss: 0.0221 (0.0193)\n",
      "Epoch: [40][9/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0313 (0.0205)\n",
      "Epoch: [40][10/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0196 (0.0204)\n",
      "Epoch: [40][11/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0298 (0.0212)\n",
      "Epoch: [40][12/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0185 (0.0210)\n",
      "Epoch: [40][13/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0162 (0.0207)\n",
      "Epoch: [40][14/2459]Time: 0.034 (0.025) Data: 0.001 (0.001) Loss: 0.0190 (0.0206)\n",
      "Epoch: [40][15/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0188 (0.0204)\n",
      "Epoch: [40][16/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0254 (0.0207)\n",
      "Epoch: [40][17/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0168 (0.0205)\n",
      "Epoch: [40][18/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0163 (0.0203)\n",
      "Epoch: [40][19/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0191 (0.0202)\n",
      "Epoch: [40][20/2459]Time: 0.036 (0.025) Data: 0.001 (0.001) Loss: 0.0188 (0.0202)\n",
      "Epoch: [40][21/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0270 (0.0205)\n",
      "Epoch: [40][22/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0226 (0.0206)\n",
      "Epoch: [40][23/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0214 (0.0206)\n",
      "10-NN,s=0.1: TOP1:  36.583333333333336\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 41\n",
      "ResNet1D\n",
      "Epoch: [41][0/2459]Time: 0.026 (0.026) Data: 0.002 (0.002) Loss: 0.0217 (0.0217)\n",
      "Epoch: [41][1/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0223 (0.0220)\n",
      "Epoch: [41][2/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0185 (0.0208)\n",
      "Epoch: [41][3/2459]Time: 0.037 (0.029) Data: 0.001 (0.001) Loss: 0.0144 (0.0192)\n",
      "Epoch: [41][4/2459]Time: 0.024 (0.028) Data: 0.001 (0.001) Loss: 0.0201 (0.0194)\n",
      "Epoch: [41][5/2459]Time: 0.023 (0.027) Data: 0.001 (0.001) Loss: 0.0203 (0.0195)\n",
      "Epoch: [41][6/2459]Time: 0.022 (0.026) Data: 0.001 (0.001) Loss: 0.0173 (0.0192)\n",
      "Epoch: [41][7/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0198 (0.0193)\n",
      "Epoch: [41][8/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0162 (0.0190)\n",
      "Epoch: [41][9/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0172 (0.0188)\n",
      "Epoch: [41][10/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0279 (0.0196)\n",
      "Epoch: [41][11/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0200 (0.0196)\n",
      "Epoch: [41][12/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0181 (0.0195)\n",
      "Epoch: [41][13/2459]Time: 0.035 (0.025) Data: 0.001 (0.001) Loss: 0.0206 (0.0196)\n",
      "Epoch: [41][14/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0311 (0.0204)\n",
      "Epoch: [41][15/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0158 (0.0201)\n",
      "Epoch: [41][16/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0219 (0.0202)\n",
      "Epoch: [41][17/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0219 (0.0203)\n",
      "Epoch: [41][18/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0268 (0.0206)\n",
      "Epoch: [41][19/2459]Time: 0.027 (0.025) Data: 0.001 (0.001) Loss: 0.0199 (0.0206)\n",
      "Epoch: [41][20/2459]Time: 0.026 (0.025) Data: 0.001 (0.001) Loss: 0.0182 (0.0205)\n",
      "Epoch: [41][21/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0230 (0.0206)\n",
      "Epoch: [41][22/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0189 (0.0205)\n",
      "Epoch: [41][23/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0186 (0.0204)\n",
      "10-NN,s=0.1: TOP1:  37.041666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 42\n",
      "ResNet1D\n",
      "Epoch: [42][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0299 (0.0299)\n",
      "Epoch: [42][1/2459]Time: 0.035 (0.029) Data: 0.001 (0.001) Loss: 0.0199 (0.0249)\n",
      "Epoch: [42][2/2459]Time: 0.025 (0.028) Data: 0.001 (0.001) Loss: 0.0184 (0.0228)\n",
      "Epoch: [42][3/2459]Time: 0.026 (0.027) Data: 0.001 (0.001) Loss: 0.0182 (0.0216)\n",
      "Epoch: [42][4/2459]Time: 0.025 (0.027) Data: 0.001 (0.001) Loss: 0.0188 (0.0210)\n",
      "Epoch: [42][5/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0188 (0.0207)\n",
      "Epoch: [42][6/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0189 (0.0204)\n",
      "Epoch: [42][7/2459]Time: 0.031 (0.026) Data: 0.001 (0.001) Loss: 0.0195 (0.0203)\n",
      "Epoch: [42][8/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0173 (0.0200)\n",
      "Epoch: [42][9/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0370 (0.0217)\n",
      "Epoch: [42][10/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0259 (0.0221)\n",
      "Epoch: [42][11/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0210 (0.0220)\n",
      "Epoch: [42][12/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0175 (0.0216)\n",
      "Epoch: [42][13/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0185 (0.0214)\n",
      "Epoch: [42][14/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0250 (0.0216)\n",
      "Epoch: [42][15/2459]Time: 0.033 (0.025) Data: 0.001 (0.001) Loss: 0.0187 (0.0215)\n",
      "Epoch: [42][16/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0205 (0.0214)\n",
      "Epoch: [42][17/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0181 (0.0212)\n",
      "Epoch: [42][18/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0286 (0.0216)\n",
      "Epoch: [42][19/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0185 (0.0214)\n",
      "Epoch: [42][20/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0202 (0.0214)\n",
      "Epoch: [42][21/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0220 (0.0214)\n",
      "Epoch: [42][22/2459]Time: 0.029 (0.025) Data: 0.001 (0.001) Loss: 0.0402 (0.0222)\n",
      "Epoch: [42][23/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0257 (0.0224)\n",
      "10-NN,s=0.1: TOP1:  36.458333333333336\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 43\n",
      "ResNet1D\n",
      "Epoch: [43][0/2459]Time: 0.032 (0.032) Data: 0.001 (0.001) Loss: 0.0165 (0.0165)\n",
      "Epoch: [43][1/2459]Time: 0.023 (0.028) Data: 0.001 (0.001) Loss: 0.0204 (0.0185)\n",
      "Epoch: [43][2/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0230 (0.0200)\n",
      "Epoch: [43][3/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0176 (0.0194)\n",
      "Epoch: [43][4/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0171 (0.0189)\n",
      "Epoch: [43][5/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0199 (0.0191)\n",
      "Epoch: [43][6/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0215 (0.0194)\n",
      "Epoch: [43][7/2459]Time: 0.029 (0.025) Data: 0.001 (0.001) Loss: 0.0243 (0.0200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43][8/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0172 (0.0197)\n",
      "Epoch: [43][9/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0247 (0.0202)\n",
      "Epoch: [43][10/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0171 (0.0199)\n",
      "Epoch: [43][11/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0173 (0.0197)\n",
      "Epoch: [43][12/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0187 (0.0196)\n",
      "Epoch: [43][13/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0191 (0.0196)\n",
      "Epoch: [43][14/2459]Time: 0.036 (0.025) Data: 0.001 (0.001) Loss: 0.0212 (0.0197)\n",
      "Epoch: [43][15/2459]Time: 0.066 (0.028) Data: 0.001 (0.001) Loss: 0.0193 (0.0197)\n",
      "Epoch: [43][16/2459]Time: 0.083 (0.031) Data: 0.026 (0.003) Loss: 0.0358 (0.0206)\n",
      "Epoch: [43][17/2459]Time: 0.035 (0.031) Data: 0.001 (0.003) Loss: 0.0193 (0.0205)\n",
      "Epoch: [43][18/2459]Time: 0.053 (0.032) Data: 0.008 (0.003) Loss: 0.0233 (0.0207)\n",
      "Epoch: [43][19/2459]Time: 0.040 (0.033) Data: 0.009 (0.003) Loss: 0.0281 (0.0211)\n",
      "Epoch: [43][20/2459]Time: 0.029 (0.032) Data: 0.001 (0.003) Loss: 0.0168 (0.0209)\n",
      "Epoch: [43][21/2459]Time: 0.025 (0.032) Data: 0.001 (0.003) Loss: 0.0184 (0.0208)\n",
      "Epoch: [43][22/2459]Time: 0.025 (0.032) Data: 0.001 (0.003) Loss: 0.0215 (0.0208)\n",
      "Epoch: [43][23/2459]Time: 0.025 (0.032) Data: 0.001 (0.003) Loss: 0.0286 (0.0211)\n",
      "10-NN,s=0.1: TOP1:  36.833333333333336\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 44\n",
      "ResNet1D\n",
      "Epoch: [44][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0159 (0.0159)\n",
      "Epoch: [44][1/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0200 (0.0180)\n",
      "Epoch: [44][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0247 (0.0202)\n",
      "Epoch: [44][3/2459]Time: 0.028 (0.024) Data: 0.001 (0.001) Loss: 0.0217 (0.0206)\n",
      "Epoch: [44][4/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0180 (0.0201)\n",
      "Epoch: [44][5/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0165 (0.0195)\n",
      "Epoch: [44][6/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0177 (0.0192)\n",
      "Epoch: [44][7/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0187 (0.0191)\n",
      "Epoch: [44][8/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0172 (0.0189)\n",
      "Epoch: [44][9/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0233 (0.0194)\n",
      "Epoch: [44][10/2459]Time: 0.029 (0.024) Data: 0.001 (0.001) Loss: 0.0174 (0.0192)\n",
      "Epoch: [44][11/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0168 (0.0190)\n",
      "Epoch: [44][12/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0155 (0.0187)\n",
      "Epoch: [44][13/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0161 (0.0185)\n",
      "Epoch: [44][14/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0179 (0.0185)\n",
      "Epoch: [44][15/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0286 (0.0191)\n",
      "Epoch: [44][16/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0214 (0.0193)\n",
      "Epoch: [44][17/2459]Time: 0.035 (0.024) Data: 0.001 (0.001) Loss: 0.0217 (0.0194)\n",
      "Epoch: [44][18/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0190 (0.0194)\n",
      "Epoch: [44][19/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0158 (0.0192)\n",
      "Epoch: [44][20/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0209 (0.0193)\n",
      "Epoch: [44][21/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0270 (0.0196)\n",
      "Epoch: [44][22/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0177 (0.0195)\n",
      "Epoch: [44][23/2459]Time: 0.031 (0.024) Data: 0.001 (0.001) Loss: 0.0193 (0.0195)\n",
      "10-NN,s=0.1: TOP1:  35.875\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 45\n",
      "ResNet1D\n",
      "Epoch: [45][0/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0178 (0.0178)\n",
      "Epoch: [45][1/2459]Time: 0.029 (0.029) Data: 0.001 (0.001) Loss: 0.0153 (0.0165)\n",
      "Epoch: [45][2/2459]Time: 0.029 (0.029) Data: 0.001 (0.001) Loss: 0.0252 (0.0194)\n",
      "Epoch: [45][3/2459]Time: 0.031 (0.030) Data: 0.001 (0.001) Loss: 0.0198 (0.0195)\n",
      "Epoch: [45][4/2459]Time: 0.031 (0.030) Data: 0.001 (0.001) Loss: 0.0178 (0.0192)\n",
      "Epoch: [45][5/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0454 (0.0235)\n",
      "Epoch: [45][6/2459]Time: 0.029 (0.030) Data: 0.001 (0.001) Loss: 0.0177 (0.0227)\n",
      "Epoch: [45][7/2459]Time: 0.031 (0.030) Data: 0.001 (0.001) Loss: 0.0177 (0.0221)\n",
      "Epoch: [45][8/2459]Time: 0.031 (0.030) Data: 0.001 (0.001) Loss: 0.0154 (0.0213)\n",
      "Epoch: [45][9/2459]Time: 0.031 (0.030) Data: 0.001 (0.001) Loss: 0.0240 (0.0216)\n",
      "Epoch: [45][10/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0178 (0.0212)\n",
      "Epoch: [45][11/2459]Time: 0.029 (0.030) Data: 0.001 (0.001) Loss: 0.0273 (0.0218)\n",
      "Epoch: [45][12/2459]Time: 0.029 (0.030) Data: 0.001 (0.001) Loss: 0.0152 (0.0212)\n",
      "Epoch: [45][13/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0275 (0.0217)\n",
      "Epoch: [45][14/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0188 (0.0215)\n",
      "Epoch: [45][15/2459]Time: 0.034 (0.030) Data: 0.001 (0.001) Loss: 0.0154 (0.0211)\n",
      "Epoch: [45][16/2459]Time: 0.031 (0.030) Data: 0.001 (0.001) Loss: 0.0214 (0.0211)\n",
      "Epoch: [45][17/2459]Time: 0.029 (0.030) Data: 0.001 (0.001) Loss: 0.0197 (0.0211)\n",
      "Epoch: [45][18/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0169 (0.0208)\n",
      "Epoch: [45][19/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0182 (0.0207)\n",
      "Epoch: [45][20/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0213 (0.0207)\n",
      "Epoch: [45][21/2459]Time: 0.035 (0.030) Data: 0.001 (0.001) Loss: 0.0230 (0.0208)\n",
      "Epoch: [45][22/2459]Time: 0.029 (0.030) Data: 0.001 (0.001) Loss: 0.0185 (0.0207)\n",
      "Epoch: [45][23/2459]Time: 0.026 (0.030) Data: 0.001 (0.001) Loss: 0.0267 (0.0210)\n",
      "10-NN,s=0.1: TOP1:  36.666666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 46\n",
      "ResNet1D\n",
      "Epoch: [46][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0196 (0.0196)\n",
      "Epoch: [46][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0210 (0.0203)\n",
      "Epoch: [46][2/2459]Time: 0.031 (0.026) Data: 0.001 (0.001) Loss: 0.0234 (0.0214)\n",
      "Epoch: [46][3/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0190 (0.0208)\n",
      "Epoch: [46][4/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0149 (0.0196)\n",
      "Epoch: [46][5/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0210 (0.0198)\n",
      "Epoch: [46][6/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0178 (0.0195)\n",
      "Epoch: [46][7/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0189 (0.0195)\n",
      "Epoch: [46][8/2459]Time: 0.032 (0.025) Data: 0.001 (0.001) Loss: 0.0185 (0.0193)\n",
      "Epoch: [46][9/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0195 (0.0194)\n",
      "Epoch: [46][10/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0246 (0.0198)\n",
      "Epoch: [46][11/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0182 (0.0197)\n",
      "Epoch: [46][12/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0165 (0.0195)\n",
      "Epoch: [46][13/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0189 (0.0194)\n",
      "Epoch: [46][14/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0149 (0.0191)\n",
      "Epoch: [46][15/2459]Time: 0.038 (0.026) Data: 0.001 (0.001) Loss: 0.0216 (0.0193)\n",
      "Epoch: [46][16/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0155 (0.0190)\n",
      "Epoch: [46][17/2459]Time: 0.027 (0.026) Data: 0.001 (0.001) Loss: 0.0188 (0.0190)\n",
      "Epoch: [46][18/2459]Time: 0.027 (0.026) Data: 0.001 (0.001) Loss: 0.0236 (0.0193)\n",
      "Epoch: [46][19/2459]Time: 0.029 (0.026) Data: 0.001 (0.001) Loss: 0.0300 (0.0198)\n",
      "Epoch: [46][20/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0230 (0.0200)\n",
      "Epoch: [46][21/2459]Time: 0.041 (0.027) Data: 0.001 (0.001) Loss: 0.0156 (0.0198)\n",
      "Epoch: [46][22/2459]Time: 0.028 (0.027) Data: 0.001 (0.001) Loss: 0.0184 (0.0197)\n",
      "Epoch: [46][23/2459]Time: 0.026 (0.027) Data: 0.001 (0.001) Loss: 0.0160 (0.0195)\n",
      "10-NN,s=0.1: TOP1:  37.0\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 47\n",
      "ResNet1D\n",
      "Epoch: [47][0/2459]Time: 0.029 (0.029) Data: 0.001 (0.001) Loss: 0.0162 (0.0162)\n",
      "Epoch: [47][1/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0175 (0.0169)\n",
      "Epoch: [47][2/2459]Time: 0.022 (0.025) Data: 0.001 (0.001) Loss: 0.0145 (0.0161)\n",
      "Epoch: [47][3/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0195 (0.0169)\n",
      "Epoch: [47][4/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0216 (0.0179)\n",
      "Epoch: [47][5/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0268 (0.0193)\n",
      "Epoch: [47][6/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0154 (0.0188)\n",
      "Epoch: [47][7/2459]Time: 0.028 (0.024) Data: 0.001 (0.001) Loss: 0.0185 (0.0188)\n",
      "Epoch: [47][8/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0128 (0.0181)\n",
      "Epoch: [47][9/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0176 (0.0180)\n",
      "Epoch: [47][10/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0149 (0.0178)\n",
      "Epoch: [47][11/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0155 (0.0176)\n",
      "Epoch: [47][12/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0163 (0.0175)\n",
      "Epoch: [47][13/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0166 (0.0174)\n",
      "Epoch: [47][14/2459]Time: 0.035 (0.024) Data: 0.001 (0.001) Loss: 0.0210 (0.0176)\n",
      "Epoch: [47][15/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0254 (0.0181)\n",
      "Epoch: [47][16/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0221 (0.0184)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47][17/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0179 (0.0183)\n",
      "Epoch: [47][18/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0198 (0.0184)\n",
      "Epoch: [47][19/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0304 (0.0190)\n",
      "Epoch: [47][20/2459]Time: 0.031 (0.024) Data: 0.001 (0.001) Loss: 0.0201 (0.0191)\n",
      "Epoch: [47][21/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0178 (0.0190)\n",
      "Epoch: [47][22/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0189 (0.0190)\n",
      "Epoch: [47][23/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0196 (0.0190)\n",
      "10-NN,s=0.1: TOP1:  36.916666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 48\n",
      "ResNet1D\n",
      "Epoch: [48][0/2459]Time: 0.023 (0.023) Data: 0.002 (0.002) Loss: 0.0334 (0.0334)\n",
      "Epoch: [48][1/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0155 (0.0244)\n",
      "Epoch: [48][2/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0168 (0.0219)\n",
      "Epoch: [48][3/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0199 (0.0214)\n",
      "Epoch: [48][4/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0182 (0.0208)\n",
      "Epoch: [48][5/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0160 (0.0200)\n",
      "Epoch: [48][6/2459]Time: 0.028 (0.023) Data: 0.001 (0.001) Loss: 0.0199 (0.0200)\n",
      "Epoch: [48][7/2459]Time: 0.024 (0.023) Data: 0.001 (0.001) Loss: 0.0143 (0.0193)\n",
      "Epoch: [48][8/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0199 (0.0193)\n",
      "Epoch: [48][9/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0161 (0.0190)\n",
      "Epoch: [48][10/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0170 (0.0188)\n",
      "Epoch: [48][11/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0188 (0.0188)\n",
      "Epoch: [48][12/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0139 (0.0185)\n",
      "Epoch: [48][13/2459]Time: 0.028 (0.023) Data: 0.001 (0.001) Loss: 0.0254 (0.0189)\n",
      "Epoch: [48][14/2459]Time: 0.024 (0.023) Data: 0.001 (0.001) Loss: 0.0213 (0.0191)\n",
      "Epoch: [48][15/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0164 (0.0189)\n",
      "Epoch: [48][16/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0169 (0.0188)\n",
      "Epoch: [48][17/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0141 (0.0186)\n",
      "Epoch: [48][18/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0199 (0.0186)\n",
      "Epoch: [48][19/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0157 (0.0185)\n",
      "Epoch: [48][20/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0152 (0.0183)\n",
      "Epoch: [48][21/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0233 (0.0186)\n",
      "Epoch: [48][22/2459]Time: 0.031 (0.023) Data: 0.001 (0.001) Loss: 0.0172 (0.0185)\n",
      "Epoch: [48][23/2459]Time: 0.024 (0.023) Data: 0.001 (0.001) Loss: 0.0168 (0.0184)\n",
      "10-NN,s=0.1: TOP1:  35.958333333333336\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 49\n",
      "ResNet1D\n",
      "Epoch: [49][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0135 (0.0135)\n",
      "Epoch: [49][1/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0176 (0.0156)\n",
      "Epoch: [49][2/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0259 (0.0190)\n",
      "Epoch: [49][3/2459]Time: 0.031 (0.025) Data: 0.001 (0.001) Loss: 0.0141 (0.0178)\n",
      "Epoch: [49][4/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0149 (0.0172)\n",
      "Epoch: [49][5/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0149 (0.0168)\n",
      "Epoch: [49][6/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0164 (0.0168)\n",
      "Epoch: [49][7/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0141 (0.0164)\n",
      "Epoch: [49][8/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0214 (0.0170)\n",
      "Epoch: [49][9/2459]Time: 0.027 (0.024) Data: 0.001 (0.001) Loss: 0.0151 (0.0168)\n",
      "Epoch: [49][10/2459]Time: 0.025 (0.024) Data: 0.001 (0.001) Loss: 0.0171 (0.0168)\n",
      "Epoch: [49][11/2459]Time: 0.037 (0.025) Data: 0.001 (0.001) Loss: 0.0164 (0.0168)\n",
      "Epoch: [49][12/2459]Time: 0.026 (0.025) Data: 0.001 (0.001) Loss: 0.0162 (0.0167)\n",
      "Epoch: [49][13/2459]Time: 0.027 (0.025) Data: 0.001 (0.001) Loss: 0.0164 (0.0167)\n",
      "Epoch: [49][14/2459]Time: 0.031 (0.026) Data: 0.001 (0.001) Loss: 0.0210 (0.0170)\n",
      "Epoch: [49][15/2459]Time: 0.033 (0.026) Data: 0.001 (0.001) Loss: 0.0361 (0.0182)\n",
      "Epoch: [49][16/2459]Time: 0.031 (0.026) Data: 0.001 (0.001) Loss: 0.0155 (0.0180)\n",
      "Epoch: [49][17/2459]Time: 0.033 (0.027) Data: 0.001 (0.001) Loss: 0.0168 (0.0180)\n",
      "Epoch: [49][18/2459]Time: 0.030 (0.027) Data: 0.001 (0.001) Loss: 0.0208 (0.0181)\n",
      "Epoch: [49][19/2459]Time: 0.029 (0.027) Data: 0.001 (0.001) Loss: 0.0194 (0.0182)\n",
      "Epoch: [49][20/2459]Time: 0.027 (0.027) Data: 0.001 (0.001) Loss: 0.0179 (0.0182)\n",
      "Epoch: [49][21/2459]Time: 0.027 (0.027) Data: 0.001 (0.001) Loss: 0.0216 (0.0183)\n",
      "Epoch: [49][22/2459]Time: 0.031 (0.027) Data: 0.001 (0.001) Loss: 0.0209 (0.0184)\n",
      "Epoch: [49][23/2459]Time: 0.032 (0.027) Data: 0.001 (0.001) Loss: 0.0156 (0.0183)\n",
      "10-NN,s=0.1: TOP1:  35.833333333333336\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 50\n",
      "ResNet1D\n",
      "Epoch: [50][0/2459]Time: 0.027 (0.027) Data: 0.001 (0.001) Loss: 0.0177 (0.0177)\n",
      "Epoch: [50][1/2459]Time: 0.026 (0.027) Data: 0.001 (0.001) Loss: 0.0142 (0.0159)\n",
      "Epoch: [50][2/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0188 (0.0169)\n",
      "Epoch: [50][3/2459]Time: 0.029 (0.027) Data: 0.001 (0.001) Loss: 0.0159 (0.0166)\n",
      "Epoch: [50][4/2459]Time: 0.033 (0.028) Data: 0.001 (0.001) Loss: 0.0167 (0.0166)\n",
      "Epoch: [50][5/2459]Time: 0.032 (0.029) Data: 0.001 (0.001) Loss: 0.0219 (0.0175)\n",
      "Epoch: [50][6/2459]Time: 0.032 (0.029) Data: 0.001 (0.001) Loss: 0.0168 (0.0174)\n",
      "Epoch: [50][7/2459]Time: 0.032 (0.030) Data: 0.001 (0.001) Loss: 0.0193 (0.0176)\n",
      "Epoch: [50][8/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0150 (0.0174)\n",
      "Epoch: [50][9/2459]Time: 0.028 (0.030) Data: 0.001 (0.001) Loss: 0.0156 (0.0172)\n",
      "Epoch: [50][10/2459]Time: 0.028 (0.029) Data: 0.001 (0.001) Loss: 0.0200 (0.0174)\n",
      "Epoch: [50][11/2459]Time: 0.027 (0.029) Data: 0.001 (0.001) Loss: 0.0184 (0.0175)\n",
      "Epoch: [50][12/2459]Time: 0.027 (0.029) Data: 0.001 (0.001) Loss: 0.0138 (0.0172)\n",
      "Epoch: [50][13/2459]Time: 0.027 (0.029) Data: 0.001 (0.001) Loss: 0.0268 (0.0179)\n",
      "Epoch: [50][14/2459]Time: 0.041 (0.030) Data: 0.001 (0.001) Loss: 0.0198 (0.0180)\n",
      "Epoch: [50][15/2459]Time: 0.028 (0.030) Data: 0.001 (0.001) Loss: 0.0162 (0.0179)\n",
      "Epoch: [50][16/2459]Time: 0.028 (0.029) Data: 0.001 (0.001) Loss: 0.0244 (0.0183)\n",
      "Epoch: [50][17/2459]Time: 0.028 (0.029) Data: 0.001 (0.001) Loss: 0.0174 (0.0183)\n",
      "Epoch: [50][18/2459]Time: 0.030 (0.029) Data: 0.001 (0.001) Loss: 0.0168 (0.0182)\n",
      "Epoch: [50][19/2459]Time: 0.029 (0.029) Data: 0.001 (0.001) Loss: 0.0146 (0.0180)\n",
      "Epoch: [50][20/2459]Time: 0.027 (0.029) Data: 0.001 (0.001) Loss: 0.0145 (0.0178)\n",
      "Epoch: [50][21/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0182 (0.0179)\n",
      "Epoch: [50][22/2459]Time: 0.029 (0.029) Data: 0.001 (0.001) Loss: 0.0215 (0.0180)\n",
      "Epoch: [50][23/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0159 (0.0179)\n",
      "10-NN,s=0.1: TOP1:  36.416666666666664\n",
      "doing PCA with 128 components ..done\n",
      "50-NN,s=0.1: TOP1:  36.708333333333336\n",
      "50-NN,s=0.5: TOP1:  36.208333333333336\n",
      "10-NN,s=0.1: TOP1:  35.041666666666664\n",
      "10-NN,s=0.5: TOP1:  34.916666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 51\n",
      "ResNet1D\n",
      "Epoch: [51][0/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0134 (0.0134)\n",
      "Epoch: [51][1/2459]Time: 0.026 (0.026) Data: 0.001 (0.001) Loss: 0.0170 (0.0152)\n",
      "Epoch: [51][2/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0124 (0.0143)\n",
      "Epoch: [51][3/2459]Time: 0.028 (0.026) Data: 0.001 (0.001) Loss: 0.0143 (0.0143)\n",
      "Epoch: [51][4/2459]Time: 0.029 (0.027) Data: 0.001 (0.001) Loss: 0.0130 (0.0140)\n",
      "Epoch: [51][5/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0140 (0.0140)\n",
      "Epoch: [51][6/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0205 (0.0149)\n",
      "Epoch: [51][7/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0122 (0.0146)\n",
      "Epoch: [51][8/2459]Time: 0.034 (0.027) Data: 0.001 (0.001) Loss: 0.0122 (0.0143)\n",
      "Epoch: [51][9/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0140 (0.0143)\n",
      "Epoch: [51][10/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0164 (0.0145)\n",
      "Epoch: [51][11/2459]Time: 0.023 (0.026) Data: 0.001 (0.001) Loss: 0.0139 (0.0144)\n",
      "Epoch: [51][12/2459]Time: 0.056 (0.028) Data: 0.001 (0.001) Loss: 0.0158 (0.0145)\n",
      "Epoch: [51][13/2459]Time: 0.049 (0.029) Data: 0.003 (0.001) Loss: 0.0152 (0.0146)\n",
      "Epoch: [51][14/2459]Time: 0.053 (0.031) Data: 0.003 (0.001) Loss: 0.0255 (0.0153)\n",
      "Epoch: [51][15/2459]Time: 0.048 (0.032) Data: 0.001 (0.001) Loss: 0.0230 (0.0158)\n",
      "Epoch: [51][16/2459]Time: 0.034 (0.032) Data: 0.001 (0.001) Loss: 0.0147 (0.0157)\n",
      "Epoch: [51][17/2459]Time: 0.033 (0.032) Data: 0.001 (0.001) Loss: 0.0172 (0.0158)\n",
      "Epoch: [51][18/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.0170 (0.0159)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51][19/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.0139 (0.0158)\n",
      "Epoch: [51][20/2459]Time: 0.025 (0.031) Data: 0.001 (0.001) Loss: 0.0136 (0.0157)\n",
      "Epoch: [51][21/2459]Time: 0.024 (0.031) Data: 0.001 (0.001) Loss: 0.0171 (0.0157)\n",
      "Epoch: [51][22/2459]Time: 0.030 (0.031) Data: 0.001 (0.001) Loss: 0.0153 (0.0157)\n",
      "Epoch: [51][23/2459]Time: 0.025 (0.031) Data: 0.001 (0.001) Loss: 0.0212 (0.0159)\n",
      "10-NN,s=0.1: TOP1:  37.125\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 52\n",
      "ResNet1D\n",
      "Epoch: [52][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0110 (0.0110)\n",
      "Epoch: [52][1/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0142 (0.0126)\n",
      "Epoch: [52][2/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0125 (0.0126)\n",
      "Epoch: [52][3/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0145 (0.0131)\n",
      "Epoch: [52][4/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0149 (0.0134)\n",
      "Epoch: [52][5/2459]Time: 0.022 (0.022) Data: 0.001 (0.001) Loss: 0.0140 (0.0135)\n",
      "Epoch: [52][6/2459]Time: 0.036 (0.024) Data: 0.001 (0.001) Loss: 0.0121 (0.0133)\n",
      "Epoch: [52][7/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0140 (0.0134)\n",
      "Epoch: [52][8/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0193 (0.0141)\n",
      "Epoch: [52][9/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0183 (0.0145)\n",
      "Epoch: [52][10/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0201 (0.0150)\n",
      "Epoch: [52][11/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0175 (0.0152)\n",
      "Epoch: [52][12/2459]Time: 0.022 (0.023) Data: 0.001 (0.001) Loss: 0.0143 (0.0151)\n",
      "Epoch: [52][13/2459]Time: 0.035 (0.024) Data: 0.001 (0.001) Loss: 0.0167 (0.0152)\n",
      "Epoch: [52][14/2459]Time: 0.022 (0.024) Data: 0.001 (0.001) Loss: 0.0155 (0.0153)\n",
      "Epoch: [52][15/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0161 (0.0153)\n",
      "Epoch: [52][16/2459]Time: 0.034 (0.025) Data: 0.001 (0.001) Loss: 0.0143 (0.0153)\n",
      "Epoch: [52][17/2459]Time: 0.032 (0.025) Data: 0.001 (0.001) Loss: 0.0122 (0.0151)\n",
      "Epoch: [52][18/2459]Time: 0.035 (0.026) Data: 0.001 (0.001) Loss: 0.0257 (0.0156)\n",
      "Epoch: [52][19/2459]Time: 0.030 (0.026) Data: 0.001 (0.001) Loss: 0.0158 (0.0157)\n",
      "Epoch: [52][20/2459]Time: 0.030 (0.026) Data: 0.001 (0.001) Loss: 0.0231 (0.0160)\n",
      "Epoch: [52][21/2459]Time: 0.032 (0.026) Data: 0.001 (0.001) Loss: 0.0163 (0.0160)\n",
      "Epoch: [52][22/2459]Time: 0.031 (0.026) Data: 0.001 (0.001) Loss: 0.0175 (0.0161)\n",
      "Epoch: [52][23/2459]Time: 0.031 (0.027) Data: 0.001 (0.001) Loss: 0.0135 (0.0160)\n",
      "10-NN,s=0.1: TOP1:  36.5\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 53\n",
      "ResNet1D\n",
      "Epoch: [53][0/2459]Time: 0.035 (0.035) Data: 0.002 (0.002) Loss: 0.0172 (0.0172)\n",
      "Epoch: [53][1/2459]Time: 0.034 (0.034) Data: 0.001 (0.002) Loss: 0.0148 (0.0160)\n",
      "Epoch: [53][2/2459]Time: 0.033 (0.034) Data: 0.001 (0.001) Loss: 0.0135 (0.0152)\n",
      "Epoch: [53][3/2459]Time: 0.034 (0.034) Data: 0.001 (0.001) Loss: 0.0124 (0.0145)\n",
      "Epoch: [53][4/2459]Time: 0.035 (0.034) Data: 0.001 (0.001) Loss: 0.0167 (0.0149)\n",
      "Epoch: [53][5/2459]Time: 0.039 (0.035) Data: 0.001 (0.001) Loss: 0.0172 (0.0153)\n",
      "Epoch: [53][6/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0157 (0.0154)\n",
      "Epoch: [53][7/2459]Time: 0.068 (0.038) Data: 0.001 (0.001) Loss: 0.0154 (0.0154)\n",
      "Epoch: [53][8/2459]Time: 0.075 (0.042) Data: 0.001 (0.001) Loss: 0.0162 (0.0155)\n",
      "Epoch: [53][9/2459]Time: 0.023 (0.040) Data: 0.001 (0.001) Loss: 0.0183 (0.0157)\n",
      "Epoch: [53][10/2459]Time: 0.023 (0.038) Data: 0.001 (0.001) Loss: 0.0150 (0.0157)\n",
      "Epoch: [53][11/2459]Time: 0.023 (0.037) Data: 0.001 (0.001) Loss: 0.0176 (0.0158)\n",
      "Epoch: [53][12/2459]Time: 0.023 (0.036) Data: 0.001 (0.001) Loss: 0.0132 (0.0156)\n",
      "Epoch: [53][13/2459]Time: 0.028 (0.035) Data: 0.001 (0.001) Loss: 0.0136 (0.0155)\n",
      "Epoch: [53][14/2459]Time: 0.034 (0.035) Data: 0.002 (0.001) Loss: 0.0195 (0.0158)\n",
      "Epoch: [53][15/2459]Time: 0.028 (0.035) Data: 0.001 (0.001) Loss: 0.0153 (0.0157)\n",
      "Epoch: [53][16/2459]Time: 0.025 (0.034) Data: 0.001 (0.001) Loss: 0.0183 (0.0159)\n",
      "Epoch: [53][17/2459]Time: 0.023 (0.034) Data: 0.001 (0.001) Loss: 0.0172 (0.0160)\n",
      "Epoch: [53][18/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0170 (0.0160)\n",
      "Epoch: [53][19/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0157 (0.0160)\n",
      "Epoch: [53][20/2459]Time: 0.043 (0.033) Data: 0.001 (0.001) Loss: 0.0126 (0.0158)\n",
      "Epoch: [53][21/2459]Time: 0.024 (0.033) Data: 0.001 (0.001) Loss: 0.0176 (0.0159)\n",
      "Epoch: [53][22/2459]Time: 0.070 (0.034) Data: 0.001 (0.001) Loss: 0.0134 (0.0158)\n",
      "Epoch: [53][23/2459]Time: 0.075 (0.036) Data: 0.001 (0.001) Loss: 0.0157 (0.0158)\n",
      "10-NN,s=0.1: TOP1:  36.125\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 54\n",
      "ResNet1D\n",
      "Epoch: [54][0/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0113 (0.0113)\n",
      "Epoch: [54][1/2459]Time: 0.062 (0.046) Data: 0.001 (0.001) Loss: 0.0182 (0.0148)\n",
      "Epoch: [54][2/2459]Time: 0.069 (0.054) Data: 0.001 (0.001) Loss: 0.0193 (0.0163)\n",
      "Epoch: [54][3/2459]Time: 0.024 (0.046) Data: 0.001 (0.001) Loss: 0.0113 (0.0150)\n",
      "Epoch: [54][4/2459]Time: 0.023 (0.042) Data: 0.001 (0.001) Loss: 0.0136 (0.0147)\n",
      "Epoch: [54][5/2459]Time: 0.024 (0.039) Data: 0.001 (0.001) Loss: 0.0182 (0.0153)\n",
      "Epoch: [54][6/2459]Time: 0.024 (0.037) Data: 0.001 (0.001) Loss: 0.0143 (0.0152)\n",
      "Epoch: [54][7/2459]Time: 0.023 (0.035) Data: 0.001 (0.001) Loss: 0.0204 (0.0158)\n",
      "Epoch: [54][8/2459]Time: 0.024 (0.034) Data: 0.001 (0.001) Loss: 0.0159 (0.0158)\n",
      "Epoch: [54][9/2459]Time: 0.024 (0.033) Data: 0.001 (0.001) Loss: 0.0107 (0.0153)\n",
      "Epoch: [54][10/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0127 (0.0151)\n",
      "Epoch: [54][11/2459]Time: 0.027 (0.032) Data: 0.001 (0.001) Loss: 0.0134 (0.0149)\n",
      "Epoch: [54][12/2459]Time: 0.026 (0.031) Data: 0.001 (0.001) Loss: 0.0122 (0.0147)\n",
      "Epoch: [54][13/2459]Time: 0.047 (0.032) Data: 0.001 (0.001) Loss: 0.0243 (0.0154)\n",
      "Epoch: [54][14/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0174 (0.0155)\n",
      "Epoch: [54][15/2459]Time: 0.069 (0.034) Data: 0.001 (0.001) Loss: 0.0150 (0.0155)\n",
      "Epoch: [54][16/2459]Time: 0.076 (0.036) Data: 0.001 (0.001) Loss: 0.0141 (0.0154)\n",
      "Epoch: [54][17/2459]Time: 0.028 (0.036) Data: 0.001 (0.001) Loss: 0.0126 (0.0153)\n",
      "Epoch: [54][18/2459]Time: 0.028 (0.036) Data: 0.001 (0.001) Loss: 0.0205 (0.0155)\n",
      "Epoch: [54][19/2459]Time: 0.028 (0.035) Data: 0.001 (0.001) Loss: 0.0174 (0.0156)\n",
      "Epoch: [54][20/2459]Time: 0.028 (0.035) Data: 0.001 (0.001) Loss: 0.0138 (0.0155)\n",
      "Epoch: [54][21/2459]Time: 0.028 (0.035) Data: 0.001 (0.001) Loss: 0.0152 (0.0155)\n",
      "Epoch: [54][22/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.0138 (0.0154)\n",
      "Epoch: [54][23/2459]Time: 0.027 (0.034) Data: 0.001 (0.001) Loss: 0.0142 (0.0154)\n",
      "10-NN,s=0.1: TOP1:  35.916666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 55\n",
      "ResNet1D\n",
      "Epoch: [55][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0132 (0.0132)\n",
      "Epoch: [55][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0225 (0.0178)\n",
      "Epoch: [55][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0120 (0.0159)\n",
      "Epoch: [55][3/2459]Time: 0.025 (0.024) Data: 0.001 (0.001) Loss: 0.0109 (0.0146)\n",
      "Epoch: [55][4/2459]Time: 0.045 (0.028) Data: 0.001 (0.001) Loss: 0.0148 (0.0147)\n",
      "Epoch: [55][5/2459]Time: 0.024 (0.027) Data: 0.001 (0.001) Loss: 0.0127 (0.0143)\n",
      "Epoch: [55][6/2459]Time: 0.068 (0.033) Data: 0.001 (0.001) Loss: 0.0151 (0.0145)\n",
      "Epoch: [55][7/2459]Time: 0.076 (0.038) Data: 0.001 (0.001) Loss: 0.0186 (0.0150)\n",
      "Epoch: [55][8/2459]Time: 0.027 (0.037) Data: 0.001 (0.001) Loss: 0.0140 (0.0149)\n",
      "Epoch: [55][9/2459]Time: 0.026 (0.036) Data: 0.001 (0.001) Loss: 0.0127 (0.0147)\n",
      "Epoch: [55][10/2459]Time: 0.026 (0.035) Data: 0.001 (0.001) Loss: 0.0137 (0.0146)\n",
      "Epoch: [55][11/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0172 (0.0148)\n",
      "Epoch: [55][12/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0117 (0.0145)\n",
      "Epoch: [55][13/2459]Time: 0.025 (0.033) Data: 0.001 (0.001) Loss: 0.0122 (0.0144)\n",
      "Epoch: [55][14/2459]Time: 0.028 (0.033) Data: 0.001 (0.001) Loss: 0.0163 (0.0145)\n",
      "Epoch: [55][15/2459]Time: 0.028 (0.033) Data: 0.001 (0.001) Loss: 0.0153 (0.0146)\n",
      "Epoch: [55][16/2459]Time: 0.028 (0.032) Data: 0.001 (0.001) Loss: 0.0188 (0.0148)\n",
      "Epoch: [55][17/2459]Time: 0.028 (0.032) Data: 0.001 (0.001) Loss: 0.0145 (0.0148)\n",
      "Epoch: [55][18/2459]Time: 0.028 (0.032) Data: 0.001 (0.001) Loss: 0.0161 (0.0149)\n",
      "Epoch: [55][19/2459]Time: 0.028 (0.032) Data: 0.001 (0.001) Loss: 0.0128 (0.0148)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [55][20/2459]Time: 0.048 (0.032) Data: 0.001 (0.001) Loss: 0.0143 (0.0147)\n",
      "Epoch: [55][21/2459]Time: 0.028 (0.032) Data: 0.002 (0.001) Loss: 0.0170 (0.0148)\n",
      "Epoch: [55][22/2459]Time: 0.065 (0.034) Data: 0.001 (0.001) Loss: 0.0165 (0.0149)\n",
      "Epoch: [55][23/2459]Time: 0.074 (0.035) Data: 0.001 (0.001) Loss: 0.0142 (0.0149)\n",
      "10-NN,s=0.1: TOP1:  35.791666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 56\n",
      "ResNet1D\n",
      "Epoch: [56][0/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0177 (0.0177)\n",
      "Epoch: [56][1/2459]Time: 0.023 (0.024) Data: 0.001 (0.001) Loss: 0.0158 (0.0167)\n",
      "Epoch: [56][2/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0150 (0.0162)\n",
      "Epoch: [56][3/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0124 (0.0152)\n",
      "Epoch: [56][4/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0179 (0.0158)\n",
      "Epoch: [56][5/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0149 (0.0156)\n",
      "Epoch: [56][6/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0169 (0.0158)\n",
      "Epoch: [56][7/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0147 (0.0157)\n",
      "Epoch: [56][8/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0141 (0.0155)\n",
      "Epoch: [56][9/2459]Time: 0.047 (0.025) Data: 0.001 (0.001) Loss: 0.0124 (0.0152)\n",
      "Epoch: [56][10/2459]Time: 0.023 (0.025) Data: 0.001 (0.001) Loss: 0.0155 (0.0152)\n",
      "Epoch: [56][11/2459]Time: 0.071 (0.029) Data: 0.001 (0.001) Loss: 0.0123 (0.0150)\n",
      "Epoch: [56][12/2459]Time: 0.076 (0.033) Data: 0.001 (0.001) Loss: 0.0176 (0.0152)\n",
      "Epoch: [56][13/2459]Time: 0.025 (0.032) Data: 0.001 (0.001) Loss: 0.0163 (0.0153)\n",
      "Epoch: [56][14/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0137 (0.0152)\n",
      "Epoch: [56][15/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0133 (0.0150)\n",
      "Epoch: [56][16/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0107 (0.0148)\n",
      "Epoch: [56][17/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0123 (0.0147)\n",
      "Epoch: [56][18/2459]Time: 0.024 (0.030) Data: 0.001 (0.001) Loss: 0.0193 (0.0149)\n",
      "Epoch: [56][19/2459]Time: 0.026 (0.030) Data: 0.001 (0.001) Loss: 0.0129 (0.0148)\n",
      "Epoch: [56][20/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0128 (0.0147)\n",
      "Epoch: [56][21/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0164 (0.0148)\n",
      "Epoch: [56][22/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0137 (0.0147)\n",
      "Epoch: [56][23/2459]Time: 0.032 (0.029) Data: 0.001 (0.001) Loss: 0.0138 (0.0147)\n",
      "10-NN,s=0.1: TOP1:  36.291666666666664\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 57\n",
      "ResNet1D\n",
      "Epoch: [57][0/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0126 (0.0126)\n",
      "Epoch: [57][1/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0142 (0.0134)\n",
      "Epoch: [57][2/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0138 (0.0135)\n",
      "Epoch: [57][3/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0296 (0.0175)\n",
      "Epoch: [57][4/2459]Time: 0.028 (0.030) Data: 0.001 (0.001) Loss: 0.0153 (0.0171)\n",
      "Epoch: [57][5/2459]Time: 0.023 (0.028) Data: 0.001 (0.001) Loss: 0.0184 (0.0173)\n",
      "Epoch: [57][6/2459]Time: 0.023 (0.028) Data: 0.001 (0.001) Loss: 0.0113 (0.0165)\n",
      "Epoch: [57][7/2459]Time: 0.027 (0.028) Data: 0.001 (0.001) Loss: 0.0152 (0.0163)\n",
      "Epoch: [57][8/2459]Time: 0.041 (0.029) Data: 0.001 (0.001) Loss: 0.0141 (0.0161)\n",
      "Epoch: [57][9/2459]Time: 0.023 (0.028) Data: 0.001 (0.001) Loss: 0.0153 (0.0160)\n",
      "Epoch: [57][10/2459]Time: 0.068 (0.032) Data: 0.001 (0.001) Loss: 0.0150 (0.0159)\n",
      "Epoch: [57][11/2459]Time: 0.076 (0.036) Data: 0.001 (0.001) Loss: 0.0125 (0.0156)\n",
      "Epoch: [57][12/2459]Time: 0.023 (0.035) Data: 0.001 (0.001) Loss: 0.0115 (0.0153)\n",
      "Epoch: [57][13/2459]Time: 0.023 (0.034) Data: 0.001 (0.001) Loss: 0.0199 (0.0156)\n",
      "Epoch: [57][14/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0160 (0.0156)\n",
      "Epoch: [57][15/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.0118 (0.0154)\n",
      "Epoch: [57][16/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.0162 (0.0155)\n",
      "Epoch: [57][17/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0192 (0.0157)\n",
      "Epoch: [57][18/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0235 (0.0161)\n",
      "Epoch: [57][19/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0250 (0.0165)\n",
      "Epoch: [57][20/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0152 (0.0165)\n",
      "Epoch: [57][21/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0153 (0.0164)\n",
      "Epoch: [57][22/2459]Time: 0.023 (0.029) Data: 0.001 (0.001) Loss: 0.0143 (0.0163)\n",
      "Epoch: [57][23/2459]Time: 0.043 (0.030) Data: 0.001 (0.001) Loss: 0.0134 (0.0162)\n",
      "10-NN,s=0.1: TOP1:  36.25\n",
      "best accuracy: 37.21\n",
      "\n",
      "Epoch: 58\n",
      "ResNet1D\n",
      "Epoch: [58][0/2459]Time: 0.060 (0.060) Data: 0.002 (0.002) Loss: 0.0118 (0.0118)\n",
      "Epoch: [58][1/2459]Time: 0.042 (0.051) Data: 0.002 (0.002) Loss: 0.0125 (0.0122)\n",
      "Epoch: [58][2/2459]Time: 0.063 (0.055) Data: 0.002 (0.002) Loss: 0.0134 (0.0126)\n",
      "Epoch: [58][3/2459]Time: 0.069 (0.058) Data: 0.002 (0.002) Loss: 0.0247 (0.0156)\n",
      "Epoch: [58][4/2459]Time: 0.042 (0.055) Data: 0.002 (0.002) Loss: 0.0164 (0.0158)\n",
      "Epoch: [58][5/2459]Time: 0.046 (0.054) Data: 0.002 (0.002) Loss: 0.0141 (0.0155)\n",
      "Epoch: [58][6/2459]Time: 0.043 (0.052) Data: 0.002 (0.002) Loss: 0.0138 (0.0152)\n",
      "Epoch: [58][7/2459]Time: 0.041 (0.051) Data: 0.002 (0.002) Loss: 0.0212 (0.0160)\n",
      "Epoch: [58][8/2459]Time: 0.035 (0.049) Data: 0.002 (0.002) Loss: 0.0125 (0.0156)\n",
      "Epoch: [58][9/2459]Time: 0.030 (0.047) Data: 0.001 (0.002) Loss: 0.0121 (0.0153)\n",
      "Epoch: [58][10/2459]Time: 0.027 (0.045) Data: 0.001 (0.002) Loss: 0.0116 (0.0149)\n",
      "Epoch: [58][11/2459]Time: 0.026 (0.044) Data: 0.001 (0.002) Loss: 0.0145 (0.0149)\n",
      "Epoch: [58][12/2459]Time: 0.032 (0.043) Data: 0.001 (0.002) Loss: 0.0140 (0.0148)\n",
      "Epoch: [58][13/2459]Time: 0.039 (0.042) Data: 0.001 (0.002) Loss: 0.0133 (0.0147)\n",
      "Epoch: [58][14/2459]Time: 0.023 (0.041) Data: 0.001 (0.002) Loss: 0.0166 (0.0148)\n",
      "Epoch: [58][15/2459]Time: 0.069 (0.043) Data: 0.001 (0.002) Loss: 0.0158 (0.0149)\n",
      "Epoch: [58][16/2459]Time: 0.076 (0.045) Data: 0.001 (0.002) Loss: 0.0194 (0.0152)\n",
      "Epoch: [58][17/2459]Time: 0.023 (0.044) Data: 0.001 (0.002) Loss: 0.0125 (0.0150)\n",
      "Epoch: [58][18/2459]Time: 0.023 (0.043) Data: 0.001 (0.002) Loss: 0.0169 (0.0151)\n",
      "Epoch: [58][19/2459]Time: 0.024 (0.042) Data: 0.001 (0.002) Loss: 0.0141 (0.0151)\n",
      "Epoch: [58][20/2459]Time: 0.023 (0.041) Data: 0.001 (0.001) Loss: 0.0127 (0.0149)\n",
      "Epoch: [58][21/2459]Time: 0.023 (0.040) Data: 0.001 (0.001) Loss: 0.0149 (0.0149)\n",
      "Epoch: [58][22/2459]Time: 0.024 (0.039) Data: 0.001 (0.001) Loss: 0.0140 (0.0149)\n",
      "Epoch: [58][23/2459]Time: 0.023 (0.039) Data: 0.001 (0.001) Loss: 0.0128 (0.0148)\n",
      "10-NN,s=0.1: TOP1:  37.625\n",
      "Saving..\n",
      "best accuracy: 37.62\n",
      "\n",
      "Epoch: 59\n",
      "ResNet1D\n",
      "Epoch: [59][0/2459]Time: 0.024 (0.024) Data: 0.001 (0.001) Loss: 0.0131 (0.0131)\n",
      "Epoch: [59][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0121 (0.0126)\n",
      "Epoch: [59][2/2459]Time: 0.045 (0.030) Data: 0.001 (0.001) Loss: 0.0105 (0.0119)\n",
      "Epoch: [59][3/2459]Time: 0.024 (0.029) Data: 0.001 (0.001) Loss: 0.0153 (0.0128)\n",
      "Epoch: [59][4/2459]Time: 0.069 (0.037) Data: 0.001 (0.001) Loss: 0.0174 (0.0137)\n",
      "Epoch: [59][5/2459]Time: 0.076 (0.043) Data: 0.001 (0.001) Loss: 0.0107 (0.0132)\n",
      "Epoch: [59][6/2459]Time: 0.024 (0.041) Data: 0.001 (0.001) Loss: 0.0297 (0.0156)\n",
      "Epoch: [59][7/2459]Time: 0.025 (0.039) Data: 0.001 (0.001) Loss: 0.0113 (0.0150)\n",
      "Epoch: [59][8/2459]Time: 0.024 (0.037) Data: 0.001 (0.001) Loss: 0.0169 (0.0152)\n",
      "Epoch: [59][9/2459]Time: 0.023 (0.036) Data: 0.001 (0.001) Loss: 0.0113 (0.0148)\n",
      "Epoch: [59][10/2459]Time: 0.023 (0.034) Data: 0.001 (0.001) Loss: 0.0119 (0.0146)\n",
      "Epoch: [59][11/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0135 (0.0145)\n",
      "Epoch: [59][12/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0155 (0.0146)\n",
      "Epoch: [59][13/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0143 (0.0145)\n",
      "Epoch: [59][14/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0161 (0.0146)\n",
      "Epoch: [59][15/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0139 (0.0146)\n",
      "Epoch: [59][16/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0275 (0.0154)\n",
      "Epoch: [59][17/2459]Time: 0.030 (0.030) Data: 0.001 (0.001) Loss: 0.0112 (0.0151)\n",
      "Epoch: [59][18/2459]Time: 0.039 (0.031) Data: 0.001 (0.001) Loss: 0.0141 (0.0151)\n",
      "Epoch: [59][19/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0156 (0.0151)\n",
      "Epoch: [59][20/2459]Time: 0.069 (0.032) Data: 0.001 (0.001) Loss: 0.0129 (0.0150)\n",
      "Epoch: [59][21/2459]Time: 0.076 (0.034) Data: 0.001 (0.001) Loss: 0.0153 (0.0150)\n",
      "Epoch: [59][22/2459]Time: 0.023 (0.034) Data: 0.001 (0.001) Loss: 0.0140 (0.0150)\n",
      "Epoch: [59][23/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0144 (0.0149)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-NN,s=0.1: TOP1:  37.125\n",
      "best accuracy: 37.62\n",
      "\n",
      "Epoch: 60\n",
      "ResNet1D\n",
      "Epoch: [60][0/2459]Time: 0.030 (0.030) Data: 0.002 (0.002) Loss: 0.0188 (0.0188)\n",
      "Epoch: [60][1/2459]Time: 0.025 (0.027) Data: 0.001 (0.002) Loss: 0.0130 (0.0159)\n",
      "Epoch: [60][2/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0110 (0.0143)\n",
      "Epoch: [60][3/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0125 (0.0138)\n",
      "Epoch: [60][4/2459]Time: 0.025 (0.026) Data: 0.001 (0.001) Loss: 0.0145 (0.0140)\n",
      "Epoch: [60][5/2459]Time: 0.024 (0.026) Data: 0.001 (0.001) Loss: 0.0109 (0.0135)\n",
      "Epoch: [60][6/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0112 (0.0131)\n",
      "Epoch: [60][7/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0137 (0.0132)\n",
      "Epoch: [60][8/2459]Time: 0.024 (0.025) Data: 0.001 (0.001) Loss: 0.0147 (0.0134)\n",
      "Epoch: [60][9/2459]Time: 0.048 (0.027) Data: 0.001 (0.001) Loss: 0.0111 (0.0131)\n",
      "Epoch: [60][10/2459]Time: 0.023 (0.027) Data: 0.001 (0.001) Loss: 0.0180 (0.0136)\n",
      "Epoch: [60][11/2459]Time: 0.070 (0.030) Data: 0.001 (0.001) Loss: 0.0130 (0.0135)\n",
      "Epoch: [60][12/2459]Time: 0.076 (0.034) Data: 0.002 (0.001) Loss: 0.0121 (0.0134)\n",
      "Epoch: [60][13/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0130 (0.0134)\n",
      "Epoch: [60][14/2459]Time: 0.023 (0.032) Data: 0.001 (0.001) Loss: 0.0130 (0.0134)\n",
      "Epoch: [60][15/2459]Time: 0.022 (0.032) Data: 0.001 (0.001) Loss: 0.0151 (0.0135)\n",
      "Epoch: [60][16/2459]Time: 0.022 (0.031) Data: 0.001 (0.001) Loss: 0.0195 (0.0138)\n",
      "Epoch: [60][17/2459]Time: 0.022 (0.031) Data: 0.001 (0.001) Loss: 0.0258 (0.0145)\n",
      "Epoch: [60][18/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0140 (0.0145)\n",
      "Epoch: [60][19/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0178 (0.0146)\n",
      "Epoch: [60][20/2459]Time: 0.026 (0.030) Data: 0.001 (0.001) Loss: 0.0138 (0.0146)\n",
      "Epoch: [60][21/2459]Time: 0.029 (0.030) Data: 0.001 (0.001) Loss: 0.0217 (0.0149)\n",
      "Epoch: [60][22/2459]Time: 0.027 (0.030) Data: 0.001 (0.001) Loss: 0.0194 (0.0151)\n",
      "Epoch: [60][23/2459]Time: 0.026 (0.029) Data: 0.001 (0.001) Loss: 0.0128 (0.0150)\n",
      "10-NN,s=0.1: TOP1:  36.0\n",
      "best accuracy: 37.62\n",
      "\n",
      "Epoch: 61\n",
      "ResNet1D\n",
      "Epoch: [61][0/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0114 (0.0114)\n",
      "Epoch: [61][1/2459]Time: 0.023 (0.023) Data: 0.001 (0.001) Loss: 0.0120 (0.0117)\n",
      "Epoch: [61][2/2459]Time: 0.026 (0.024) Data: 0.001 (0.001) Loss: 0.0146 (0.0126)\n",
      "Epoch: [61][3/2459]Time: 0.043 (0.029) Data: 0.001 (0.001) Loss: 0.0108 (0.0122)\n",
      "Epoch: [61][4/2459]Time: 0.023 (0.028) Data: 0.001 (0.001) Loss: 0.0217 (0.0141)\n",
      "Epoch: [61][5/2459]Time: 0.069 (0.034) Data: 0.001 (0.001) Loss: 0.0204 (0.0151)\n",
      "Epoch: [61][6/2459]Time: 0.076 (0.040) Data: 0.001 (0.001) Loss: 0.0179 (0.0155)\n",
      "Epoch: [61][7/2459]Time: 0.023 (0.038) Data: 0.001 (0.001) Loss: 0.0133 (0.0153)\n",
      "Epoch: [61][8/2459]Time: 0.023 (0.037) Data: 0.001 (0.001) Loss: 0.0126 (0.0150)\n",
      "Epoch: [61][9/2459]Time: 0.023 (0.035) Data: 0.001 (0.001) Loss: 0.0151 (0.0150)\n",
      "Epoch: [61][10/2459]Time: 0.024 (0.034) Data: 0.001 (0.001) Loss: 0.0158 (0.0150)\n",
      "Epoch: [61][11/2459]Time: 0.023 (0.033) Data: 0.001 (0.001) Loss: 0.0147 (0.0150)\n",
      "Epoch: [61][12/2459]Time: 0.024 (0.033) Data: 0.001 (0.001) Loss: 0.0118 (0.0148)\n",
      "Epoch: [61][13/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0140 (0.0147)\n",
      "Epoch: [61][14/2459]Time: 0.023 (0.031) Data: 0.001 (0.001) Loss: 0.0167 (0.0148)\n",
      "Epoch: [61][15/2459]Time: 0.028 (0.031) Data: 0.001 (0.001) Loss: 0.0155 (0.0149)\n",
      "Epoch: [61][16/2459]Time: 0.034 (0.031) Data: 0.001 (0.001) Loss: 0.0130 (0.0148)\n",
      "Epoch: [61][17/2459]Time: 0.047 (0.032) Data: 0.001 (0.001) Loss: 0.0127 (0.0147)\n",
      "Epoch: [61][18/2459]Time: 0.024 (0.032) Data: 0.001 (0.001) Loss: 0.0163 (0.0147)\n",
      "Epoch: [61][19/2459]Time: 0.070 (0.034) Data: 0.001 (0.001) Loss: 0.0172 (0.0149)\n",
      "Epoch: [61][20/2459]Time: 0.076 (0.036) Data: 0.001 (0.001) Loss: 0.0110 (0.0147)\n",
      "Epoch: [61][21/2459]Time: 0.027 (0.035) Data: 0.001 (0.001) Loss: 0.0145 (0.0147)\n",
      "Epoch: [61][22/2459]Time: 0.026 (0.035) Data: 0.001 (0.001) Loss: 0.0130 (0.0146)\n",
      "Epoch: [61][23/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0136 (0.0146)\n",
      "10-NN,s=0.1: TOP1:  36.666666666666664\n",
      "best accuracy: 37.62\n",
      "\n",
      "Epoch: 62\n",
      "ResNet1D\n",
      "Epoch: [62][0/2459]Time: 0.025 (0.025) Data: 0.001 (0.001) Loss: 0.0111 (0.0111)\n",
      "Epoch: [62][1/2459]Time: 0.043 (0.034) Data: 0.001 (0.001) Loss: 0.0134 (0.0122)\n",
      "Epoch: [62][2/2459]Time: 0.023 (0.030) Data: 0.001 (0.001) Loss: 0.0157 (0.0134)\n",
      "Epoch: [62][3/2459]Time: 0.069 (0.040) Data: 0.001 (0.001) Loss: 0.0155 (0.0139)\n",
      "Epoch: [62][4/2459]Time: 0.076 (0.047) Data: 0.001 (0.001) Loss: 0.0110 (0.0133)\n",
      "Epoch: [62][5/2459]Time: 0.023 (0.043) Data: 0.001 (0.001) Loss: 0.0173 (0.0140)\n",
      "Epoch: [62][6/2459]Time: 0.023 (0.040) Data: 0.001 (0.001) Loss: 0.0120 (0.0137)\n",
      "Epoch: [62][7/2459]Time: 0.022 (0.038) Data: 0.001 (0.001) Loss: 0.0123 (0.0135)\n",
      "Epoch: [62][8/2459]Time: 0.023 (0.036) Data: 0.001 (0.001) Loss: 0.0151 (0.0137)\n",
      "Epoch: [62][9/2459]Time: 0.028 (0.035) Data: 0.001 (0.001) Loss: 0.0104 (0.0134)\n",
      "Epoch: [62][10/2459]Time: 0.031 (0.035) Data: 0.001 (0.001) Loss: 0.0236 (0.0143)\n",
      "Epoch: [62][11/2459]Time: 0.028 (0.034) Data: 0.001 (0.001) Loss: 0.0202 (0.0148)\n",
      "Epoch: [62][12/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0124 (0.0146)\n",
      "Epoch: [62][13/2459]Time: 0.026 (0.033) Data: 0.001 (0.001) Loss: 0.0153 (0.0147)\n",
      "Epoch: [62][14/2459]Time: 0.025 (0.033) Data: 0.001 (0.001) Loss: 0.0129 (0.0145)\n",
      "Epoch: [62][15/2459]Time: 0.028 (0.032) Data: 0.001 (0.001) Loss: 0.0117 (0.0144)\n",
      "Epoch: [62][16/2459]Time: 0.047 (0.033) Data: 0.001 (0.001) Loss: 0.0189 (0.0146)\n",
      "Epoch: [62][17/2459]Time: 0.037 (0.033) Data: 0.001 (0.001) Loss: 0.0169 (0.0148)\n",
      "Epoch: [62][18/2459]Time: 0.063 (0.035) Data: 0.002 (0.001) Loss: 0.0109 (0.0146)\n",
      "Epoch: [62][19/2459]Time: 0.069 (0.037) Data: 0.002 (0.001) Loss: 0.0116 (0.0144)\n",
      "Epoch: [62][20/2459]Time: 0.035 (0.037) Data: 0.002 (0.001) Loss: 0.0127 (0.0143)\n",
      "Epoch: [62][21/2459]Time: 0.034 (0.036) Data: 0.001 (0.001) Loss: 0.0151 (0.0144)\n",
      "Epoch: [62][22/2459]Time: 0.031 (0.036) Data: 0.001 (0.001) Loss: 0.0174 (0.0145)\n",
      "Epoch: [62][23/2459]Time: 0.029 (0.036) Data: 0.001 (0.001) Loss: 0.0138 (0.0145)\n",
      "10-NN,s=0.1: TOP1:  37.166666666666664\n",
      "best accuracy: 37.62\n",
      "\n",
      "Epoch: 63\n",
      "ResNet1D\n",
      "Epoch: [63][0/2459]Time: 0.030 (0.030) Data: 0.002 (0.002) Loss: 0.0116 (0.0116)\n",
      "Epoch: [63][1/2459]Time: 0.029 (0.029) Data: 0.001 (0.002) Loss: 0.0103 (0.0109)\n",
      "Epoch: [63][2/2459]Time: 0.047 (0.035) Data: 0.001 (0.002) Loss: 0.0134 (0.0117)\n",
      "Epoch: [63][3/2459]Time: 0.027 (0.033) Data: 0.001 (0.002) Loss: 0.0120 (0.0118)\n",
      "Epoch: [63][4/2459]Time: 0.065 (0.040) Data: 0.001 (0.001) Loss: 0.0113 (0.0117)\n",
      "Epoch: [63][5/2459]Time: 0.075 (0.045) Data: 0.001 (0.001) Loss: 0.0123 (0.0118)\n",
      "Epoch: [63][6/2459]Time: 0.028 (0.043) Data: 0.001 (0.001) Loss: 0.0118 (0.0118)\n",
      "Epoch: [63][7/2459]Time: 0.027 (0.041) Data: 0.001 (0.001) Loss: 0.0111 (0.0117)\n",
      "Epoch: [63][8/2459]Time: 0.026 (0.039) Data: 0.001 (0.001) Loss: 0.0142 (0.0120)\n",
      "Epoch: [63][9/2459]Time: 0.027 (0.038) Data: 0.001 (0.001) Loss: 0.0112 (0.0119)\n",
      "Epoch: [63][10/2459]Time: 0.026 (0.037) Data: 0.002 (0.001) Loss: 0.0128 (0.0120)\n",
      "Epoch: [63][11/2459]Time: 0.026 (0.036) Data: 0.001 (0.001) Loss: 0.0104 (0.0119)\n",
      "Epoch: [63][12/2459]Time: 0.027 (0.035) Data: 0.001 (0.001) Loss: 0.0107 (0.0118)\n",
      "Epoch: [63][13/2459]Time: 0.025 (0.035) Data: 0.001 (0.001) Loss: 0.0190 (0.0123)\n",
      "Epoch: [63][14/2459]Time: 0.025 (0.034) Data: 0.001 (0.001) Loss: 0.0113 (0.0122)\n",
      "Epoch: [63][15/2459]Time: 0.026 (0.033) Data: 0.001 (0.001) Loss: 0.0106 (0.0121)\n",
      "Epoch: [63][16/2459]Time: 0.046 (0.034) Data: 0.001 (0.001) Loss: 0.0168 (0.0124)\n",
      "Epoch: [63][17/2459]Time: 0.026 (0.034) Data: 0.001 (0.001) Loss: 0.0133 (0.0124)\n",
      "Epoch: [63][18/2459]Time: 0.065 (0.035) Data: 0.001 (0.001) Loss: 0.0149 (0.0126)\n",
      "Epoch: [63][19/2459]Time: 0.075 (0.037) Data: 0.001 (0.001) Loss: 0.0162 (0.0128)\n",
      "Epoch: [63][20/2459]Time: 0.026 (0.037) Data: 0.001 (0.001) Loss: 0.0117 (0.0127)\n",
      "Epoch: [63][21/2459]Time: 0.026 (0.036) Data: 0.001 (0.001) Loss: 0.0106 (0.0126)\n",
      "Epoch: [63][22/2459]Time: 0.025 (0.036) Data: 0.001 (0.001) Loss: 0.0147 (0.0127)\n",
      "Epoch: [63][23/2459]Time: 0.025 (0.035) Data: 0.002 (0.001) Loss: 0.0108 (0.0126)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    \n",
    "    acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim)\n",
    "    feature_return_switch(model, False)\n",
    "#     writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "    if epoch % 400 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = my_kNN(model, K=[50, 10], sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "#         for num_nn in [50, 10]:\n",
    "#             for sig in [0.1, 0.5]:\n",
    "#                 writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "#                 i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "end = time.time()\n",
    "\n",
    "checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
