{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"/root/data/Multivariate_arff\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 30   # numbers of epochs\n",
    "exp = './resnet1d_exp' # experiments results dir\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 300\n",
    "lr=0.03     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 10\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 12, 15, 18, 21, 24, 27, 30, 33]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncl=6       # number of clusters\n",
    "\n",
    "numc = [i for i in range(6,36,3)]\n",
    "\n",
    "hc=len(numc)      # number of heads\n",
    "\n",
    "# # (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "# CFG = {\n",
    "#     'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "#     'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "# }\n",
    "numc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    data = arff.loadarff(filepath)\n",
    "    data = pd.DataFrame(data[0])\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    return X.values, y.values\n",
    "\n",
    "def load_group(prefix, filenames): \n",
    "    loaded = []\n",
    "    for name in filenames: \n",
    "        X, y = load_file(prefix + \"/\" + name) \n",
    "        loaded.append(X)\n",
    "    # stack group so that features are the 3rd dimension \n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded, y\n",
    "\n",
    "def load_dataset_group(folder_path, ds_path, dims_num, is_train=True, label_enc=False): \n",
    "    filenames = []\n",
    "    if is_train:\n",
    "        postfix = \"_TRAIN.arff\"\n",
    "    else:\n",
    "        postfix = \"_TEST.arff\"\n",
    "    for dim_num in range(1, dims_num + 1):\n",
    "        filenames.append(ds_path + str(dim_num) + postfix)\n",
    "\n",
    "    X, y = load_group(folder_path, filenames)\n",
    "    X = torch.from_numpy(np.array(X, dtype=np.float64))\n",
    "    if label_enc:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        y = torch.from_numpy(np.array(y, dtype=np.int32))\n",
    "    else:\n",
    "        y = torch.from_numpy(np.array(y, dtype=np.int32)) - 1\n",
    "    X = X.transpose(1, 2)\n",
    "    return X, y\n",
    "\n",
    "def load_dataset(folder_path, ds_path, dims_num, label_enc=False): \n",
    "    X_train, y_train = load_dataset_group(folder_path, ds_path, dims_num, \n",
    "                                          is_train=True, label_enc=label_enc) \n",
    "    X_test, y_test = load_dataset_group(folder_path, ds_path, dims_num, \n",
    "                                        is_train=False, label_enc=label_enc)\n",
    "    X_train = F.normalize(X_train, dim=1)\n",
    "    X_test = F.normalize(X_test, dim=1)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt], excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: torch.Size([2459, 6, 36]) \n",
      "y_train.shape: torch.Size([2459])\n",
      "X_test.shape: torch.Size([2466, 6, 36]) \n",
      "y_test.shape: torch.Size([2466])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds_path = \"LSST/LSSTDimension\"\n",
    "dims_num = 6\n",
    "num_classes = 14\n",
    "magic_dim = 2304\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataset(datadir, ds_path, dims_num)\n",
    "# X_train[0], y_train\n",
    "print(\"X_train.shape:\", X_train.shape, \"\\ny_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape, \"\\ny_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2459"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2459, 6, 36])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "__all__ = ['resnetv1','resnetv1_18']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, in_channel=3, width=1, num_classes=[1000]):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.headcount = len(num_classes)\n",
    "        self.base = int(16 * width)\n",
    "        self.features = nn.Sequential(*[                                                     # [100, 8, 18]\n",
    "                            nn.Conv1d(in_channel, 16, kernel_size=3, padding=1, bias=False), # [100, 16, 36]\n",
    "                            nn.BatchNorm1d(16),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            self._make_layer(block, self.base, layers[0]),                   # [100, 16, 36]\n",
    "                            self._make_layer(block, self.base * 2, layers[1]),               # [100, 32, 36]\n",
    "                            self._make_layer(block, self.base * 4, layers[2]),               # [100, 64, 36]\n",
    "                            self._make_layer(block, self.base * 8, layers[3]),               # [100, 128, 36]\n",
    "                            nn.AvgPool1d(2),                                                 # [100, 128, 18]\n",
    "        ])\n",
    "        self.return_features = False\n",
    "    \n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Sequential(nn.Linear(magic_dim, num_classes[0]))\n",
    "        else:\n",
    "            for a, i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(magic_dim, i))\n",
    "            self.top_layer = None\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x.float())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer:\n",
    "                out = self.top_layer(out)\n",
    "            return out\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(out))\n",
    "            return outp\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnetv1_18(num_classes=[1000]):\n",
    "    \"\"\"Encoder for instance discrimination and MoCo\"\"\"\n",
    "    return resnet18(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-3:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "        \n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((N, ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((N, magic_dim)) # knn_dim\n",
    "    \n",
    "    for batch_idx, (data, _, _selected) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data.float())\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy()\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N + batch_idx\n",
    "        if len(optimize_times) > 0 and niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h],\n",
    "                                                     selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if True:\n",
    "#         if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "#             writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*len(trainloader.dataset))\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(net, K, sigma=0.1, dim=128, use_pca=False):\n",
    "    net.eval()\n",
    "    # this part is ugly but made to be backwards-compatible. there was a change in cifar dataset's structure.\n",
    "    trainLabels = y_train\n",
    "    LEN = N\n",
    "    C = trainLabels.max() + 1\n",
    "\n",
    "    trainFeatures = torch.zeros((magic_dim, LEN))  # , device='cuda:0') # dim\n",
    "    normalize = Normalize()\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=False)):\n",
    "        batchSize = batch_size\n",
    "        inputs = inputs.cuda()\n",
    "        features = net(inputs.float())\n",
    "        if not use_pca:\n",
    "            features = normalize(features)\n",
    "        tmp = trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize]\n",
    "        trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu()\n",
    "        \n",
    "    if use_pca:\n",
    "        comps = 128\n",
    "        print('doing PCA with %s components'%comps, end=' ')\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=comps, whiten=False)\n",
    "        trainFeatures = pca.fit_transform(trainFeatures.numpy().T)\n",
    "        trainFeatures = torch.Tensor(trainFeatures)\n",
    "        trainFeatures = normalize(trainFeatures).t()\n",
    "        print('..done')\n",
    "    def eval_k_s(K_,sigma_):\n",
    "        total = 0\n",
    "        top1 = 0.\n",
    "        top5 = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            retrieval_one_hot = torch.zeros(K_, C)# .cuda()\n",
    "            for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=False)):\n",
    "                targets = targets # .cuda(async=True) # or without async for py3.7\n",
    "                inputs = inputs.cuda()\n",
    "                batchSize = batch_size\n",
    "                features = net(inputs)\n",
    "                if use_pca:\n",
    "                    features = pca.transform(features.cpu().numpy())\n",
    "                    features = torch.Tensor(features).cuda()\n",
    "                features = normalize(features).cpu()\n",
    "\n",
    "                dist = torch.mm(features, trainFeatures)\n",
    "\n",
    "                yd, yi = dist.topk(K_, dim=1, largest=True, sorted=True)\n",
    "                candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "                retrieval = torch.gather(candidates, 1, yi).long()\n",
    "\n",
    "                retrieval_one_hot.resize_(batchSize * K_, C).zero_()\n",
    "                retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1.)\n",
    "                \n",
    "                yd_transform = yd.clone().div_(sigma_).exp_()\n",
    "                probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C),\n",
    "                                            yd_transform.view(batchSize, -1, 1)),\n",
    "                                  1)\n",
    "                _, predictions = probs.sort(1, True)\n",
    "\n",
    "                # Find which predictions match the target\n",
    "                correct = predictions.eq(targets.data.view(-1, 1))\n",
    "\n",
    "                top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "                top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "        print(f\"{K_}-NN,s={sigma_}: TOP1: \", top1 * 100. / total)\n",
    "        return top1 / total\n",
    "\n",
    "    if isinstance(K, list):\n",
    "        res = []\n",
    "        for K_ in K:\n",
    "            for sigma_ in sigma:\n",
    "                res.append(eval_k_s(K_, sigma_))\n",
    "        return res\n",
    "    else:\n",
    "        res = eval_k_s(K, sigma)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = resnet18(num_classes=numc, in_channel=dims_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [40.0, 31.0, 30.92, 30.84, 30.77, 30.69, 30.61, 30.53, 30.46, 30.38, 30.3, 30.22, 30.15, 30.07, 29.99, 29.91, 29.83, 29.76, 29.68, 29.6, 29.52, 29.45, 29.37, 29.29, 29.21, 29.14, 29.06, 28.98, 28.9, 28.82, 28.75, 28.67, 28.59, 28.51, 28.44, 28.36, 28.28, 28.2, 28.13, 28.05, 27.97, 27.89, 27.81, 27.74, 27.66, 27.58, 27.5, 27.43, 27.35, 27.27, 27.19, 27.12, 27.04, 26.96, 26.88, 26.8, 26.73, 26.65, 26.57, 26.49, 26.42, 26.34, 26.26, 26.18, 26.11, 26.03, 25.95, 25.87, 25.79, 25.72, 25.64, 25.56, 25.48, 25.41, 25.33, 25.25, 25.17, 25.1, 25.02, 24.94, 24.86, 24.78, 24.71, 24.63, 24.55, 24.47, 24.4, 24.32, 24.24, 24.16, 24.09, 24.01, 23.93, 23.85, 23.77, 23.7, 23.62, 23.54, 23.46, 23.39, 23.31, 23.23, 23.15, 23.08, 23.0, 22.92, 22.84, 22.76, 22.69, 22.61, 22.53, 22.45, 22.38, 22.3, 22.22, 22.14, 22.07, 21.99, 21.91, 21.83, 21.75, 21.68, 21.6, 21.52, 21.44, 21.37, 21.29, 21.21, 21.13, 21.06, 20.98, 20.9, 20.82, 20.74, 20.67, 20.59, 20.51, 20.43, 20.36, 20.28, 20.2, 20.12, 20.05, 19.97, 19.89, 19.81, 19.73, 19.66, 19.58, 19.5, 19.42, 19.35, 19.27, 19.19, 19.11, 19.04, 18.96, 18.88, 18.8, 18.72, 18.65, 18.57, 18.49, 18.41, 18.34, 18.26, 18.18, 18.1, 18.03, 17.95, 17.87, 17.79, 17.71, 17.64, 17.56, 17.48, 17.4, 17.33, 17.25, 17.17, 17.09, 17.02, 16.94, 16.86, 16.78, 16.7, 16.63, 16.55, 16.47, 16.39, 16.32, 16.24, 16.16, 16.08, 16.01, 15.93, 15.85, 15.77, 15.69, 15.62, 15.54, 15.46, 15.38, 15.31, 15.23, 15.15, 15.07, 15.0, 14.92, 14.84, 14.76, 14.68, 14.61, 14.53, 14.45, 14.37, 14.3, 14.22, 14.14, 14.06, 13.99, 13.91, 13.83, 13.75, 13.67, 13.6, 13.52, 13.44, 13.36, 13.29, 13.21, 13.13, 13.05, 12.97, 12.9, 12.82, 12.74, 12.66, 12.59, 12.51, 12.43, 12.35, 12.28, 12.2, 12.12, 12.04, 11.96, 11.89, 11.81, 11.73, 11.65, 11.58, 11.5, 11.42, 11.34, 11.27, 11.19, 11.11, 11.03, 10.95, 10.88, 10.8, 10.72, 10.64, 10.57, 10.49, 10.41, 10.33, 10.26, 10.18, 10.1, 10.02, 9.94, 9.87, 9.79, 9.71, 9.63, 9.56, 9.48, 9.4, 9.32, 9.25, 9.17, 9.09, 9.01, 8.93, 8.86, 8.78, 8.7, 8.62, 8.55, 8.47, 8.39, 8.31, 8.24, 8.16, 8.08, 8.0, 7.92, 7.85, 7.77, 7.69, 7.61, 7.54, 7.46, 7.38, 7.3, 7.23, 7.15, 7.07, 6.99, 6.91, 6.84, 6.76, 6.68, 6.6, 6.53, 6.45, 6.37, 6.29, 6.22, 6.14, 6.06, 5.98, 5.9, 5.83, 5.75, 5.67, 5.59, 5.52, 5.44, 5.36, 5.28, 5.21, 5.13, 5.05, 4.97, 4.89, 4.82, 4.74, 4.66, 4.58, 4.51, 4.43, 4.35, 4.27, 4.2, 4.12, 4.04, 3.96, 3.88, 3.81, 3.73, 3.65, 3.57, 3.5, 3.42, 3.34, 3.26, 3.19, 3.11, 3.03, 2.95, 2.87, 2.8, 2.72, 2.64, 2.56, 2.49, 2.41, 2.33, 2.25, 2.18, 2.1, 2.02, 1.94, 1.86, 1.79, 1.71, 1.63, 1.55, 1.48, 1.4, 1.32, 1.24, 1.17, 1.09, 1.01, 0.93, 0.85, 0.78, 0.7, 0.62, 0.54, 0.47, 0.39, 0.31, 0.23, 0.16, 0.08, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ResNet1D\"\n",
    "writer = SummaryWriter(f'./runs/ERing/{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "ResNet1D\n",
      "error:  0.00014390381574547195 step  41\n",
      "cost:  1.422989671386523\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [0][0/2459]Time: 0.183 (0.183) Data: 0.131 (0.131) Loss: 2.9777 (2.9777)\n",
      "error:  0.00011761677591848319 step  41\n",
      "cost:  1.3514934322383634\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [0][1/2459]Time: 0.197 (0.190) Data: 0.142 (0.136) Loss: 2.9556 (2.9666)\n",
      "error:  0.0003353056875193827 step  41\n",
      "cost:  1.3417622999567298\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [0][2/2459]Time: 0.182 (0.187) Data: 0.133 (0.135) Loss: 2.9266 (2.9533)\n",
      "error:  0.00038117495865697126 step  41\n",
      "cost:  1.3811691434431654\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [0][3/2459]Time: 0.173 (0.184) Data: 0.122 (0.132) Loss: 2.9263 (2.9465)\n",
      "error:  0.0006994712035833794 step  41\n",
      "cost:  1.3581219633147834\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [0][4/2459]Time: 0.208 (0.189) Data: 0.158 (0.137) Loss: 2.9372 (2.9447)\n",
      "error:  9.944607707990105e-05 step  51\n",
      "cost:  1.291638493763578\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [0][5/2459]Time: 0.200 (0.190) Data: 0.150 (0.139) Loss: 2.9433 (2.9444)\n",
      "error:  0.0003156300309693494 step  51\n",
      "cost:  1.2930787774559727\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [0][6/2459]Time: 0.191 (0.191) Data: 0.142 (0.140) Loss: 2.9418 (2.9441)\n",
      "error:  0.0005503760529808055 step  51\n",
      "cost:  1.2905250939404103\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [0][7/2459]Time: 0.209 (0.193) Data: 0.158 (0.142) Loss: 2.9293 (2.9422)\n",
      "10-NN,s=0.1: TOP1:  40.208333333333336\n",
      "Saving..\n",
      "Saving..\n",
      "doing PCA with 128 components ..done\n",
      "50-NN,s=0.1: TOP1:  35.916666666666664\n",
      "50-NN,s=0.5: TOP1:  36.25\n",
      "10-NN,s=0.1: TOP1:  33.958333333333336\n",
      "10-NN,s=0.5: TOP1:  33.833333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 1\n",
      "ResNet1D\n",
      "error:  1.4167963017674978e-05 step  41\n",
      "cost:  1.7211512011041932\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][0/2459]Time: 0.168 (0.168) Data: 0.108 (0.108) Loss: 2.8103 (2.8103)\n",
      "error:  0.000923702790111447 step  31\n",
      "cost:  1.7167372603191078\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [1][1/2459]Time: 0.191 (0.180) Data: 0.141 (0.125) Loss: 2.7929 (2.8016)\n",
      "error:  2.284391087137294e-05 step  41\n",
      "cost:  1.7007379025299982\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][2/2459]Time: 0.180 (0.180) Data: 0.132 (0.127) Loss: 2.7990 (2.8008)\n",
      "error:  1.4858513644022686e-05 step  41\n",
      "cost:  1.6871502744427642\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][3/2459]Time: 0.164 (0.176) Data: 0.118 (0.125) Loss: 2.7704 (2.7932)\n",
      "error:  3.0584177180492667e-05 step  41\n",
      "cost:  1.6797835506265113\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][4/2459]Time: 0.183 (0.177) Data: 0.135 (0.127) Loss: 2.7652 (2.7876)\n",
      "error:  0.00016528014195349439 step  41\n",
      "cost:  1.6845416349485427\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][5/2459]Time: 0.202 (0.182) Data: 0.154 (0.132) Loss: 2.7695 (2.7845)\n",
      "error:  0.0005599215539571212 step  41\n",
      "cost:  1.6677302469019581\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [1][6/2459]Time: 0.180 (0.181) Data: 0.134 (0.132) Loss: 2.7623 (2.7814)\n",
      "error:  5.730698838191106e-05 step  51\n",
      "cost:  1.6351595662978053\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [1][7/2459]Time: 0.183 (0.182) Data: 0.137 (0.132) Loss: 2.7590 (2.7786)\n",
      "10-NN,s=0.1: TOP1:  39.375\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 2\n",
      "ResNet1D\n",
      "error:  0.0004758656306333986 step  31\n",
      "cost:  1.9319330918965025\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [2][0/2459]Time: 0.138 (0.138) Data: 0.097 (0.097) Loss: 2.6325 (2.6325)\n",
      "error:  1.3404692132845675e-05 step  41\n",
      "cost:  1.9160514230818104\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [2][1/2459]Time: 0.209 (0.174) Data: 0.163 (0.130) Loss: 2.6223 (2.6274)\n",
      "error:  4.2488382594574325e-05 step  41\n",
      "cost:  1.9081251043903344\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [2][2/2459]Time: 0.181 (0.176) Data: 0.136 (0.132) Loss: 2.6342 (2.6297)\n",
      "error:  0.00011851816341013066 step  41\n",
      "cost:  1.902678382354359\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [2][3/2459]Time: 0.173 (0.175) Data: 0.128 (0.131) Loss: 2.6229 (2.6280)\n",
      "error:  0.0009436240993969669 step  41\n",
      "cost:  1.8953468635188406\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [2][4/2459]Time: 0.192 (0.179) Data: 0.146 (0.134) Loss: 2.6104 (2.6244)\n",
      "error:  0.0001186014797039947 step  51\n",
      "cost:  1.8574161079002414\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [2][5/2459]Time: 0.175 (0.178) Data: 0.129 (0.133) Loss: 2.6149 (2.6229)\n",
      "error:  0.000193977535890677 step  51\n",
      "cost:  1.826920648262255\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [2][6/2459]Time: 0.183 (0.179) Data: 0.138 (0.134) Loss: 2.5953 (2.6189)\n",
      "error:  0.0007619299331873552 step  51\n",
      "cost:  1.8104109662233872\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [2][7/2459]Time: 0.205 (0.182) Data: 0.159 (0.137) Loss: 2.5829 (2.6144)\n",
      "10-NN,s=0.1: TOP1:  39.875\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 3\n",
      "ResNet1D\n",
      "error:  0.0005486953564730612 step  31\n",
      "cost:  2.1025631762499555\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [3][0/2459]Time: 0.136 (0.136) Data: 0.092 (0.092) Loss: 2.4862 (2.4862)\n",
      "error:  0.0009522761987766915 step  31\n",
      "cost:  2.099348710958954\n",
      "opt took 0.00min,   31iters\n",
      "Epoch: [3][1/2459]Time: 0.190 (0.163) Data: 0.146 (0.119) Loss: 2.4603 (2.4733)\n",
      "error:  3.883408473026595e-05 step  41\n",
      "cost:  2.0981420904562733\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [3][2/2459]Time: 0.209 (0.179) Data: 0.165 (0.134) Loss: 2.4647 (2.4704)\n",
      "error:  0.0003187917388767225 step  41\n",
      "cost:  2.091223546478265\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [3][3/2459]Time: 0.177 (0.178) Data: 0.132 (0.134) Loss: 2.4584 (2.4674)\n",
      "error:  5.527065566401923e-05 step  51\n",
      "cost:  2.0632043397765862\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [3][4/2459]Time: 0.204 (0.183) Data: 0.159 (0.139) Loss: 2.4587 (2.4657)\n",
      "error:  7.256819628298494e-05 step  51\n",
      "cost:  2.0445014007547098\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [3][5/2459]Time: 0.194 (0.185) Data: 0.150 (0.141) Loss: 2.4442 (2.4621)\n",
      "error:  0.0001244678234343377 step  51\n",
      "cost:  2.021663660220463\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [3][6/2459]Time: 0.158 (0.181) Data: 0.115 (0.137) Loss: 2.4518 (2.4606)\n",
      "error:  0.0003905188700047413 step  51\n",
      "cost:  2.0016052678794907\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [3][7/2459]Time: 0.189 (0.182) Data: 0.145 (0.138) Loss: 2.4199 (2.4555)\n",
      "10-NN,s=0.1: TOP1:  38.708333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 4\n",
      "ResNet1D\n",
      "error:  0.0002652485122260906 step  41\n",
      "cost:  2.1874268145284406\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [4][0/2459]Time: 0.208 (0.208) Data: 0.160 (0.160) Loss: 2.3133 (2.3133)\n",
      "error:  0.0002710786575559343 step  41\n",
      "cost:  2.1712686594507775\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [4][1/2459]Time: 0.198 (0.203) Data: 0.154 (0.157) Loss: 2.3138 (2.3135)\n",
      "error:  0.0006928934320845626 step  41\n",
      "cost:  2.1554273989464727\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [4][2/2459]Time: 0.191 (0.199) Data: 0.147 (0.153) Loss: 2.3089 (2.3120)\n",
      "error:  0.00016590967115459598 step  51\n",
      "cost:  2.139430912435259\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [4][3/2459]Time: 0.196 (0.198) Data: 0.152 (0.153) Loss: 2.2845 (2.3051)\n",
      "error:  0.0001715648993638741 step  51\n",
      "cost:  2.1285589906109967\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [4][4/2459]Time: 0.184 (0.195) Data: 0.140 (0.150) Loss: 2.2876 (2.3016)\n",
      "error:  0.0004568785526065655 step  51\n",
      "cost:  2.109649315878048\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [4][5/2459]Time: 0.192 (0.195) Data: 0.148 (0.150) Loss: 2.2898 (2.2996)\n",
      "error:  0.0006326833624655803 step  51\n",
      "cost:  2.08463735203001\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [4][6/2459]Time: 0.184 (0.193) Data: 0.139 (0.148) Loss: 2.2729 (2.2958)\n",
      "error:  0.00017704191626932708 step  61\n",
      "cost:  2.0561328171333844\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [4][7/2459]Time: 0.192 (0.193) Data: 0.148 (0.148) Loss: 2.2540 (2.2906)\n",
      "10-NN,s=0.1: TOP1:  39.916666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 5\n",
      "ResNet1D\n",
      "error:  2.0241695561584727e-05 step  41\n",
      "cost:  2.296823181586188\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [5][0/2459]Time: 0.148 (0.148) Data: 0.103 (0.103) Loss: 2.1487 (2.1487)\n",
      "error:  1.987571122985532e-05 step  41\n",
      "cost:  2.285056153060445\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [5][1/2459]Time: 0.186 (0.167) Data: 0.141 (0.122) Loss: 2.1297 (2.1392)\n",
      "error:  6.56508702839842e-05 step  41\n",
      "cost:  2.280673858585643\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [5][2/2459]Time: 0.211 (0.181) Data: 0.166 (0.137) Loss: 2.1618 (2.1467)\n",
      "error:  0.000296064654400241 step  41\n",
      "cost:  2.2600188914840924\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [5][3/2459]Time: 0.197 (0.185) Data: 0.156 (0.142) Loss: 2.1414 (2.1454)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.0005123365933552559 step  41\n",
      "cost:  2.233628327652542\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [5][4/2459]Time: 0.179 (0.184) Data: 0.135 (0.140) Loss: 2.1247 (2.1413)\n",
      "error:  5.97201586142182e-05 step  51\n",
      "cost:  2.21247800165279\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [5][5/2459]Time: 0.188 (0.185) Data: 0.144 (0.141) Loss: 2.1043 (2.1351)\n",
      "error:  0.00020567449665798154 step  51\n",
      "cost:  2.192598815326588\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [5][6/2459]Time: 0.194 (0.186) Data: 0.150 (0.142) Loss: 2.1380 (2.1355)\n",
      "error:  0.000494425656726305 step  51\n",
      "cost:  2.178580076285469\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [5][7/2459]Time: 0.204 (0.188) Data: 0.159 (0.144) Loss: 2.0922 (2.1301)\n",
      "10-NN,s=0.1: TOP1:  39.666666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 6\n",
      "ResNet1D\n",
      "error:  0.0001878654220642817 step  41\n",
      "cost:  2.3404520958499417\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [6][0/2459]Time: 0.158 (0.158) Data: 0.113 (0.113) Loss: 1.9950 (1.9950)\n",
      "error:  0.0002657567095702973 step  41\n",
      "cost:  2.3338403035514292\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [6][1/2459]Time: 0.213 (0.185) Data: 0.169 (0.141) Loss: 1.9784 (1.9867)\n",
      "error:  3.6038567534646404e-05 step  51\n",
      "cost:  2.3158944443104024\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [6][2/2459]Time: 0.195 (0.189) Data: 0.150 (0.144) Loss: 1.9692 (1.9808)\n",
      "error:  0.0006745167565205268 step  41\n",
      "cost:  2.3077056362666033\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [6][3/2459]Time: 0.185 (0.188) Data: 0.140 (0.143) Loss: 1.9587 (1.9753)\n",
      "error:  0.0007699293968548293 step  41\n",
      "cost:  2.293625626754127\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [6][4/2459]Time: 0.206 (0.191) Data: 0.161 (0.147) Loss: 1.9766 (1.9756)\n",
      "error:  0.0005803695370827366 step  41\n",
      "cost:  2.2664529811860827\n",
      "opt took 0.00min,   41iters\n",
      "Epoch: [6][5/2459]Time: 0.196 (0.192) Data: 0.156 (0.148) Loss: 1.9427 (1.9701)\n",
      "error:  5.7485411675828324e-05 step  51\n",
      "cost:  2.239093041712477\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [6][6/2459]Time: 0.199 (0.193) Data: 0.155 (0.149) Loss: 1.9503 (1.9673)\n",
      "error:  0.0006832007889938652 step  51\n",
      "cost:  2.210371267951867\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [6][7/2459]Time: 0.205 (0.194) Data: 0.161 (0.151) Loss: 1.9204 (1.9614)\n",
      "10-NN,s=0.1: TOP1:  39.125\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 7\n",
      "ResNet1D\n",
      "error:  4.4026144071840356e-05 step  51\n",
      "cost:  2.3533603163724495\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [7][0/2459]Time: 0.212 (0.212) Data: 0.167 (0.167) Loss: 1.8412 (1.8412)\n",
      "error:  5.943619246717713e-05 step  51\n",
      "cost:  2.34725894318115\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [7][1/2459]Time: 0.220 (0.216) Data: 0.175 (0.171) Loss: 1.8295 (1.8354)\n",
      "error:  0.00012170303457537113 step  51\n",
      "cost:  2.3397658781948114\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [7][2/2459]Time: 0.191 (0.208) Data: 0.147 (0.163) Loss: 1.8514 (1.8407)\n",
      "error:  0.00011716382909343626 step  51\n",
      "cost:  2.318364153209476\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [7][3/2459]Time: 0.170 (0.198) Data: 0.126 (0.154) Loss: 1.8326 (1.8387)\n",
      "error:  0.00027404886579152077 step  51\n",
      "cost:  2.2958940728134887\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [7][4/2459]Time: 0.197 (0.198) Data: 0.153 (0.154) Loss: 1.7834 (1.8276)\n",
      "error:  0.0005342482799252091 step  51\n",
      "cost:  2.272509830587771\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [7][5/2459]Time: 0.192 (0.197) Data: 0.148 (0.153) Loss: 1.7933 (1.8219)\n",
      "error:  0.0006443728495307832 step  51\n",
      "cost:  2.2479645987304195\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [7][6/2459]Time: 0.188 (0.196) Data: 0.144 (0.151) Loss: 1.7901 (1.8174)\n",
      "error:  6.92936010372458e-05 step  61\n",
      "cost:  2.229349003584007\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [7][7/2459]Time: 0.199 (0.196) Data: 0.155 (0.152) Loss: 1.7663 (1.8110)\n",
      "10-NN,s=0.1: TOP1:  39.25\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 8\n",
      "ResNet1D\n",
      "error:  8.477625489689e-05 step  51\n",
      "cost:  2.359462358068416\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][0/2459]Time: 0.150 (0.150) Data: 0.106 (0.106) Loss: 1.7007 (1.7007)\n",
      "error:  5.406353150538834e-05 step  51\n",
      "cost:  2.3572329282648607\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][1/2459]Time: 0.207 (0.179) Data: 0.163 (0.135) Loss: 1.6943 (1.6975)\n",
      "error:  9.506657631619397e-05 step  51\n",
      "cost:  2.3510773732991703\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][2/2459]Time: 0.235 (0.198) Data: 0.192 (0.154) Loss: 1.6499 (1.6816)\n",
      "error:  0.00017256372884222326 step  51\n",
      "cost:  2.3298217882328824\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][3/2459]Time: 0.198 (0.198) Data: 0.155 (0.154) Loss: 1.6358 (1.6702)\n",
      "error:  0.0003273823891982719 step  51\n",
      "cost:  2.3033324715862116\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][4/2459]Time: 0.183 (0.195) Data: 0.139 (0.151) Loss: 1.6807 (1.6723)\n",
      "error:  0.0003678711946992763 step  51\n",
      "cost:  2.26979963313733\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][5/2459]Time: 0.194 (0.195) Data: 0.149 (0.151) Loss: 1.6486 (1.6683)\n",
      "error:  0.00077103031453718 step  51\n",
      "cost:  2.245501487910543\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [8][6/2459]Time: 0.183 (0.193) Data: 0.139 (0.149) Loss: 1.6368 (1.6638)\n",
      "error:  0.00013263443045052892 step  61\n",
      "cost:  2.223508331764329\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [8][7/2459]Time: 0.190 (0.193) Data: 0.145 (0.149) Loss: 1.6225 (1.6587)\n",
      "10-NN,s=0.1: TOP1:  39.333333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 9\n",
      "ResNet1D\n",
      "error:  0.0002451361607423985 step  51\n",
      "cost:  2.3472382722749363\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [9][0/2459]Time: 0.151 (0.151) Data: 0.106 (0.106) Loss: 1.5431 (1.5431)\n",
      "error:  0.00022889050253616094 step  51\n",
      "cost:  2.3347387276211307\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [9][1/2459]Time: 0.198 (0.174) Data: 0.154 (0.130) Loss: 1.5196 (1.5313)\n",
      "error:  0.0008218313169631175 step  51\n",
      "cost:  2.321767265207946\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [9][2/2459]Time: 0.196 (0.182) Data: 0.152 (0.137) Loss: 1.4893 (1.5173)\n",
      "error:  0.0006956492300497219 step  51\n",
      "cost:  2.3053185985098503\n",
      "opt took 0.00min,   51iters\n",
      "Epoch: [9][3/2459]Time: 0.199 (0.186) Data: 0.155 (0.142) Loss: 1.5019 (1.5135)\n",
      "error:  0.00015828324483724288 step  61\n",
      "cost:  2.295445278270615\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [9][4/2459]Time: 0.207 (0.190) Data: 0.164 (0.146) Loss: 1.5037 (1.5115)\n",
      "error:  0.0006342634525005053 step  61\n",
      "cost:  2.263304740696789\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [9][5/2459]Time: 0.193 (0.191) Data: 0.149 (0.147) Loss: 1.5044 (1.5103)\n",
      "error:  0.00037849887690399875 step  61\n",
      "cost:  2.2358543661165746\n",
      "opt took 0.00min,   61iters\n",
      "Epoch: [9][6/2459]Time: 0.197 (0.192) Data: 0.153 (0.148) Loss: 1.4650 (1.5038)\n",
      "error:  0.00018797215066757023 step  71\n",
      "cost:  2.2080118854641295\n",
      "opt took 0.00min,   71iters\n",
      "Epoch: [9][7/2459]Time: 0.199 (0.193) Data: 0.155 (0.149) Loss: 1.4988 (1.5032)\n",
      "10-NN,s=0.1: TOP1:  39.375\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 10\n",
      "ResNet1D\n",
      "error:  0.0006476144470711764 step  251\n",
      "cost:  0.5497266776028641\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [10][0/2459]Time: 0.232 (0.232) Data: 0.188 (0.188) Loss: 1.3878 (1.3878)\n",
      "error:  0.0007680776004489465 step  201\n",
      "cost:  0.5385213957689088\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [10][1/2459]Time: 0.210 (0.221) Data: 0.166 (0.177) Loss: 1.3885 (1.3882)\n",
      "error:  0.000729217898924106 step  241\n",
      "cost:  0.5394247691460422\n",
      "opt took 0.00min,  241iters\n",
      "Epoch: [10][2/2459]Time: 0.243 (0.228) Data: 0.184 (0.179) Loss: 1.3927 (1.3897)\n",
      "error:  0.0007301378873151299 step  261\n",
      "cost:  0.5434158466287701\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [10][3/2459]Time: 0.195 (0.220) Data: 0.152 (0.172) Loss: 1.3769 (1.3865)\n",
      "error:  0.0007943536396484152 step  241\n",
      "cost:  0.538529901543584\n",
      "opt took 0.00min,  241iters\n",
      "Epoch: [10][4/2459]Time: 0.182 (0.212) Data: 0.138 (0.165) Loss: 1.3831 (1.3858)\n",
      "error:  0.0008612861473583644 step  211\n",
      "cost:  0.5241970167444262\n",
      "opt took 0.00min,  211iters\n",
      "Epoch: [10][5/2459]Time: 0.220 (0.214) Data: 0.177 (0.167) Loss: 1.3288 (1.3763)\n",
      "error:  0.0006337257416931807 step  251\n",
      "cost:  0.5092010832988003\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [10][6/2459]Time: 0.181 (0.209) Data: 0.137 (0.163) Loss: 1.3408 (1.3712)\n",
      "error:  0.0008819288973687112 step  211\n",
      "cost:  0.5095590514199075\n",
      "opt took 0.00min,  211iters\n",
      "Epoch: [10][7/2459]Time: 0.169 (0.204) Data: 0.125 (0.158) Loss: 1.3453 (1.3680)\n",
      "10-NN,s=0.1: TOP1:  39.041666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 11\n",
      "ResNet1D\n",
      "error:  0.0006265126713735913 step  191\n",
      "cost:  0.758011995590898\n",
      "opt took 0.00min,  191iters\n",
      "Epoch: [11][0/2459]Time: 0.185 (0.185) Data: 0.141 (0.141) Loss: 1.2712 (1.2712)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.0009311308801263074 step  231\n",
      "cost:  0.7627133489455441\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [11][1/2459]Time: 0.223 (0.204) Data: 0.176 (0.158) Loss: 1.2681 (1.2696)\n",
      "error:  0.0006483602289745205 step  241\n",
      "cost:  0.7676014003269278\n",
      "opt took 0.00min,  241iters\n",
      "Epoch: [11][2/2459]Time: 0.205 (0.205) Data: 0.162 (0.160) Loss: 1.2848 (1.2747)\n",
      "error:  0.0007546185892404189 step  251\n",
      "cost:  0.7452092767356919\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [11][3/2459]Time: 0.226 (0.210) Data: 0.183 (0.165) Loss: 1.2383 (1.2656)\n",
      "error:  0.0006277058043913097 step  231\n",
      "cost:  0.733904594438344\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [11][4/2459]Time: 0.223 (0.213) Data: 0.180 (0.168) Loss: 1.2377 (1.2600)\n",
      "error:  0.000846186787812675 step  201\n",
      "cost:  0.7200015216146677\n",
      "opt took 0.00min,  201iters\n",
      "Epoch: [11][5/2459]Time: 0.221 (0.214) Data: 0.178 (0.170) Loss: 1.2291 (1.2549)\n",
      "error:  0.0006605912591582941 step  231\n",
      "cost:  0.7301379046815237\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [11][6/2459]Time: 0.278 (0.223) Data: 0.234 (0.179) Loss: 1.2165 (1.2494)\n",
      "error:  0.0005886211123996565 step  231\n",
      "cost:  0.7385947621243818\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [11][7/2459]Time: 0.213 (0.222) Data: 0.168 (0.178) Loss: 1.2030 (1.2436)\n",
      "10-NN,s=0.1: TOP1:  39.083333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 12\n",
      "ResNet1D\n",
      "error:  0.0007641958304666074 step  291\n",
      "cost:  0.7720021884044327\n",
      "opt took 0.00min,  291iters\n",
      "Epoch: [12][0/2459]Time: 0.252 (0.252) Data: 0.209 (0.209) Loss: 1.1312 (1.1312)\n",
      "error:  0.000988489131371062 step  261\n",
      "cost:  0.7584557055540809\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [12][1/2459]Time: 0.210 (0.231) Data: 0.166 (0.187) Loss: 1.1413 (1.1363)\n",
      "error:  0.0008819282347710677 step  311\n",
      "cost:  0.751953568223609\n",
      "opt took 0.00min,  311iters\n",
      "Epoch: [12][2/2459]Time: 0.314 (0.259) Data: 0.259 (0.211) Loss: 1.1416 (1.1380)\n",
      "error:  0.0008154392266944699 step  281\n",
      "cost:  0.7473054463638181\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [12][3/2459]Time: 0.211 (0.247) Data: 0.169 (0.201) Loss: 1.1498 (1.1410)\n",
      "error:  0.0007609489211771736 step  261\n",
      "cost:  0.7369197081031374\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [12][4/2459]Time: 0.214 (0.240) Data: 0.170 (0.194) Loss: 1.1311 (1.1390)\n",
      "error:  0.000900255562654273 step  391\n",
      "cost:  0.7277578444648468\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [12][5/2459]Time: 0.240 (0.240) Data: 0.196 (0.195) Loss: 1.1124 (1.1346)\n",
      "error:  0.0009219882854474282 step  341\n",
      "cost:  0.7157517469539068\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [12][6/2459]Time: 0.283 (0.246) Data: 0.230 (0.200) Loss: 1.1023 (1.1299)\n",
      "error:  0.0007514956818943652 step  341\n",
      "cost:  0.7136729119242272\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [12][7/2459]Time: 0.234 (0.245) Data: 0.189 (0.198) Loss: 1.1202 (1.1287)\n",
      "10-NN,s=0.1: TOP1:  39.291666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 13\n",
      "ResNet1D\n",
      "error:  0.000611660377927592 step  231\n",
      "cost:  0.8620998388753229\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [13][0/2459]Time: 0.167 (0.167) Data: 0.123 (0.123) Loss: 1.0658 (1.0658)\n",
      "error:  0.0008669244389725161 step  261\n",
      "cost:  0.8486722896113406\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [13][1/2459]Time: 0.218 (0.192) Data: 0.174 (0.148) Loss: 1.0154 (1.0406)\n",
      "error:  0.0009869709684395822 step  241\n",
      "cost:  0.8464254478108857\n",
      "opt took 0.00min,  241iters\n",
      "Epoch: [13][2/2459]Time: 0.216 (0.200) Data: 0.172 (0.156) Loss: 1.0353 (1.0388)\n",
      "error:  0.0007342929448922453 step  251\n",
      "cost:  0.8374141519546339\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [13][3/2459]Time: 0.201 (0.200) Data: 0.157 (0.156) Loss: 1.0406 (1.0393)\n",
      "error:  0.0006662897805532753 step  241\n",
      "cost:  0.828338868825423\n",
      "opt took 0.00min,  241iters\n",
      "Epoch: [13][4/2459]Time: 0.238 (0.208) Data: 0.194 (0.164) Loss: 1.0357 (1.0386)\n",
      "error:  0.000739562992395193 step  261\n",
      "cost:  0.8169958618356608\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [13][5/2459]Time: 0.228 (0.211) Data: 0.184 (0.167) Loss: 1.0069 (1.0333)\n",
      "error:  0.0007801753365014319 step  231\n",
      "cost:  0.8110829465197984\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [13][6/2459]Time: 0.215 (0.212) Data: 0.171 (0.168) Loss: 1.0165 (1.0309)\n",
      "error:  0.0006873635617576923 step  241\n",
      "cost:  0.8005789087634569\n",
      "opt took 0.00min,  241iters\n",
      "Epoch: [13][7/2459]Time: 0.233 (0.215) Data: 0.189 (0.170) Loss: 1.0235 (1.0300)\n",
      "10-NN,s=0.1: TOP1:  39.041666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 14\n",
      "ResNet1D\n",
      "error:  0.0007585655900133048 step  321\n",
      "cost:  0.841640441151417\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [14][0/2459]Time: 0.188 (0.188) Data: 0.144 (0.144) Loss: 0.9541 (0.9541)\n",
      "error:  0.0008731347485151364 step  321\n",
      "cost:  0.830001654484053\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [14][1/2459]Time: 0.235 (0.212) Data: 0.191 (0.168) Loss: 0.9347 (0.9444)\n",
      "error:  0.0008596368163411583 step  351\n",
      "cost:  0.8119980401492887\n",
      "opt took 0.00min,  351iters\n",
      "Epoch: [14][2/2459]Time: 0.238 (0.220) Data: 0.193 (0.176) Loss: 0.9240 (0.9376)\n",
      "error:  0.0009023556197296223 step  281\n",
      "cost:  0.8052951028772526\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [14][3/2459]Time: 0.213 (0.219) Data: 0.169 (0.174) Loss: 0.9480 (0.9402)\n",
      "error:  0.0007510083922617206 step  281\n",
      "cost:  0.7969270638259183\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [14][4/2459]Time: 0.236 (0.222) Data: 0.192 (0.178) Loss: 0.9423 (0.9406)\n",
      "error:  0.0007512327373460082 step  251\n",
      "cost:  0.7906900066717629\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [14][5/2459]Time: 0.224 (0.222) Data: 0.180 (0.178) Loss: 0.9282 (0.9385)\n",
      "error:  0.000809314489724211 step  251\n",
      "cost:  0.7903899348160974\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [14][6/2459]Time: 0.235 (0.224) Data: 0.179 (0.178) Loss: 0.8985 (0.9328)\n",
      "error:  0.0009071258689042905 step  331\n",
      "cost:  0.7861201345473915\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [14][7/2459]Time: 0.250 (0.227) Data: 0.204 (0.181) Loss: 0.9142 (0.9305)\n",
      "10-NN,s=0.1: TOP1:  39.166666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 15\n",
      "ResNet1D\n",
      "error:  0.000720707585462721 step  321\n",
      "cost:  0.8968697395436183\n",
      "opt took 0.00min,  321iters\n",
      "Epoch: [15][0/2459]Time: 0.197 (0.197) Data: 0.151 (0.151) Loss: 0.8924 (0.8924)\n",
      "error:  0.0009856327107018448 step  271\n",
      "cost:  0.8881550705431751\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [15][1/2459]Time: 0.213 (0.205) Data: 0.169 (0.160) Loss: 0.8901 (0.8913)\n",
      "error:  0.000822099984061575 step  301\n",
      "cost:  0.8779080073831964\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [15][2/2459]Time: 0.211 (0.207) Data: 0.167 (0.162) Loss: 0.8706 (0.8844)\n",
      "error:  0.000813866661264151 step  301\n",
      "cost:  0.8725345130817772\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [15][3/2459]Time: 0.282 (0.226) Data: 0.237 (0.181) Loss: 0.8583 (0.8779)\n",
      "error:  0.0008240526043294949 step  261\n",
      "cost:  0.8578971982635623\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [15][4/2459]Time: 0.247 (0.230) Data: 0.190 (0.183) Loss: 0.8529 (0.8729)\n",
      "error:  0.000840586812717703 step  241\n",
      "cost:  0.851103316279826\n",
      "opt took 0.00min,  241iters\n",
      "Epoch: [15][5/2459]Time: 0.222 (0.229) Data: 0.178 (0.182) Loss: 0.8359 (0.8667)\n",
      "error:  0.0007536395180497424 step  271\n",
      "cost:  0.8499745483329418\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [15][6/2459]Time: 0.290 (0.237) Data: 0.247 (0.191) Loss: 0.8524 (0.8647)\n",
      "error:  0.0007640708752753023 step  271\n",
      "cost:  0.8453513113527399\n",
      "opt took 0.00min,  271iters\n",
      "Epoch: [15][7/2459]Time: 0.300 (0.245) Data: 0.256 (0.199) Loss: 0.8395 (0.8615)\n",
      "10-NN,s=0.1: TOP1:  38.166666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 16\n",
      "ResNet1D\n",
      "error:  0.0007491269264839895 step  301\n",
      "cost:  0.9102184438553051\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [16][0/2459]Time: 0.187 (0.187) Data: 0.143 (0.143) Loss: 0.7700 (0.7700)\n",
      "error:  0.0007943500625623434 step  281\n",
      "cost:  0.9077906735454551\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [16][1/2459]Time: 0.250 (0.219) Data: 0.209 (0.176) Loss: 0.8126 (0.7913)\n",
      "error:  0.0007365224280511118 step  261\n",
      "cost:  0.9031217332796031\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [16][2/2459]Time: 0.239 (0.225) Data: 0.194 (0.182) Loss: 0.7916 (0.7914)\n",
      "error:  0.0008180546556484369 step  251\n",
      "cost:  0.8984290532751907\n",
      "opt took 0.00min,  251iters\n",
      "Epoch: [16][3/2459]Time: 0.287 (0.241) Data: 0.243 (0.197) Loss: 0.7709 (0.7863)\n",
      "error:  0.000893939607293559 step  231\n",
      "cost:  0.8936525321553964\n",
      "opt took 0.00min,  231iters\n",
      "Epoch: [16][4/2459]Time: 0.238 (0.240) Data: 0.194 (0.197) Loss: 0.7479 (0.7786)\n",
      "error:  0.0009622153502255593 step  261\n",
      "cost:  0.8706959912425128\n",
      "opt took 0.00min,  261iters\n",
      "Epoch: [16][5/2459]Time: 0.226 (0.238) Data: 0.182 (0.194) Loss: 0.7888 (0.7803)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.0007040960317433731 step  281\n",
      "cost:  0.8577186728646817\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [16][6/2459]Time: 0.237 (0.238) Data: 0.193 (0.194) Loss: 0.7766 (0.7798)\n",
      "error:  0.0009844540596781748 step  281\n",
      "cost:  0.857054348531436\n",
      "opt took 0.00min,  281iters\n",
      "Epoch: [16][7/2459]Time: 0.248 (0.239) Data: 0.203 (0.195) Loss: 0.7752 (0.7792)\n",
      "10-NN,s=0.1: TOP1:  38.166666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 17\n",
      "ResNet1D\n",
      "error:  0.0008200405316300952 step  341\n",
      "cost:  0.8976795284115304\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [17][0/2459]Time: 0.267 (0.267) Data: 0.227 (0.227) Loss: 0.7024 (0.7024)\n",
      "error:  0.000787579960255802 step  311\n",
      "cost:  0.8878365890278975\n",
      "opt took 0.00min,  311iters\n",
      "Epoch: [17][1/2459]Time: 0.290 (0.278) Data: 0.243 (0.235) Loss: 0.7142 (0.7083)\n",
      "error:  0.0008342397954753666 step  351\n",
      "cost:  0.879803171008417\n",
      "opt took 0.00min,  351iters\n",
      "Epoch: [17][2/2459]Time: 0.250 (0.269) Data: 0.203 (0.224) Loss: 0.7115 (0.7093)\n",
      "error:  0.0009532188837984013 step  331\n",
      "cost:  0.8746051657636923\n",
      "opt took 0.00min,  331iters\n",
      "Epoch: [17][3/2459]Time: 0.299 (0.276) Data: 0.251 (0.231) Loss: 0.7269 (0.7137)\n",
      "error:  0.000843930099544643 step  341\n",
      "cost:  0.8693985562986805\n",
      "opt took 0.00min,  341iters\n",
      "Epoch: [17][4/2459]Time: 0.250 (0.271) Data: 0.204 (0.226) Loss: 0.7092 (0.7128)\n",
      "error:  0.000813638779691872 step  371\n",
      "cost:  0.8610097170340671\n",
      "opt took 0.00min,  371iters\n",
      "Epoch: [17][5/2459]Time: 0.277 (0.272) Data: 0.230 (0.226) Loss: 0.7033 (0.7112)\n",
      "error:  0.0007753381836159701 step  381\n",
      "cost:  0.8550891168091628\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [17][6/2459]Time: 0.252 (0.269) Data: 0.205 (0.223) Loss: 0.7005 (0.7097)\n",
      "error:  0.0009672360043206929 step  301\n",
      "cost:  0.8351022584306881\n",
      "opt took 0.00min,  301iters\n",
      "Epoch: [17][7/2459]Time: 0.235 (0.265) Data: 0.188 (0.219) Loss: 0.6948 (0.7078)\n",
      "10-NN,s=0.1: TOP1:  38.416666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 18\n",
      "ResNet1D\n",
      "error:  0.0009794707082160237 step  451\n",
      "cost:  0.8668051534335321\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [18][0/2459]Time: 0.207 (0.207) Data: 0.160 (0.160) Loss: 0.6908 (0.6908)\n",
      "error:  0.0008913005533540685 step  391\n",
      "cost:  0.8673026222885761\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [18][1/2459]Time: 0.311 (0.259) Data: 0.264 (0.212) Loss: 0.6765 (0.6836)\n",
      "error:  0.0009519801700383912 step  411\n",
      "cost:  0.8633172688243808\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [18][2/2459]Time: 0.251 (0.257) Data: 0.204 (0.210) Loss: 0.6767 (0.6813)\n",
      "error:  0.0007987368024938712 step  421\n",
      "cost:  0.8513312810622496\n",
      "opt took 0.00min,  421iters\n",
      "Epoch: [18][3/2459]Time: 0.357 (0.282) Data: 0.309 (0.235) Loss: 0.6855 (0.6824)\n",
      "error:  0.0009521035926971511 step  421\n",
      "cost:  0.8515772681815247\n",
      "opt took 0.00min,  421iters\n",
      "Epoch: [18][4/2459]Time: 0.268 (0.279) Data: 0.220 (0.232) Loss: 0.6542 (0.6768)\n",
      "error:  0.0009652915092623893 step  381\n",
      "cost:  0.8339756604414317\n",
      "opt took 0.00min,  381iters\n",
      "Epoch: [18][5/2459]Time: 0.300 (0.282) Data: 0.252 (0.235) Loss: 0.6393 (0.6705)\n",
      "error:  0.0008797274802795352 step  421\n",
      "cost:  0.8174349505506708\n",
      "opt took 0.00min,  421iters\n",
      "Epoch: [18][6/2459]Time: 0.270 (0.280) Data: 0.223 (0.233) Loss: 0.6336 (0.6652)\n",
      "error:  0.0007965460163814608 step  391\n",
      "cost:  0.8102039139090346\n",
      "opt took 0.00min,  391iters\n",
      "Epoch: [18][7/2459]Time: 0.235 (0.275) Data: 0.187 (0.228) Loss: 0.6456 (0.6628)\n",
      "10-NN,s=0.1: TOP1:  39.666666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 19\n",
      "ResNet1D\n",
      "error:  0.0008479770137326259 step  401\n",
      "cost:  0.894875590548099\n",
      "opt took 0.00min,  401iters\n",
      "Epoch: [19][0/2459]Time: 0.210 (0.210) Data: 0.164 (0.164) Loss: 0.5951 (0.5951)\n",
      "error:  0.0009473848043674016 step  411\n",
      "cost:  0.8927583239740464\n",
      "opt took 0.00min,  411iters\n",
      "Epoch: [19][1/2459]Time: 0.234 (0.222) Data: 0.189 (0.176) Loss: 0.5880 (0.5915)\n",
      "error:  0.0008567676240320843 step  431\n",
      "cost:  0.8756112069682167\n",
      "opt took 0.00min,  431iters\n",
      "Epoch: [19][2/2459]Time: 0.244 (0.229) Data: 0.198 (0.183) Loss: 0.6157 (0.5996)\n",
      "error:  0.0009430266685112043 step  541\n",
      "cost:  0.8534673888263359\n",
      "opt took 0.00min,  541iters\n",
      "Epoch: [19][3/2459]Time: 0.259 (0.237) Data: 0.213 (0.191) Loss: 0.5960 (0.5987)\n",
      "error:  0.0008931933164246786 step  581\n",
      "cost:  0.8453541006525186\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [19][4/2459]Time: 0.313 (0.252) Data: 0.268 (0.206) Loss: 0.6226 (0.6035)\n",
      "error:  0.0009709714240413447 step  451\n",
      "cost:  0.8520236368094215\n",
      "opt took 0.00min,  451iters\n",
      "Epoch: [19][5/2459]Time: 0.236 (0.249) Data: 0.191 (0.204) Loss: 0.5828 (0.6000)\n",
      "error:  0.000906094906456878 step  521\n",
      "cost:  0.8303838818002651\n",
      "opt took 0.00min,  521iters\n",
      "Epoch: [19][6/2459]Time: 0.245 (0.249) Data: 0.199 (0.203) Loss: 0.6014 (0.6002)\n",
      "error:  0.0009013874730668947 step  531\n",
      "cost:  0.8182694704500921\n",
      "opt took 0.00min,  531iters\n",
      "Epoch: [19][7/2459]Time: 0.289 (0.254) Data: 0.241 (0.208) Loss: 0.5999 (0.6002)\n",
      "10-NN,s=0.1: TOP1:  39.166666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 20\n",
      "ResNet1D\n",
      "error:  0.0008870376039272765 step  511\n",
      "cost:  0.24734961856169824\n",
      "opt took 0.00min,  511iters\n",
      "Epoch: [20][0/2459]Time: 0.192 (0.192) Data: 0.147 (0.147) Loss: 0.5600 (0.5600)\n",
      "error:  0.0009459084807935669 step  861\n",
      "cost:  0.2555831780626174\n",
      "opt took 0.00min,  861iters\n",
      "Epoch: [20][1/2459]Time: 0.272 (0.232) Data: 0.224 (0.186) Loss: 0.5548 (0.5574)\n",
      "error:  0.0009745787988139076 step  951\n",
      "cost:  0.25670923551746605\n",
      "opt took 0.00min,  951iters\n",
      "Epoch: [20][2/2459]Time: 0.323 (0.262) Data: 0.275 (0.216) Loss: 0.5668 (0.5605)\n",
      "error:  0.000995855522623934 step  891\n",
      "cost:  0.25074482342505705\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [20][3/2459]Time: 0.305 (0.273) Data: 0.260 (0.227) Loss: 0.5441 (0.5564)\n",
      "error:  0.0008391471004649542 step  511\n",
      "cost:  0.24444212646477495\n",
      "opt took 0.00min,  511iters\n",
      "Epoch: [20][4/2459]Time: 0.449 (0.308) Data: 0.404 (0.262) Loss: 0.5404 (0.5532)\n",
      "error:  0.0009805974338888968 step  581\n",
      "cost:  0.25671096431460105\n",
      "opt took 0.00min,  581iters\n",
      "Epoch: [20][5/2459]Time: 0.389 (0.322) Data: 0.344 (0.276) Loss: 0.5567 (0.5538)\n",
      "error:  0.000902977264734206 step  731\n",
      "cost:  0.2503533543337707\n",
      "opt took 0.00min,  731iters\n",
      "Epoch: [20][6/2459]Time: 0.263 (0.313) Data: 0.218 (0.268) Loss: 0.5296 (0.5503)\n",
      "error:  0.0008916340419773405 step  641\n",
      "cost:  0.2482254391161332\n",
      "opt took 0.00min,  641iters\n",
      "Epoch: [20][7/2459]Time: 0.231 (0.303) Data: 0.186 (0.257) Loss: 0.5319 (0.5480)\n",
      "10-NN,s=0.1: TOP1:  39.416666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 21\n",
      "ResNet1D\n",
      "error:  0.000973277054927224 step  781\n",
      "cost:  0.3338648882093631\n",
      "opt took 0.00min,  781iters\n",
      "Epoch: [21][0/2459]Time: 0.328 (0.328) Data: 0.270 (0.270) Loss: 0.4867 (0.4867)\n",
      "error:  0.0009343577656315549 step  661\n",
      "cost:  0.3340193658322991\n",
      "opt took 0.00min,  661iters\n",
      "Epoch: [21][1/2459]Time: 0.267 (0.297) Data: 0.221 (0.246) Loss: 0.5153 (0.5010)\n",
      "error:  0.0008305967207158371 step  501\n",
      "cost:  0.32458859272444085\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [21][2/2459]Time: 0.241 (0.279) Data: 0.195 (0.229) Loss: 0.4940 (0.4987)\n",
      "error:  0.0008573980705524065 step  571\n",
      "cost:  0.32959440268508006\n",
      "opt took 0.00min,  571iters\n",
      "Epoch: [21][3/2459]Time: 0.260 (0.274) Data: 0.214 (0.225) Loss: 0.5090 (0.5013)\n",
      "error:  0.0009541014505558554 step  721\n",
      "cost:  0.3327062494040676\n",
      "opt took 0.00min,  721iters\n",
      "Epoch: [21][4/2459]Time: 0.267 (0.273) Data: 0.222 (0.225) Loss: 0.5063 (0.5023)\n",
      "error:  0.0009992506827755099 step  881\n",
      "cost:  0.3278245926085652\n",
      "opt took 0.00min,  881iters\n",
      "Epoch: [21][5/2459]Time: 0.257 (0.270) Data: 0.214 (0.223) Loss: 0.5000 (0.5019)\n",
      "error:  0.0009334368648113989 step  561\n",
      "cost:  0.32298654622800455\n",
      "opt took 0.00min,  561iters\n",
      "Epoch: [21][6/2459]Time: 0.245 (0.266) Data: 0.199 (0.219) Loss: 0.5148 (0.5037)\n",
      "error:  0.0009595649806507511 step  691\n",
      "cost:  0.319039393482407\n",
      "opt took 0.00min,  691iters\n",
      "Epoch: [21][7/2459]Time: 0.262 (0.266) Data: 0.216 (0.219) Loss: 0.5217 (0.5060)\n",
      "10-NN,s=0.1: TOP1:  39.541666666666664\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 22\n",
      "ResNet1D\n",
      "error:  0.0009572730167290988 step  951\n",
      "cost:  0.3235568719694595\n",
      "opt took 0.00min,  951iters\n",
      "Epoch: [22][0/2459]Time: 0.251 (0.251) Data: 0.206 (0.206) Loss: 0.4702 (0.4702)\n",
      "error:  0.000895763731126431 step  981\n",
      "cost:  0.32721990652166955\n",
      "opt took 0.00min,  981iters\n",
      "Epoch: [22][1/2459]Time: 0.290 (0.270) Data: 0.245 (0.225) Loss: 0.4425 (0.4563)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.0009432952996876187 step  1291\n",
      "cost:  0.3297274015495966\n",
      "opt took 0.00min, 1291iters\n",
      "Epoch: [22][2/2459]Time: 0.315 (0.285) Data: 0.270 (0.240) Loss: 0.4946 (0.4691)\n",
      "error:  0.0009143243586814398 step  791\n",
      "cost:  0.3287684813839734\n",
      "opt took 0.00min,  791iters\n",
      "Epoch: [22][3/2459]Time: 0.292 (0.287) Data: 0.233 (0.238) Loss: 0.4744 (0.4704)\n",
      "error:  0.0009897131431763073 step  891\n",
      "cost:  0.3235507613076406\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [22][4/2459]Time: 0.489 (0.327) Data: 0.402 (0.271) Loss: 0.4372 (0.4638)\n",
      "error:  0.0009444829850302483 step  501\n",
      "cost:  0.34233145844306945\n",
      "opt took 0.00min,  501iters\n",
      "Epoch: [22][5/2459]Time: 0.220 (0.310) Data: 0.174 (0.255) Loss: 0.4793 (0.4664)\n",
      "error:  0.0009965403441386522 step  961\n",
      "cost:  0.3373932796390956\n",
      "opt took 0.00min,  961iters\n",
      "Epoch: [22][6/2459]Time: 0.279 (0.305) Data: 0.232 (0.252) Loss: 0.4828 (0.4687)\n",
      "error:  0.0009780145439853838 step  1841\n",
      "cost:  0.3239928526334219\n",
      "opt took 0.00min, 1841iters\n",
      "Epoch: [22][7/2459]Time: 0.436 (0.322) Data: 0.389 (0.269) Loss: 0.4956 (0.4721)\n",
      "10-NN,s=0.1: TOP1:  38.375\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 23\n",
      "ResNet1D\n",
      "error:  0.0009514127914130865 step  1191\n",
      "cost:  0.37465356755902957\n",
      "opt took 0.00min, 1191iters\n",
      "Epoch: [23][0/2459]Time: 0.264 (0.264) Data: 0.217 (0.217) Loss: 0.4408 (0.4408)\n",
      "error:  0.0009965003852961951 step  2031\n",
      "cost:  0.3766340295573699\n",
      "opt took 0.01min, 2031iters\n",
      "Epoch: [23][1/2459]Time: 0.591 (0.428) Data: 0.544 (0.381) Loss: 0.4473 (0.4440)\n",
      "error:  0.0009971440565100798 step  1231\n",
      "cost:  0.36968768627631643\n",
      "opt took 0.00min, 1231iters\n",
      "Epoch: [23][2/2459]Time: 0.320 (0.392) Data: 0.274 (0.345) Loss: 0.4352 (0.4411)\n",
      "error:  0.0009064656665493942 step  931\n",
      "cost:  0.37402381705673554\n",
      "opt took 0.00min,  931iters\n",
      "Epoch: [23][3/2459]Time: 0.273 (0.362) Data: 0.226 (0.316) Loss: 0.4211 (0.4361)\n",
      "error:  0.0009689355082075357 step  1891\n",
      "cost:  0.37790461082123195\n",
      "opt took 0.01min, 1891iters\n",
      "Epoch: [23][4/2459]Time: 0.483 (0.386) Data: 0.436 (0.340) Loss: 0.4258 (0.4340)\n",
      "error:  0.0009391457886621657 step  1031\n",
      "cost:  0.3726503517656353\n",
      "opt took 0.00min, 1031iters\n",
      "Epoch: [23][5/2459]Time: 0.403 (0.389) Data: 0.358 (0.343) Loss: 0.4359 (0.4343)\n",
      "error:  0.0009491687357761069 step  1311\n",
      "cost:  0.36622181976672896\n",
      "opt took 0.00min, 1311iters\n",
      "Epoch: [23][6/2459]Time: 0.425 (0.394) Data: 0.378 (0.348) Loss: 0.4534 (0.4371)\n",
      "error:  0.000998866395692799 step  1231\n",
      "cost:  0.3663690925199925\n",
      "opt took 0.00min, 1231iters\n",
      "Epoch: [23][7/2459]Time: 0.326 (0.386) Data: 0.281 (0.339) Loss: 0.4535 (0.4391)\n",
      "10-NN,s=0.1: TOP1:  38.458333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 24\n",
      "ResNet1D\n",
      "error:  0.0009934779597420285 step  901\n",
      "cost:  0.37429432209555563\n",
      "opt took 0.00min,  901iters\n",
      "Epoch: [24][0/2459]Time: 0.295 (0.295) Data: 0.251 (0.251) Loss: 0.3991 (0.3991)\n",
      "error:  0.0009883975391499211 step  1371\n",
      "cost:  0.3750974067973408\n",
      "opt took 0.01min, 1371iters\n",
      "Epoch: [24][1/2459]Time: 0.519 (0.407) Data: 0.458 (0.354) Loss: 0.4085 (0.4038)\n",
      "error:  0.0009436693569389032 step  961\n",
      "cost:  0.3690845253942933\n",
      "opt took 0.00min,  961iters\n",
      "Epoch: [24][2/2459]Time: 0.415 (0.410) Data: 0.371 (0.360) Loss: 0.4227 (0.4101)\n",
      "error:  0.0009926864410921832 step  1221\n",
      "cost:  0.3672916297705361\n",
      "opt took 0.00min, 1221iters\n",
      "Epoch: [24][3/2459]Time: 0.367 (0.399) Data: 0.321 (0.350) Loss: 0.4112 (0.4104)\n",
      "error:  0.0009501772740766468 step  1121\n",
      "cost:  0.36243656524739304\n",
      "opt took 0.00min, 1121iters\n",
      "Epoch: [24][4/2459]Time: 0.320 (0.383) Data: 0.275 (0.335) Loss: 0.4161 (0.4115)\n",
      "error:  0.0009534813095259942 step  1221\n",
      "cost:  0.36151268264736863\n",
      "opt took 0.00min, 1221iters\n",
      "Epoch: [24][5/2459]Time: 0.375 (0.382) Data: 0.330 (0.334) Loss: 0.3903 (0.4080)\n",
      "error:  0.0009958732626886846 step  1681\n",
      "cost:  0.3584260461419285\n",
      "opt took 0.00min, 1681iters\n",
      "Epoch: [24][6/2459]Time: 0.400 (0.385) Data: 0.355 (0.337) Loss: 0.4229 (0.4101)\n",
      "error:  0.0009627218203077659 step  1051\n",
      "cost:  0.35826824850389327\n",
      "opt took 0.00min, 1051iters\n",
      "Epoch: [24][7/2459]Time: 0.432 (0.390) Data: 0.386 (0.343) Loss: 0.3953 (0.4083)\n",
      "10-NN,s=0.1: TOP1:  39.833333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 25\n",
      "ResNet1D\n",
      "error:  0.0009889591055011193 step  1021\n",
      "cost:  0.3847178559373481\n",
      "opt took 0.00min, 1021iters\n",
      "Epoch: [25][0/2459]Time: 0.313 (0.313) Data: 0.267 (0.267) Loss: 0.3745 (0.3745)\n",
      "error:  0.0009453620130465135 step  1261\n",
      "cost:  0.3820632896322233\n",
      "opt took 0.00min, 1261iters\n",
      "Epoch: [25][1/2459]Time: 0.364 (0.339) Data: 0.319 (0.293) Loss: 0.3652 (0.3699)\n",
      "error:  0.0009375275542986561 step  1201\n",
      "cost:  0.3779124445525984\n",
      "opt took 0.00min, 1201iters\n",
      "Epoch: [25][2/2459]Time: 0.331 (0.336) Data: 0.286 (0.291) Loss: 0.3808 (0.3735)\n",
      "error:  0.0009361852610508814 step  1471\n",
      "cost:  0.37559896627603506\n",
      "opt took 0.00min, 1471iters\n",
      "Epoch: [25][3/2459]Time: 0.436 (0.361) Data: 0.391 (0.316) Loss: 0.3783 (0.3747)\n",
      "error:  0.0009405235683885849 step  1251\n",
      "cost:  0.38424752885421865\n",
      "opt took 0.00min, 1251iters\n",
      "Epoch: [25][4/2459]Time: 0.335 (0.356) Data: 0.290 (0.311) Loss: 0.4006 (0.3799)\n",
      "error:  0.0009711228527226057 step  1171\n",
      "cost:  0.3835323989046942\n",
      "opt took 0.00min, 1171iters\n",
      "Epoch: [25][5/2459]Time: 0.344 (0.354) Data: 0.298 (0.309) Loss: 0.4023 (0.3836)\n",
      "error:  0.0009473322050634891 step  711\n",
      "cost:  0.39490611731589326\n",
      "opt took 0.00min,  711iters\n",
      "Epoch: [25][6/2459]Time: 0.308 (0.347) Data: 0.263 (0.302) Loss: 0.4166 (0.3884)\n",
      "error:  0.0009648276570836112 step  1371\n",
      "cost:  0.3820806598387405\n",
      "opt took 0.00min, 1371iters\n",
      "Epoch: [25][7/2459]Time: 0.332 (0.345) Data: 0.287 (0.300) Loss: 0.4024 (0.3901)\n",
      "10-NN,s=0.1: TOP1:  39.25\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 26\n",
      "ResNet1D\n",
      "error:  0.0009756722921993033 step  1461\n",
      "cost:  0.43119166406298554\n",
      "opt took 0.01min, 1461iters\n",
      "Epoch: [26][0/2459]Time: 0.526 (0.526) Data: 0.479 (0.479) Loss: 0.3842 (0.3842)\n",
      "error:  0.0009948699887054602 step  1251\n",
      "cost:  0.4357129938305997\n",
      "opt took 0.00min, 1251iters\n",
      "Epoch: [26][1/2459]Time: 0.313 (0.420) Data: 0.268 (0.373) Loss: 0.3941 (0.3891)\n",
      "error:  0.0009633341179929866 step  1391\n",
      "cost:  0.4195192010982558\n",
      "opt took 0.00min, 1391iters\n",
      "Epoch: [26][2/2459]Time: 0.362 (0.401) Data: 0.317 (0.354) Loss: 0.3861 (0.3881)\n",
      "error:  0.0009754301088238382 step  1601\n",
      "cost:  0.4023609250724345\n",
      "opt took 0.00min, 1601iters\n",
      "Epoch: [26][3/2459]Time: 0.410 (0.403) Data: 0.365 (0.357) Loss: 0.3613 (0.3814)\n",
      "error:  0.0009291910638524747 step  891\n",
      "cost:  0.4020967849870032\n",
      "opt took 0.00min,  891iters\n",
      "Epoch: [26][4/2459]Time: 0.298 (0.382) Data: 0.256 (0.337) Loss: 0.3610 (0.3774)\n",
      "error:  0.0009562057648712097 step  1621\n",
      "cost:  0.4073292970362865\n",
      "opt took 0.00min, 1621iters\n",
      "Epoch: [26][5/2459]Time: 0.364 (0.379) Data: 0.318 (0.334) Loss: 0.3566 (0.3739)\n",
      "error:  0.0009248430791933826 step  1121\n",
      "cost:  0.4059491397903223\n",
      "opt took 0.00min, 1121iters\n",
      "Epoch: [26][6/2459]Time: 0.333 (0.372) Data: 0.288 (0.327) Loss: 0.3543 (0.3711)\n",
      "error:  0.0009591046273527137 step  1711\n",
      "cost:  0.40246854071753685\n",
      "opt took 0.00min, 1711iters\n",
      "Epoch: [26][7/2459]Time: 0.419 (0.378) Data: 0.374 (0.333) Loss: 0.3685 (0.3708)\n",
      "10-NN,s=0.1: TOP1:  38.958333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 27\n",
      "ResNet1D\n",
      "error:  0.0009631853979350513 step  1381\n",
      "cost:  0.4385143128041158\n",
      "opt took 0.01min, 1381iters\n",
      "Epoch: [27][0/2459]Time: 0.473 (0.473) Data: 0.416 (0.416) Loss: 0.3618 (0.3618)\n",
      "error:  0.000984828906821611 step  1631\n",
      "cost:  0.42793906498579926\n",
      "opt took 0.00min, 1631iters\n",
      "Epoch: [27][1/2459]Time: 0.468 (0.470) Data: 0.423 (0.419) Loss: 0.3522 (0.3570)\n",
      "error:  0.0009841104294661118 step  2411\n",
      "cost:  0.41637164056744475\n",
      "opt took 0.01min, 2411iters\n",
      "Epoch: [27][2/2459]Time: 0.618 (0.519) Data: 0.572 (0.470) Loss: 0.3910 (0.3683)\n",
      "error:  0.0009840609058737737 step  1701\n",
      "cost:  0.41116499747531465\n",
      "opt took 0.00min, 1701iters\n",
      "Epoch: [27][3/2459]Time: 0.473 (0.508) Data: 0.415 (0.456) Loss: 0.3405 (0.3614)\n",
      "error:  0.00098591225292044 step  3211\n",
      "cost:  0.4129621968747438\n",
      "opt took 0.01min, 3211iters\n",
      "Epoch: [27][4/2459]Time: 0.638 (0.534) Data: 0.593 (0.484) Loss: 0.3486 (0.3588)\n",
      "error:  0.00099890458205254 step  2361\n",
      "cost:  0.403879299985396\n",
      "opt took 0.01min, 2361iters\n",
      "Epoch: [27][5/2459]Time: 0.646 (0.552) Data: 0.600 (0.503) Loss: 0.3480 (0.3570)\n",
      "error:  0.0009769930970369156 step  1551\n",
      "cost:  0.41112224889547455\n",
      "opt took 0.00min, 1551iters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][6/2459]Time: 0.417 (0.533) Data: 0.372 (0.484) Loss: 0.3687 (0.3587)\n",
      "error:  0.0009804724320519043 step  1121\n",
      "cost:  0.4078992303169037\n",
      "opt took 0.00min, 1121iters\n",
      "Epoch: [27][7/2459]Time: 0.377 (0.514) Data: 0.332 (0.465) Loss: 0.3537 (0.3581)\n",
      "10-NN,s=0.1: TOP1:  39.458333333333336\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 28\n",
      "ResNet1D\n",
      "error:  0.000986561409401232 step  1661\n",
      "cost:  0.4119992899007616\n",
      "opt took 0.01min, 1661iters\n",
      "Epoch: [28][0/2459]Time: 0.459 (0.459) Data: 0.413 (0.413) Loss: 0.3417 (0.3417)\n",
      "error:  0.0009935410564766434 step  2011\n",
      "cost:  0.3917406983229844\n",
      "opt took 0.01min, 2011iters\n",
      "Epoch: [28][1/2459]Time: 0.556 (0.508) Data: 0.511 (0.462) Loss: 0.3308 (0.3362)\n",
      "error:  0.0009980791417155688 step  7821\n",
      "cost:  0.39767236438981035\n",
      "opt took 0.03min, 7821iters\n",
      "Epoch: [28][2/2459]Time: 1.692 (0.902) Data: 1.646 (0.857) Loss: 0.3260 (0.3328)\n",
      "error:  0.0009998004015262518 step  7811\n",
      "cost:  0.40033042358937554\n",
      "opt took 0.02min, 7811iters\n",
      "Epoch: [28][3/2459]Time: 1.330 (1.009) Data: 1.285 (0.964) Loss: 0.3209 (0.3298)\n",
      "error:  0.000996930348387326 step  13501\n",
      "cost:  0.38877880110855123\n",
      "opt took 0.06min, 13501iters\n",
      "Epoch: [28][4/2459]Time: 3.579 (1.523) Data: 3.533 (1.478) Loss: 0.3249 (0.3289)\n",
      "error:  0.0009840636930396052 step  1511\n",
      "cost:  0.38930648265527257\n",
      "opt took 0.00min, 1511iters\n",
      "Epoch: [28][5/2459]Time: 0.392 (1.335) Data: 0.347 (1.289) Loss: 0.3276 (0.3287)\n",
      "error:  0.0009740992719927677 step  2121\n",
      "cost:  0.39767935504252966\n",
      "opt took 0.01min, 2121iters\n",
      "Epoch: [28][6/2459]Time: 0.650 (1.237) Data: 0.605 (1.192) Loss: 0.3376 (0.3299)\n",
      "error:  0.0009785429939179746 step  1871\n",
      "cost:  0.3862519847244047\n",
      "opt took 0.00min, 1871iters\n",
      "Epoch: [28][7/2459]Time: 0.450 (1.138) Data: 0.404 (1.093) Loss: 0.3380 (0.3309)\n",
      "10-NN,s=0.1: TOP1:  40.0\n",
      "best accuracy: 40.21\n",
      "\n",
      "Epoch: 29\n",
      "ResNet1D\n",
      "error:  0.0009962764363128374 step  2201\n",
      "cost:  0.41131320190118986\n",
      "opt took 0.01min, 2201iters\n",
      "Epoch: [29][0/2459]Time: 0.499 (0.499) Data: 0.453 (0.453) Loss: 0.3220 (0.3220)\n",
      "error:  0.0009569846940433724 step  2121\n",
      "cost:  0.4106554620542778\n",
      "opt took 0.00min, 2121iters\n",
      "Epoch: [29][1/2459]Time: 0.466 (0.483) Data: 0.421 (0.437) Loss: 0.3338 (0.3279)\n",
      "error:  0.000991584270462309 step  2601\n",
      "cost:  0.41818364397501656\n",
      "opt took 0.01min, 2601iters\n",
      "Epoch: [29][2/2459]Time: 0.610 (0.525) Data: 0.555 (0.476) Loss: 0.3370 (0.3309)\n",
      "error:  0.0009644328233151533 step  2031\n",
      "cost:  0.4167969530258271\n",
      "opt took 0.01min, 2031iters\n",
      "Epoch: [29][3/2459]Time: 0.515 (0.523) Data: 0.469 (0.475) Loss: 0.3192 (0.3280)\n",
      "error:  0.0009958658220180494 step  2561\n",
      "cost:  0.4134769932117006\n",
      "opt took 0.01min, 2561iters\n",
      "Epoch: [29][4/2459]Time: 0.619 (0.542) Data: 0.575 (0.495) Loss: 0.3006 (0.3225)\n",
      "error:  0.0009754248371806051 step  2261\n",
      "cost:  0.42431224741009116\n",
      "opt took 0.01min, 2261iters\n",
      "Epoch: [29][5/2459]Time: 0.575 (0.547) Data: 0.530 (0.501) Loss: 0.3043 (0.3195)\n",
      "error:  0.0009685429741655982 step  2151\n",
      "cost:  0.42259489411496237\n",
      "opt took 0.00min, 2151iters\n",
      "Epoch: [29][6/2459]Time: 0.488 (0.539) Data: 0.443 (0.492) Loss: 0.3288 (0.3208)\n",
      "error:  0.0009695981942734466 step  1631\n",
      "cost:  0.40104979618886516\n",
      "opt took 0.01min, 1631iters\n",
      "Epoch: [29][7/2459]Time: 0.568 (0.543) Data: 0.505 (0.494) Loss: 0.2899 (0.3170)\n",
      "10-NN,s=0.1: TOP1:  39.541666666666664\n",
      "best accuracy: 40.21\n",
      "doing PCA with 128 components ..done\n",
      "10-NN,s=0.1: TOP1:  33.791666666666664\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    \n",
    "    acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim)\n",
    "    feature_return_switch(model, False)\n",
    "#     writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = my_kNN(model, K=[50, 10], sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "#         for num_nn in [50, 10]:\n",
    "#             for sig in [0.1, 0.5]:\n",
    "#                 writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "#                 i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "end = time.time()\n",
    "\n",
    "checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = my_kNN(model, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_ckpt.t7  ep0.t7\r\n"
     ]
    }
   ],
   "source": [
    "!ls './resnet1d_exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "ResNet1D\n",
      "Epoch: [0][0/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 2.7465 (2.7465)\n",
      "Epoch: [0][1/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 6.9886 (4.8675)\n",
      "Epoch: [0][2/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 9.7415 (6.4922)\n",
      "Epoch: [0][3/2459]Time: 0.009 (0.009) Data: 0.002 (0.001) Loss: 10.1891 (7.4164)\n",
      "Epoch: [0][4/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 12.8986 (8.5129)\n",
      "Epoch: [0][5/2459]Time: 0.009 (0.009) Data: 0.002 (0.001) Loss: 14.5807 (9.5242)\n",
      "Epoch: [0][6/2459]Time: 0.009 (0.009) Data: 0.002 (0.002) Loss: 19.0564 (10.8859)\n",
      "Epoch: [0][7/2459]Time: 0.009 (0.009) Data: 0.002 (0.002) Loss: 25.1008 (12.6628)\n",
      "\n",
      "Epoch: 1\n",
      "ResNet1D\n",
      "Epoch: [1][0/2459]Time: 0.009 (0.009) Data: 0.002 (0.002) Loss: 30.0927 (30.0927)\n",
      "Epoch: [1][1/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 34.5391 (32.3159)\n",
      "Epoch: [1][2/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 43.6711 (36.1010)\n",
      "Epoch: [1][3/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 38.9770 (36.8200)\n",
      "Epoch: [1][4/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 38.2411 (37.1042)\n",
      "Epoch: [1][5/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 27.9135 (35.5724)\n",
      "Epoch: [1][6/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 26.6171 (34.2931)\n",
      "Epoch: [1][7/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 35.2638 (34.4144)\n",
      "\n",
      "Epoch: 2\n",
      "ResNet1D\n",
      "Epoch: [2][0/2459]Time: 0.009 (0.009) Data: 0.002 (0.002) Loss: 34.4732 (34.4732)\n",
      "Epoch: [2][1/2459]Time: 0.009 (0.009) Data: 0.001 (0.002) Loss: 50.9236 (42.6984)\n",
      "Epoch: [2][2/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 54.6650 (46.6873)\n",
      "Epoch: [2][3/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 41.4305 (45.3731)\n",
      "Epoch: [2][4/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 28.1335 (41.9251)\n",
      "Epoch: [2][5/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 36.4492 (41.0125)\n",
      "Epoch: [2][6/2459]Time: 0.009 (0.009) Data: 0.001 (0.001) Loss: 35.9622 (40.2910)\n",
      "Epoch: [2][7/2459]Time: 0.008 (0.009) Data: 0.001 (0.001) Loss: 33.5633 (39.4500)\n",
      "\n",
      "Epoch: 3\n",
      "ResNet1D\n",
      "Epoch: [3][0/2459]Time: 0.008 (0.008) Data: 0.002 (0.002) Loss: 42.2576 (42.2576)\n",
      "Epoch: [3][1/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 39.1949 (40.7263)\n",
      "Epoch: [3][2/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 46.5060 (42.6528)\n",
      "Epoch: [3][3/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 60.6943 (47.1632)\n",
      "Epoch: [3][4/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 46.5664 (47.0438)\n",
      "Epoch: [3][5/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 51.2675 (47.7478)\n",
      "Epoch: [3][6/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 50.8842 (48.1958)\n",
      "Epoch: [3][7/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 50.0961 (48.4334)\n",
      "\n",
      "Epoch: 4\n",
      "ResNet1D\n",
      "Epoch: [4][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 37.6309 (37.6309)\n",
      "Epoch: [4][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 41.2560 (39.4435)\n",
      "Epoch: [4][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 51.6174 (43.5014)\n",
      "Epoch: [4][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 53.1852 (45.9224)\n",
      "Epoch: [4][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 44.8120 (45.7003)\n",
      "Epoch: [4][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.0411 (44.7571)\n",
      "Epoch: [4][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.7258 (43.1812)\n",
      "Epoch: [4][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.8358 (42.7630)\n",
      "\n",
      "Epoch: 5\n",
      "ResNet1D\n",
      "Epoch: [5][0/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 43.0865 (43.0865)\n",
      "Epoch: [5][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.3923 (41.2394)\n",
      "Epoch: [5][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 42.4523 (41.6437)\n",
      "Epoch: [5][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 52.8842 (44.4538)\n",
      "Epoch: [5][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.2411 (43.6113)\n",
      "Epoch: [5][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 41.0794 (43.1893)\n",
      "Epoch: [5][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 27.2769 (40.9161)\n",
      "Epoch: [5][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 47.5058 (41.7398)\n",
      "\n",
      "Epoch: 6\n",
      "ResNet1D\n",
      "Epoch: [6][0/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 30.7647 (30.7647)\n",
      "Epoch: [6][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.9884 (35.8766)\n",
      "Epoch: [6][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 42.0703 (37.9411)\n",
      "Epoch: [6][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 46.7336 (40.1393)\n",
      "Epoch: [6][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 42.8913 (40.6897)\n",
      "Epoch: [6][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 49.9213 (42.2283)\n",
      "Epoch: [6][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 38.6393 (41.7156)\n",
      "Epoch: [6][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 44.5896 (42.0748)\n",
      "\n",
      "Epoch: 7\n",
      "ResNet1D\n",
      "Epoch: [7][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 39.5310 (39.5310)\n",
      "Epoch: [7][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 38.8587 (39.1948)\n",
      "Epoch: [7][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.4474 (38.2790)\n",
      "Epoch: [7][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.5344 (38.8429)\n",
      "Epoch: [7][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.9678 (39.0679)\n",
      "Epoch: [7][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 43.5598 (39.8165)\n",
      "Epoch: [7][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.3509 (39.3214)\n",
      "Epoch: [7][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 42.5109 (39.7201)\n",
      "\n",
      "Epoch: 8\n",
      "ResNet1D\n",
      "Epoch: [8][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 39.2139 (39.2139)\n",
      "Epoch: [8][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 37.8543 (38.5341)\n",
      "Epoch: [8][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 51.0894 (42.7192)\n",
      "Epoch: [8][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.5110 (41.1672)\n",
      "Epoch: [8][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 41.3012 (41.1940)\n",
      "Epoch: [8][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 52.3622 (43.0553)\n",
      "Epoch: [8][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 55.8076 (44.8771)\n",
      "Epoch: [8][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.4646 (44.2005)\n",
      "\n",
      "Epoch: 9\n",
      "ResNet1D\n",
      "Epoch: [9][0/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 62.5859 (62.5859)\n",
      "Epoch: [9][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 62.4600 (62.5230)\n",
      "Epoch: [9][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 75.5549 (66.8669)\n",
      "Epoch: [9][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 53.3433 (63.4860)\n",
      "Epoch: [9][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 30.9040 (56.9696)\n",
      "Epoch: [9][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 30.7843 (52.6054)\n",
      "Epoch: [9][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 41.5781 (51.0301)\n",
      "Epoch: [9][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 46.7295 (50.4925)\n",
      "\n",
      "Epoch: 10\n",
      "ResNet1D\n",
      "Epoch: [10][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 50.3942 (50.3942)\n",
      "Epoch: [10][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 52.5936 (51.4939)\n",
      "Epoch: [10][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 55.6152 (52.8677)\n",
      "Epoch: [10][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 62.6303 (55.3083)\n",
      "Epoch: [10][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 59.0351 (56.0537)\n",
      "Epoch: [10][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 46.6187 (54.4812)\n",
      "Epoch: [10][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.0947 (52.4260)\n",
      "Epoch: [10][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 47.6618 (51.8304)\n",
      "\n",
      "Epoch: 11\n",
      "ResNet1D\n",
      "Epoch: [11][0/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 45.8440 (45.8440)\n",
      "Epoch: [11][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 67.0849 (56.4645)\n",
      "Epoch: [11][2/2459]Time: 0.013 (0.009) Data: 0.001 (0.001) Loss: 49.0448 (53.9913)\n",
      "Epoch: [11][3/2459]Time: 0.007 (0.009) Data: 0.001 (0.001) Loss: 36.0000 (49.4934)\n",
      "Epoch: [11][4/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 51.4173 (49.8782)\n",
      "Epoch: [11][5/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 44.3388 (48.9550)\n",
      "Epoch: [11][6/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 58.0352 (50.2522)\n",
      "Epoch: [11][7/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 36.8282 (48.5742)\n",
      "\n",
      "Epoch: 12\n",
      "ResNet1D\n",
      "Epoch: [12][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 40.9361 (40.9361)\n",
      "Epoch: [12][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 41.0385 (40.9873)\n",
      "Epoch: [12][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 35.6368 (39.2038)\n",
      "Epoch: [12][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 32.9270 (37.6346)\n",
      "Epoch: [12][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 23.2543 (34.7585)\n",
      "Epoch: [12][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.2042 (35.4995)\n",
      "Epoch: [12][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.6810 (35.2397)\n",
      "Epoch: [12][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 34.2215 (35.1124)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13\n",
      "ResNet1D\n",
      "Epoch: [13][0/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.0037 (33.0037)\n",
      "Epoch: [13][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 37.4506 (35.2272)\n",
      "Epoch: [13][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 41.5180 (37.3241)\n",
      "Epoch: [13][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 50.7870 (40.6898)\n",
      "Epoch: [13][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 48.5341 (42.2587)\n",
      "Epoch: [13][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.8700 (42.0272)\n",
      "Epoch: [13][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 52.3112 (43.4964)\n",
      "Epoch: [13][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 54.2547 (44.8412)\n",
      "\n",
      "Epoch: 14\n",
      "ResNet1D\n",
      "Epoch: [14][0/2459]Time: 0.012 (0.012) Data: 0.002 (0.002) Loss: 35.1577 (35.1577)\n",
      "Epoch: [14][1/2459]Time: 0.011 (0.012) Data: 0.001 (0.002) Loss: 42.9316 (39.0446)\n",
      "Epoch: [14][2/2459]Time: 0.011 (0.012) Data: 0.001 (0.002) Loss: 41.6714 (39.9202)\n",
      "Epoch: [14][3/2459]Time: 0.010 (0.011) Data: 0.001 (0.001) Loss: 30.0688 (37.4574)\n",
      "Epoch: [14][4/2459]Time: 0.010 (0.011) Data: 0.001 (0.001) Loss: 27.1328 (35.3925)\n",
      "Epoch: [14][5/2459]Time: 0.009 (0.011) Data: 0.001 (0.001) Loss: 31.1926 (34.6925)\n",
      "Epoch: [14][6/2459]Time: 0.009 (0.010) Data: 0.001 (0.001) Loss: 27.2594 (33.6306)\n",
      "Epoch: [14][7/2459]Time: 0.009 (0.010) Data: 0.001 (0.001) Loss: 29.9112 (33.1657)\n",
      "\n",
      "Epoch: 15\n",
      "ResNet1D\n",
      "Epoch: [15][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 23.6885 (23.6885)\n",
      "Epoch: [15][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 22.6719 (23.1802)\n",
      "Epoch: [15][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 20.2047 (22.1884)\n",
      "Epoch: [15][3/2459]Time: 0.009 (0.008) Data: 0.001 (0.001) Loss: 22.0700 (22.1588)\n",
      "Epoch: [15][4/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 13.1403 (20.3551)\n",
      "Epoch: [15][5/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 29.1057 (21.8135)\n",
      "Epoch: [15][6/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 17.3902 (21.1816)\n",
      "Epoch: [15][7/2459]Time: 0.008 (0.008) Data: 0.002 (0.001) Loss: 20.7594 (21.1288)\n",
      "\n",
      "Epoch: 16\n",
      "ResNet1D\n",
      "Epoch: [16][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 24.3779 (24.3779)\n",
      "Epoch: [16][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 22.9009 (23.6394)\n",
      "Epoch: [16][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 16.9173 (21.3987)\n",
      "Epoch: [16][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 19.2240 (20.8550)\n",
      "Epoch: [16][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 20.3540 (20.7548)\n",
      "Epoch: [16][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 21.9302 (20.9507)\n",
      "Epoch: [16][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 30.6332 (22.3339)\n",
      "Epoch: [16][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 19.4636 (21.9751)\n",
      "\n",
      "Epoch: 17\n",
      "ResNet1D\n",
      "Epoch: [17][0/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 23.3762 (23.3762)\n",
      "Epoch: [17][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 21.7924 (22.5843)\n",
      "Epoch: [17][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 21.3889 (22.1858)\n",
      "Epoch: [17][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 27.4271 (23.4961)\n",
      "Epoch: [17][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 24.4549 (23.6879)\n",
      "Epoch: [17][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 34.2531 (25.4488)\n",
      "Epoch: [17][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 34.6691 (26.7659)\n",
      "Epoch: [17][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 28.7868 (27.0185)\n",
      "\n",
      "Epoch: 18\n",
      "ResNet1D\n",
      "Epoch: [18][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 30.3715 (30.3715)\n",
      "Epoch: [18][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 29.1415 (29.7565)\n",
      "Epoch: [18][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 37.6055 (32.3728)\n",
      "Epoch: [18][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 37.3306 (33.6123)\n",
      "Epoch: [18][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 30.6080 (33.0114)\n",
      "Epoch: [18][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.4471 (33.0840)\n",
      "Epoch: [18][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 50.9397 (35.6348)\n",
      "Epoch: [18][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.8115 (36.1569)\n",
      "\n",
      "Epoch: 19\n",
      "ResNet1D\n",
      "Epoch: [19][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 35.3750 (35.3750)\n",
      "Epoch: [19][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 42.3267 (38.8508)\n",
      "Epoch: [19][2/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 52.3361 (43.3459)\n",
      "Epoch: [19][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 41.3951 (42.8582)\n",
      "Epoch: [19][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 38.6960 (42.0258)\n",
      "Epoch: [19][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.6381 (41.7945)\n",
      "Epoch: [19][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 59.4623 (44.3185)\n",
      "Epoch: [19][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 31.6053 (42.7293)\n",
      "\n",
      "Epoch: 20\n",
      "ResNet1D\n",
      "Epoch: [20][0/2459]Time: 0.008 (0.008) Data: 0.002 (0.002) Loss: 41.3044 (41.3044)\n",
      "Epoch: [20][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 47.6544 (44.4794)\n",
      "Epoch: [20][2/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 26.6299 (38.5295)\n",
      "Epoch: [20][3/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 34.6873 (37.5690)\n",
      "Epoch: [20][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 57.8628 (41.6278)\n",
      "Epoch: [20][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 59.1409 (44.5466)\n",
      "Epoch: [20][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 47.3032 (44.9404)\n",
      "Epoch: [20][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 29.9517 (43.0668)\n",
      "\n",
      "Epoch: 21\n",
      "ResNet1D\n",
      "Epoch: [21][0/2459]Time: 0.008 (0.008) Data: 0.002 (0.002) Loss: 25.3540 (25.3540)\n",
      "Epoch: [21][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 27.3256 (26.3398)\n",
      "Epoch: [21][2/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 30.5232 (27.7343)\n",
      "Epoch: [21][3/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 32.5037 (28.9266)\n",
      "Epoch: [21][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 31.9345 (29.5282)\n",
      "Epoch: [21][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 22.8891 (28.4217)\n",
      "Epoch: [21][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 16.3592 (26.6985)\n",
      "Epoch: [21][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 27.4887 (26.7972)\n",
      "\n",
      "Epoch: 22\n",
      "ResNet1D\n",
      "Epoch: [22][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 29.8208 (29.8208)\n",
      "Epoch: [22][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 20.2453 (25.0330)\n",
      "Epoch: [22][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 26.4510 (25.5057)\n",
      "Epoch: [22][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 34.9070 (27.8560)\n",
      "Epoch: [22][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 30.3806 (28.3609)\n",
      "Epoch: [22][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.6488 (29.7422)\n",
      "Epoch: [22][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 38.7877 (31.0344)\n",
      "Epoch: [22][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.6677 (31.7386)\n",
      "\n",
      "Epoch: 23\n",
      "ResNet1D\n",
      "Epoch: [23][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 33.6847 (33.6847)\n",
      "Epoch: [23][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 35.6256 (34.6552)\n",
      "Epoch: [23][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.1127 (34.1410)\n",
      "Epoch: [23][3/2459]Time: 0.009 (0.008) Data: 0.001 (0.001) Loss: 18.2036 (30.1567)\n",
      "Epoch: [23][4/2459]Time: 0.008 (0.008) Data: 0.002 (0.002) Loss: 24.5697 (29.0393)\n",
      "Epoch: [23][5/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 32.9181 (29.6857)\n",
      "Epoch: [23][6/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 28.0203 (29.4478)\n",
      "Epoch: [23][7/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 35.8329 (30.2460)\n",
      "\n",
      "Epoch: 24\n",
      "ResNet1D\n",
      "Epoch: [24][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 39.8273 (39.8273)\n",
      "Epoch: [24][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 48.9817 (44.4045)\n",
      "Epoch: [24][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 30.8904 (39.8998)\n",
      "Epoch: [24][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.0008 (38.1750)\n",
      "Epoch: [24][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.2979 (38.3996)\n",
      "Epoch: [24][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 23.7568 (35.9592)\n",
      "Epoch: [24][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 26.4332 (34.5983)\n",
      "Epoch: [24][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 25.9157 (33.5130)\n",
      "\n",
      "Epoch: 25\n",
      "ResNet1D\n",
      "Epoch: [25][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 32.4829 (32.4829)\n",
      "Epoch: [25][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 37.5520 (35.0174)\n",
      "Epoch: [25][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 49.2408 (39.7586)\n",
      "Epoch: [25][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.0054 (39.5703)\n",
      "Epoch: [25][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.5701 (38.9702)\n",
      "Epoch: [25][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 40.9687 (39.3033)\n",
      "Epoch: [25][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 26.2308 (37.4358)\n",
      "Epoch: [25][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.4682 (37.3149)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 26\n",
      "ResNet1D\n",
      "Epoch: [26][0/2459]Time: 0.008 (0.008) Data: 0.002 (0.002) Loss: 35.2589 (35.2589)\n",
      "Epoch: [26][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 36.3146 (35.7867)\n",
      "Epoch: [26][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 26.9745 (32.8493)\n",
      "Epoch: [26][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 38.1892 (34.1843)\n",
      "Epoch: [26][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 52.3237 (37.8122)\n",
      "Epoch: [26][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 44.5439 (38.9341)\n",
      "Epoch: [26][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 27.9803 (37.3693)\n",
      "Epoch: [26][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 27.9673 (36.1941)\n",
      "\n",
      "Epoch: 27\n",
      "ResNet1D\n",
      "Epoch: [27][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 33.5173 (33.5173)\n",
      "Epoch: [27][1/2459]Time: 0.007 (0.008) Data: 0.001 (0.001) Loss: 48.2758 (40.8966)\n",
      "Epoch: [27][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 45.7723 (42.5218)\n",
      "Epoch: [27][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 59.5326 (46.7745)\n",
      "Epoch: [27][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 50.3400 (47.4876)\n",
      "Epoch: [27][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 51.0574 (48.0826)\n",
      "Epoch: [27][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.7036 (46.0284)\n",
      "Epoch: [27][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 31.7528 (44.2440)\n",
      "\n",
      "Epoch: 28\n",
      "ResNet1D\n",
      "Epoch: [28][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 31.5140 (31.5140)\n",
      "Epoch: [28][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 36.5694 (34.0417)\n",
      "Epoch: [28][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 29.4996 (32.5276)\n",
      "Epoch: [28][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 27.6730 (31.3140)\n",
      "Epoch: [28][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 25.4599 (30.1432)\n",
      "Epoch: [28][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 33.9450 (30.7768)\n",
      "Epoch: [28][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 39.5991 (32.0371)\n",
      "Epoch: [28][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 35.0168 (32.4096)\n",
      "\n",
      "Epoch: 29\n",
      "ResNet1D\n",
      "Epoch: [29][0/2459]Time: 0.008 (0.008) Data: 0.001 (0.001) Loss: 39.3746 (39.3746)\n",
      "Epoch: [29][1/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 23.8147 (31.5946)\n",
      "Epoch: [29][2/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 37.4846 (33.5580)\n",
      "Epoch: [29][3/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 45.1598 (36.4584)\n",
      "Epoch: [29][4/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 51.3151 (39.4298)\n",
      "Epoch: [29][5/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 26.7678 (37.3195)\n",
      "Epoch: [29][6/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 26.6244 (35.7916)\n",
      "Epoch: [29][7/2459]Time: 0.007 (0.007) Data: 0.001 (0.001) Loss: 43.8700 (36.8014)\n",
      "best accuracy: tensor(0.0146)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "model = resnet18(num_classes=numc, in_channel=dims_num)\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model.top_layer = nn.Linear(magic_dim, 256),\n",
    "                  nn.Linear(100,num_classes)\n",
    "model.headcount = 1\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    model.top_layer.train()\n",
    "\n",
    "    end = time.time()\n",
    "    best_acc = 0\n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(iterate_minibatches(X_train, y_train, batch_size, shuffle=True)):\n",
    "        inputs = inputs.float().to(device)\n",
    "        niter = epoch * N + batch_idx\n",
    "        if len(optimize_times) > 0 and niter * batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets = inputs.to(device), targets.to(device).long()#, indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if True:\n",
    "#         if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, N, batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "    acc = []\n",
    "    for batch_idx, (inputs, targets, _) in enumerate(iterate_minibatches(X_test, y_test, batch_size, shuffle=False)):\n",
    "        with torch.no_grad():\n",
    "            targets = targets\n",
    "            inputs = inputs.to(device)\n",
    "            pred = model(inputs).argmax(dim=1)\n",
    "            acc.append((pred == targets.to(device)).float().mean())\n",
    "    acc = torch.Tensor(acc)\n",
    "    if torch.mean(acc)>best_acc:\n",
    "        best_acc = torch.mean(acc)\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': best_acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/my_best_ckpt.t7' % (exp))\n",
    "print(\"best accuracy:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5, dtype=torch.int32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
