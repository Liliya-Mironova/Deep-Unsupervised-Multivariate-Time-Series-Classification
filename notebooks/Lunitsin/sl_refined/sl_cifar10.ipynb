{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from utils import kNN, AverageMeter, py_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "datadir = \"CIFAR-10\"\n",
    "\n",
    "# optimization\n",
    "lamb = 10      # SK lambda-parameter\n",
    "nopts = 400    # number of SK-optimizations\n",
    "epochs = 1   # numbers of epochs\n",
    "momentum = 0.9 # sgd momentum\n",
    "exp = './cifar_exp' # experiments results dir\n",
    "\n",
    "\n",
    "\n",
    "# other\n",
    "devc='0'  # cuda device\n",
    "batch_size = 128\n",
    "lr=0.03     #learning rate\n",
    "alr=0.03    #starting learning rate\n",
    "\n",
    "knn_dim = 4096\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + devc) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"GPU device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters (AlexNet in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc=10       # number of heads\n",
    "ncl=128     # number of clusters\n",
    "\n",
    "numc = [ncl] * hc\n",
    "# (number of filters, kernel size, stride, pad) for AlexNet, two vesions\n",
    "CFG = {\n",
    "    'big': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M'],\n",
    "    'small': [(64, 11, 4, 2), 'M', (192, 5, 1, 2), 'M', (384, 3, 1, 1), (256, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cifar-10-python.tar.gz', 'test', 'train', '.ipynb_checkpoints', 'cifar-10-batches-py']\n"
     ]
    }
   ],
   "source": [
    "print (os.listdir(datadir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Instance(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"CIFAR10Instance Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(CIFAR10Instance, self).__init__(root=root,\n",
    "                                                           train=train,\n",
    "                                                           transform=transform,\n",
    "                                                           target_transform=target_transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tfs.Compose([\n",
    "    tfs.Resize(256),\n",
    "    tfs.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n",
    "    tfs.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "    tfs.RandomGrayscale(p=0.2),\n",
    "    tfs.RandomHorizontalFlip(),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = tfs.Compose([\n",
    "        tfs.Resize(256),\n",
    "        tfs.CenterCrop(224),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CIFAR10Instance(root=datadir, train=True, download=False,\n",
    "                               transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "testset = CIFAR10Instance(root=datadir, train=False, download=False,\n",
    "                              transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "N = len(trainloader.dataset)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.8550,  1.8356,  1.7968,  ...,  1.1765,  1.1765,  1.1571],\n",
      "          [ 1.7581,  1.7193,  1.6805,  ...,  1.0796,  1.0796,  1.0408],\n",
      "          [ 1.5836,  1.5642,  1.5061,  ...,  0.9245,  0.9245,  0.9051],\n",
      "          ...,\n",
      "          [-2.0026, -2.0026, -2.0026,  ...,  1.1765,  1.0796,  1.0021],\n",
      "          [-2.0026, -2.0220, -2.0220,  ...,  1.2153,  1.1184,  1.0602],\n",
      "          [-2.0026, -2.0220, -2.0220,  ...,  1.2347,  1.1765,  1.0796]],\n",
      "\n",
      "         [[ 1.9281,  1.9085,  1.8691,  ...,  1.2398,  1.2398,  1.2201],\n",
      "          [ 1.8298,  1.7904,  1.7511,  ...,  1.1414,  1.1414,  1.1021],\n",
      "          [ 1.6528,  1.6331,  1.5741,  ...,  0.9841,  0.9841,  0.9644],\n",
      "          ...,\n",
      "          [-1.9856, -1.9856, -1.9856,  ...,  1.2398,  1.1414,  1.0628],\n",
      "          [-1.9856, -2.0053, -2.0053,  ...,  1.2791,  1.1808,  1.1218],\n",
      "          [-1.9856, -2.0053, -2.0053,  ...,  1.2988,  1.2398,  1.1414]],\n",
      "\n",
      "         [[ 2.0904,  2.0709,  2.0319,  ...,  1.4075,  1.4075,  1.3880],\n",
      "          [ 1.9928,  1.9538,  1.9148,  ...,  1.3100,  1.3100,  1.2709],\n",
      "          [ 1.8172,  1.7977,  1.7392,  ...,  1.1539,  1.1539,  1.1344],\n",
      "          ...,\n",
      "          [-1.7922, -1.7922, -1.7922,  ...,  1.4075,  1.3100,  1.2319],\n",
      "          [-1.7922, -1.8117, -1.8117,  ...,  1.4465,  1.3490,  1.2905],\n",
      "          [-1.7922, -1.8117, -1.8117,  ...,  1.4661,  1.4075,  1.3100]]],\n",
      "\n",
      "\n",
      "        [[[-0.9170, -0.8977, -0.8977,  ..., -1.0140, -1.0140, -1.0140],\n",
      "          [-0.9170, -0.8977, -0.8977,  ..., -0.9946, -0.9946, -0.9946],\n",
      "          [-0.9170, -0.8977, -0.8977,  ..., -0.9946, -0.9946, -0.9946],\n",
      "          ...,\n",
      "          [-1.5180, -1.5180, -1.5180,  ..., -1.5374, -1.5374, -1.5374],\n",
      "          [-1.5180, -1.5180, -1.5180,  ..., -1.5180, -1.5180, -1.5180],\n",
      "          [-1.4986, -1.4986, -1.4986,  ..., -1.5180, -1.5180, -1.5180]],\n",
      "\n",
      "         [[-0.8842, -0.8646, -0.8646,  ..., -0.9826, -0.9826, -0.9826],\n",
      "          [-0.8842, -0.8646, -0.8646,  ..., -0.9629, -0.9629, -0.9629],\n",
      "          [-0.8842, -0.8646, -0.8646,  ..., -0.9629, -0.9629, -0.9629],\n",
      "          ...,\n",
      "          [-1.4939, -1.4939, -1.4939,  ..., -1.5136, -1.5136, -1.5136],\n",
      "          [-1.4939, -1.4939, -1.4939,  ..., -1.4939, -1.4939, -1.4939],\n",
      "          [-1.4742, -1.4742, -1.4742,  ..., -1.4939, -1.4939, -1.4939]],\n",
      "\n",
      "         [[-0.6996, -0.6801, -0.6801,  ..., -0.7971, -0.7971, -0.7971],\n",
      "          [-0.6996, -0.6801, -0.6801,  ..., -0.7776, -0.7776, -0.7776],\n",
      "          [-0.6996, -0.6801, -0.6801,  ..., -0.7776, -0.7776, -0.7776],\n",
      "          ...,\n",
      "          [-1.3044, -1.3044, -1.3044,  ..., -1.3239, -1.3239, -1.3239],\n",
      "          [-1.3044, -1.3044, -1.3044,  ..., -1.3044, -1.3044, -1.3044],\n",
      "          [-1.2849, -1.2849, -1.2849,  ..., -1.3044, -1.3044, -1.3044]]],\n",
      "\n",
      "\n",
      "        [[[-1.1109, -1.1109, -1.0915,  ..., -0.9752, -0.9364, -0.9170],\n",
      "          [-1.1109, -1.1109, -1.0915,  ..., -0.9752, -0.9364, -0.9170],\n",
      "          [-1.0915, -1.0915, -1.0721,  ..., -0.9558, -0.9170, -0.9170],\n",
      "          ...,\n",
      "          [-0.2773, -0.2773, -0.2773,  ..., -0.5293, -0.5681, -0.6263],\n",
      "          [-0.2580, -0.2580, -0.2580,  ..., -0.5487, -0.5681, -0.6263],\n",
      "          [-0.2192, -0.2192, -0.2192,  ..., -0.5681, -0.6069, -0.6650]],\n",
      "\n",
      "         [[-1.0416, -1.0416, -1.0416,  ..., -0.7072, -0.6679, -0.6679],\n",
      "          [-1.0219, -1.0219, -1.0022,  ..., -0.6876, -0.6679, -0.6482],\n",
      "          [-1.0022, -1.0022, -1.0022,  ..., -0.6876, -0.6679, -0.6482],\n",
      "          ...,\n",
      "          [-0.1959, -0.1959, -0.1959,  ...,  1.4364,  1.4168,  1.3774],\n",
      "          [-0.1566, -0.1566, -0.1566,  ...,  1.4561,  1.4168,  1.3578],\n",
      "          [-0.1172, -0.1172, -0.1172,  ...,  1.4758,  1.4168,  1.3578]],\n",
      "\n",
      "         [[-0.2704, -0.2704, -0.2899,  ..., -0.3679, -0.3094, -0.2899],\n",
      "          [-0.2509, -0.2509, -0.2704,  ..., -0.3679, -0.3094, -0.2899],\n",
      "          [-0.2313, -0.2313, -0.2313,  ..., -0.3484, -0.2899, -0.2704],\n",
      "          ...,\n",
      "          [ 0.0028,  0.0028,  0.0223,  ...,  1.0758,  1.0173,  0.9783],\n",
      "          [ 0.0418,  0.0418,  0.0613,  ...,  1.0954,  1.0173,  0.9588],\n",
      "          [ 0.0808,  0.0808,  0.1003,  ...,  1.0563,  0.9978,  0.9393]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.4211, -1.3823, -1.3241,  ..., -1.8281, -1.8475, -1.8475],\n",
      "          [-1.4211, -1.3823, -1.3241,  ..., -1.8087, -1.8281, -1.8475],\n",
      "          [-1.4017, -1.3629, -1.3047,  ..., -1.7894, -1.8087, -1.8087],\n",
      "          ...,\n",
      "          [-1.9444, -1.9444, -1.9444,  ..., -1.2466, -1.2272, -1.2272],\n",
      "          [-1.8863, -1.8863, -1.9057,  ..., -1.1884, -1.1884, -1.1690],\n",
      "          [-1.8475, -1.8669, -1.8669,  ..., -1.1497, -1.1497, -1.1303]],\n",
      "\n",
      "         [[-1.3956, -1.3562, -1.2972,  ..., -1.8086, -1.8282, -1.8282],\n",
      "          [-1.3956, -1.3562, -1.2972,  ..., -1.7889, -1.8086, -1.8282],\n",
      "          [-1.3759, -1.3366, -1.2776,  ..., -1.7692, -1.7889, -1.7889],\n",
      "          ...,\n",
      "          [-1.9266, -1.9266, -1.9266,  ..., -1.2186, -1.1989, -1.1989],\n",
      "          [-1.8676, -1.8676, -1.8873,  ..., -1.1596, -1.1596, -1.1399],\n",
      "          [-1.8282, -1.8479, -1.8479,  ..., -1.1202, -1.1202, -1.1006]],\n",
      "\n",
      "         [[-1.2069, -1.1678, -1.1093,  ..., -1.6166, -1.6361, -1.6361],\n",
      "          [-1.2069, -1.1678, -1.1093,  ..., -1.5971, -1.6166, -1.6361],\n",
      "          [-1.1873, -1.1483, -1.0898,  ..., -1.5776, -1.5971, -1.5971],\n",
      "          ...,\n",
      "          [-1.7336, -1.7336, -1.7336,  ..., -1.0313, -1.0118, -1.0118],\n",
      "          [-1.6751, -1.6751, -1.6946,  ..., -0.9727, -0.9727, -0.9532],\n",
      "          [-1.6361, -1.6556, -1.6556,  ..., -0.9337, -0.9337, -0.9142]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5141,  2.5141,  2.5141,  ...,  1.6612,  1.6224,  1.6224],\n",
      "          [ 2.5141,  2.5141,  2.5141,  ...,  1.6224,  1.5836,  1.5836],\n",
      "          [ 2.5141,  2.5141,  2.5141,  ...,  1.5255,  1.4673,  1.4673],\n",
      "          ...,\n",
      "          [-1.2466, -1.2078, -1.1884,  ..., -0.8977, -0.8977, -0.8977],\n",
      "          [-1.2660, -1.2466, -1.2078,  ..., -0.8977, -0.8977, -0.8977],\n",
      "          [-1.3047, -1.2660, -1.2466,  ..., -0.8977, -0.8977, -0.8977]],\n",
      "\n",
      "         [[ 2.5968,  2.5968,  2.5968,  ...,  2.3215,  2.3215,  2.3215],\n",
      "          [ 2.5968,  2.5968,  2.5968,  ...,  2.3018,  2.2821,  2.2821],\n",
      "          [ 2.5968,  2.5968,  2.5968,  ...,  2.2428,  2.2231,  2.2231],\n",
      "          ...,\n",
      "          [-1.1202, -1.1006, -1.0809,  ..., -0.6286, -0.6286, -0.6286],\n",
      "          [-1.1596, -1.1202, -1.1006,  ..., -0.6286, -0.6286, -0.6286],\n",
      "          [-1.1792, -1.1596, -1.1202,  ..., -0.6286, -0.5892, -0.5892]],\n",
      "\n",
      "         [[ 2.7537,  2.7537,  2.7537,  ...,  1.7392,  1.6807,  1.6807],\n",
      "          [ 2.7537,  2.7537,  2.7537,  ...,  1.6612,  1.6612,  1.6612],\n",
      "          [ 2.7537,  2.7537,  2.7537,  ...,  1.6416,  1.5831,  1.5831],\n",
      "          ...,\n",
      "          [-0.8947, -0.8752, -0.8557,  ..., -0.2899, -0.2899, -0.2899],\n",
      "          [-0.9337, -0.8947, -0.8752,  ..., -0.2899, -0.2899, -0.2899],\n",
      "          [-0.9532, -0.9337, -0.8947,  ..., -0.2899, -0.2899, -0.2899]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0796,  1.0796,  1.0796,  ...,  1.0796,  1.0796,  1.0796],\n",
      "          [ 1.0796,  1.0796,  1.0796,  ...,  1.0796,  1.0796,  1.0796],\n",
      "          [ 1.0796,  1.0796,  1.0796,  ...,  1.0796,  1.0796,  1.0796],\n",
      "          ...,\n",
      "          [ 1.2735,  1.2541,  1.2541,  ...,  1.0796,  1.0796,  1.0796],\n",
      "          [ 1.2928,  1.2541,  1.2541,  ...,  1.0796,  1.0796,  1.0796],\n",
      "          [ 1.2928,  1.2541,  1.2541,  ...,  1.0796,  1.0796,  1.0796]],\n",
      "\n",
      "         [[ 1.6724,  1.6724,  1.6724,  ...,  1.6724,  1.6724,  1.6724],\n",
      "          [ 1.6724,  1.6724,  1.6724,  ...,  1.6724,  1.6724,  1.6724],\n",
      "          [ 1.6724,  1.6724,  1.6724,  ...,  1.6724,  1.6724,  1.6724],\n",
      "          ...,\n",
      "          [ 1.8888,  1.9085,  1.9085,  ...,  1.6921,  1.6921,  1.6921],\n",
      "          [ 1.9085,  1.9085,  1.9085,  ...,  1.6921,  1.6921,  1.6921],\n",
      "          [ 1.9085,  1.9085,  1.9085,  ...,  1.6921,  1.6921,  1.6921]],\n",
      "\n",
      "         [[ 1.4856,  1.4856,  1.4856,  ...,  1.4856,  1.4856,  1.4856],\n",
      "          [ 1.4856,  1.4856,  1.4856,  ...,  1.4856,  1.4856,  1.4856],\n",
      "          [ 1.4856,  1.4856,  1.4856,  ...,  1.4856,  1.4856,  1.4856],\n",
      "          ...,\n",
      "          [ 1.7392,  1.7392,  1.7392,  ...,  1.5246,  1.5246,  1.5246],\n",
      "          [ 1.7392,  1.7392,  1.7392,  ...,  1.5246,  1.5246,  1.5246],\n",
      "          [ 1.7392,  1.7392,  1.7392,  ...,  1.5246,  1.5246,  1.5246]]]]) tensor([9, 3, 4, 5, 8, 0, 4, 0, 7, 9, 7, 0, 0, 9, 8, 5, 5, 0, 3, 0, 0, 7, 7, 6,\n",
      "        4, 7, 8, 6, 6, 4, 3, 4, 0, 7, 7, 7, 9, 6, 3, 9, 3, 2, 8, 3, 0, 9, 0, 1,\n",
      "        7, 7, 5, 0, 2, 9, 0, 9, 2, 7, 5, 7, 1, 9, 4, 2, 2, 5, 6, 2, 4, 4, 0, 4,\n",
      "        5, 3, 7, 5, 2, 0, 5, 3, 5, 0, 5, 8, 9, 6, 0, 5, 0, 5, 0, 0, 8, 0, 5, 0,\n",
      "        2, 5, 4, 8, 6, 3, 4, 7, 5, 0, 3, 0, 5, 7, 1, 9, 4, 3, 9, 9, 8, 5, 3, 9,\n",
      "        9, 5, 2, 2, 1, 7, 9, 2]) tensor([ 3298, 19838,  3027,  7208,  4380, 33662,  9359,  7914, 45995, 40774,\n",
      "        26811, 32629, 33516,  9955, 36974, 28069, 38788, 36175,  1655,  4552,\n",
      "        16686, 28589, 40892,   645, 28039,  1197, 35912, 49639,  9084,  4382,\n",
      "        12329,  3260, 20334, 38113, 15691, 10347, 49278, 30390, 29520, 27695,\n",
      "        27013, 23414, 49532, 43733,  2027, 20675, 39531,  8449,  8555, 13525,\n",
      "        11922, 34231,  3309, 39743, 43980, 18930, 35486, 46524, 31574, 33904,\n",
      "        25306, 35533, 47570, 11241, 30542, 36347, 35261,  9503, 40663, 19917,\n",
      "        26599, 41804, 45215, 14234, 30556, 12621, 31519, 19216, 40430, 28811,\n",
      "         1482, 45656, 10724, 44362, 16617, 16841,   436, 18915,  9525, 47810,\n",
      "        44741, 42722, 29014,  5481,   932, 38576, 42894, 26278, 47611,  5869,\n",
      "        31639, 17650, 10765, 10858, 37610, 37493, 25443, 49067, 21370, 20230,\n",
      "        13006, 46723, 37116, 36543, 33896, 39371, 43858,  8334, 28020, 30215,\n",
      "         6657, 39229, 34614, 10926,  7352, 26397,  9115, 38705])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, _, _selected) in enumerate(trainloader):\n",
    "    print (data, _, _selected)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, features, num_classes, init=True):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.5),\n",
    "                            nn.Linear(256 * 6 * 6, 4096),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(4096, 4096),\n",
    "                            nn.ReLU(inplace=True))\n",
    "        self.headcount = len(num_classes)\n",
    "        self.return_features = False\n",
    "        if len(num_classes) == 1:\n",
    "            self.top_layer = nn.Linear(4096, num_classes[0])\n",
    "        else:\n",
    "            for a,i in enumerate(num_classes):\n",
    "                setattr(self, \"top_layer%d\" % a, nn.Linear(4096, i))\n",
    "            self.top_layer = None  # this way headcount can act as switch.\n",
    "        if init:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(self.features)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        if self.return_features:\n",
    "            return x\n",
    "        x = self.classifier(x)\n",
    "        if self.headcount == 1:\n",
    "            if self.top_layer: # this way headcount can act as switch.\n",
    "                x = self.top_layer(x)\n",
    "            return x\n",
    "        else:\n",
    "            outp = []\n",
    "            for i in range(self.headcount):\n",
    "                outp.append(getattr(self, \"top_layer%d\" % i)(x))\n",
    "            return outp\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for y, m in enumerate(self.modules()):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                for i in range(m.out_channels):\n",
    "                    m.weight.data[i].normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def make_layers_features(cfg, input_dim, bn):\n",
    "    layers = []\n",
    "    in_channels = input_dim\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=3, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v[0], kernel_size=v[1], stride=v[2], padding=v[3])#,bias=False)\n",
    "            if bn:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v[0]), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v[0]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def alexnet(bn=True, num_classes=[1000], init=True, size='big'):\n",
    "    dim = 3\n",
    "    model = AlexNet(make_layers_features(CFG[size], dim, bn=bn), num_classes, init)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_L_sk(PS):\n",
    "    N, K = PS.shape\n",
    "    tt = time.time()\n",
    "    PS = PS.T  # now it is K x N\n",
    "    r = np.ones((K, 1)) / K\n",
    "    c = np.ones((N, 1)) / N\n",
    "    PS **= lamb  # K x N\n",
    "    inv_K = 1. / K\n",
    "    inv_N = 1. / N\n",
    "    err = 1e3\n",
    "    _counter = 0\n",
    "    while err > 1e-2:\n",
    "        r = inv_K / (PS @ c)  # (KxN)@(N,1) = K x 1\n",
    "        c_new = inv_N / (r.T @ PS).T  # ((1,K)@(KxN)).t() = N x 1\n",
    "        if _counter % 10 == 0:\n",
    "            err = np.nansum(np.abs(c / c_new - 1))\n",
    "        c = c_new\n",
    "        _counter += 1\n",
    "    print(\"error: \", err, 'step ', _counter, flush=True)  # \" nonneg: \", sum(I), flush=True)\n",
    "    # inplace calculations.\n",
    "    PS *= np.squeeze(c)\n",
    "    PS = PS.T\n",
    "    PS *= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    argmaxes = np.nanargmax(PS, 0)  # size N\n",
    "    newL = torch.LongTensor(argmaxes)\n",
    "    selflabels = newL.to(device)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(r)\n",
    "    PS = PS.T\n",
    "    PS /= np.squeeze(c)\n",
    "    sol = PS[argmaxes, np.arange(N)]\n",
    "    np.log(sol, sol)\n",
    "    cost = -(1. / lamb) * np.nansum(sol) / N\n",
    "    print('cost: ', cost, flush=True)\n",
    "    print('opt took {0:.2f}min, {1:4d}iters'.format(((time.time() - tt) / 60.), _counter), flush=True)\n",
    "    return cost, selflabels\n",
    "\n",
    "def opt_sk(model, selflabels_in, epoch):\n",
    "    if hc == 1:\n",
    "        PS = np.zeros((len(trainloader.dataset), ncl))\n",
    "    else:\n",
    "        PS_pre = np.zeros((len(trainloader.dataset), knn_dim))\n",
    "    for batch_idx, (data, _, _selected) in enumerate(trainloader):\n",
    "        data = data.to(device)#cuda()\n",
    "        if hc == 1:\n",
    "            p = nn.functional.softmax(model(data), 1)\n",
    "            PS[_selected, :] = p.detach().cpu().numpy()\n",
    "        else:\n",
    "            p = model(data)\n",
    "            PS_pre[_selected, :] = p.detach().cpu().numpy()\n",
    "    if hc == 1:\n",
    "        cost, selflabels = optimize_L_sk(PS)\n",
    "        _costs = [cost]\n",
    "    else:\n",
    "        _nmis = np.zeros(hc)\n",
    "        _costs = np.zeros(hc)\n",
    "        nh = epoch % hc  # np.random.randint(args.hc)\n",
    "        print(\"computing head %s \" % nh, end=\"\\r\", flush=True)\n",
    "        tl = getattr(model, \"top_layer%d\" % nh)\n",
    "        # do the forward pass:\n",
    "        PS = (PS_pre @ tl.weight.cpu().numpy().T\n",
    "                   + tl.bias.cpu().numpy())\n",
    "        PS = py_softmax(PS, 1)\n",
    "        c, selflabels_ = optimize_L_sk(PS)\n",
    "        _costs[nh] = c\n",
    "        selflabels_in[nh] = selflabels_\n",
    "        selflabels = selflabels_in\n",
    "    return selflabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = alr\n",
    "    if epochs == 200:\n",
    "        if epoch >= 80:\n",
    "            lr = alr * (0.1 ** ((epoch - 80) // 40))  # i.e. 120, 160\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 400:\n",
    "        if epoch >= 160:\n",
    "            lr = alr * (0.1 ** ((epoch - 160) // 80))  # i.e. 240,320\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 800:\n",
    "        if epoch >= 320:\n",
    "            lr = alr * (0.1 ** ((epoch - 320) // 160))  # i.e. 480, 640\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epochs == 1600:\n",
    "        if epoch >= 640:\n",
    "            lr = alr * (0.1 ** ((epoch - 640) // 320))\n",
    "            print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_return_switch(model, bool=True):\n",
    "    \"\"\"\n",
    "    switch between network output or conv5features\n",
    "        if True: changes switch s.t. forward pass returns post-conv5 features\n",
    "        if False: changes switch s.t. forward will give full network output\n",
    "    \"\"\"\n",
    "    if bool:\n",
    "        model.headcount = 1\n",
    "    else:\n",
    "        model.headcount = hc\n",
    "    model.return_feature = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, selflabels):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    print(name)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, targets, indexes) in enumerate(trainloader):\n",
    "        niter = epoch * len(trainloader) + batch_idx\n",
    "        if niter * trainloader.batch_size >= optimize_times[-1]:\n",
    "            with torch.no_grad():\n",
    "                _ = optimize_times.pop()\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, True)\n",
    "                selflabels = opt_sk(model, selflabels, epoch)\n",
    "                if hc >1:\n",
    "                    feature_return_switch(model, False)\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets, indexes = inputs.to(device), targets.to(device), indexes.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        if hc == 1:\n",
    "            loss = criterion(outputs, selflabels[indexes])\n",
    "        else:\n",
    "            loss = torch.mean(torch.stack([criterion(outputs[h],\n",
    "                                                     selflabels[h, indexes]) for h in range(hc)]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}]'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, len(trainloader), batch_time=batch_time, data_time=data_time, train_loss=train_loss))\n",
    "            writer.add_scalar(\"loss\", loss.item(), batch_idx*512 +epoch*len(trainloader.dataset))\n",
    "    return selflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet(num_classes=numc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will optimize L at epochs: [11.0, 2.0, 2.0, 1.99, 1.99, 1.98, 1.98, 1.97, 1.97, 1.96, 1.95, 1.95, 1.94, 1.94, 1.93, 1.93, 1.92, 1.92, 1.91, 1.91, 1.9, 1.9, 1.89, 1.89, 1.88, 1.88, 1.87, 1.87, 1.86, 1.86, 1.85, 1.85, 1.84, 1.84, 1.83, 1.83, 1.82, 1.82, 1.81, 1.81, 1.8, 1.8, 1.79, 1.79, 1.78, 1.78, 1.77, 1.77, 1.76, 1.76, 1.75, 1.75, 1.74, 1.74, 1.73, 1.73, 1.72, 1.72, 1.71, 1.71, 1.7, 1.7, 1.69, 1.69, 1.68, 1.68, 1.67, 1.67, 1.66, 1.66, 1.65, 1.65, 1.64, 1.64, 1.63, 1.63, 1.62, 1.62, 1.61, 1.61, 1.6, 1.6, 1.59, 1.59, 1.58, 1.58, 1.57, 1.57, 1.56, 1.56, 1.55, 1.55, 1.54, 1.54, 1.53, 1.53, 1.52, 1.52, 1.51, 1.51, 1.5, 1.5, 1.49, 1.49, 1.48, 1.48, 1.47, 1.47, 1.46, 1.46, 1.45, 1.45, 1.44, 1.44, 1.43, 1.43, 1.42, 1.42, 1.41, 1.41, 1.4, 1.4, 1.39, 1.39, 1.38, 1.38, 1.37, 1.37, 1.36, 1.36, 1.35, 1.35, 1.34, 1.34, 1.33, 1.33, 1.32, 1.32, 1.31, 1.31, 1.3, 1.3, 1.29, 1.29, 1.28, 1.28, 1.27, 1.27, 1.26, 1.26, 1.25, 1.25, 1.24, 1.24, 1.23, 1.23, 1.22, 1.22, 1.21, 1.21, 1.2, 1.2, 1.19, 1.19, 1.18, 1.18, 1.17, 1.17, 1.16, 1.16, 1.15, 1.15, 1.14, 1.14, 1.13, 1.13, 1.12, 1.12, 1.11, 1.11, 1.1, 1.1, 1.09, 1.09, 1.08, 1.08, 1.07, 1.07, 1.06, 1.06, 1.05, 1.05, 1.04, 1.04, 1.03, 1.03, 1.02, 1.02, 1.01, 1.01, 1.0, 1.0, 0.99, 0.99, 0.98, 0.98, 0.97, 0.97, 0.96, 0.96, 0.95, 0.95, 0.94, 0.94, 0.93, 0.93, 0.92, 0.92, 0.91, 0.91, 0.9, 0.9, 0.89, 0.89, 0.88, 0.88, 0.87, 0.87, 0.86, 0.86, 0.85, 0.85, 0.84, 0.84, 0.83, 0.83, 0.82, 0.82, 0.81, 0.81, 0.8, 0.8, 0.79, 0.79, 0.78, 0.78, 0.77, 0.77, 0.76, 0.76, 0.75, 0.75, 0.74, 0.74, 0.73, 0.73, 0.72, 0.72, 0.71, 0.71, 0.7, 0.7, 0.69, 0.69, 0.68, 0.68, 0.67, 0.67, 0.66, 0.66, 0.65, 0.65, 0.64, 0.64, 0.63, 0.63, 0.62, 0.62, 0.61, 0.61, 0.6, 0.6, 0.59, 0.59, 0.58, 0.58, 0.57, 0.57, 0.56, 0.56, 0.55, 0.55, 0.54, 0.54, 0.53, 0.53, 0.52, 0.52, 0.51, 0.51, 0.5, 0.5, 0.49, 0.49, 0.48, 0.48, 0.47, 0.47, 0.46, 0.46, 0.45, 0.45, 0.44, 0.44, 0.43, 0.43, 0.42, 0.42, 0.41, 0.41, 0.4, 0.4, 0.39, 0.39, 0.38, 0.38, 0.37, 0.37, 0.36, 0.36, 0.35, 0.35, 0.34, 0.34, 0.33, 0.33, 0.32, 0.32, 0.31, 0.31, 0.3, 0.3, 0.29, 0.29, 0.28, 0.28, 0.27, 0.27, 0.26, 0.26, 0.25, 0.25, 0.24, 0.24, 0.23, 0.23, 0.22, 0.22, 0.21, 0.21, 0.2, 0.2, 0.19, 0.19, 0.18, 0.18, 0.17, 0.17, 0.16, 0.16, 0.15, 0.15, 0.14, 0.14, 0.13, 0.13, 0.12, 0.12, 0.11, 0.11, 0.1, 0.1, 0.09, 0.09, 0.08, 0.08, 0.07, 0.07, 0.06, 0.06, 0.05, 0.05, 0.04, 0.04, 0.03, 0.03, 0.02, 0.02, 0.01, 0.01, 0.0]\n"
     ]
    }
   ],
   "source": [
    "optimize_times = ((epochs + 1.0001)*N*(np.linspace(0, 1, nopts))[::-1]).tolist()\n",
    "optimize_times = [(epochs +10)*N] + optimize_times\n",
    "print('We will optimize L at epochs:', [np.round(1.0*t/N, 2) for t in optimize_times], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init selflabels randomly\n",
    "if hc == 1:\n",
    "    selflabels = np.zeros(N, dtype=np.int32)\n",
    "    for qq in range(N):\n",
    "        selflabels[qq] = qq % ncl\n",
    "    selflabels = np.random.permutation(selflabels)\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)\n",
    "else:\n",
    "    selflabels = np.zeros((hc, N), dtype=np.int32)\n",
    "    for nh in range(hc):\n",
    "        for _i in range(N):\n",
    "            selflabels[nh, _i] = _i % numc[nh]\n",
    "        selflabels[nh] = np.random.permutation(selflabels[nh])\n",
    "    selflabels = torch.LongTensor(selflabels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"AlexNetfromthepaper\"\n",
    "writer = SummaryWriter(f'./runs/cifar10/{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training! \n",
    "Takes a couple of minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "AlexNetfromthepaper\n",
      "error:  0.001731549280007405 step  41\n",
      "cost:  4.037465754152494\n",
      "opt took 0.01min,   41iters\n",
      "Epoch: [0][0/391]Time: 60.157 (60.157) Data: 60.061 (60.061) Loss: 4.9980 (4.9980)\n",
      "error:  0.00021789791959248017 step  41\n",
      "cost:  4.046793832648332\n",
      "opt took 0.01min,   41iters\n",
      "error:  2.8810438144510897e-05 step  31\n",
      "cost:  4.067144840513707\n",
      "opt took 0.01min,   31iters\n",
      "error:  3.65470477095009e-05 step  31\n",
      "cost:  4.1126818190276015\n",
      "opt took 0.01min,   31iters\n",
      "error:  2.093409278114411e-05 step  31\n",
      "cost:  4.164415387680921\n",
      "opt took 0.01min,   31iters\n",
      "error:  0.0033792110201565917 step  21\n",
      "cost:  4.211539159079097\n",
      "opt took 0.01min,   21iters\n",
      "Epoch: [0][10/391]Time: 56.384 (30.564) Data: 56.283 (30.464) Loss: 4.9195 (4.9507)\n",
      "error:  0.001550757876231601 step  21\n",
      "cost:  4.255514343930079\n",
      "opt took 0.01min,   21iters\n",
      "error:  0.0011722925246078209 step  21\n",
      "cost:  4.294483021882808\n",
      "opt took 0.01min,   21iters\n",
      "error:  0.0003713507278589878 step  21\n",
      "cost:  4.326902248495272\n",
      "opt took 0.01min,   21iters\n",
      "error:  2.2645035687829385e-05 step  21\n",
      "cost:  4.3545941452062005\n",
      "opt took 0.01min,   21iters\n",
      "error:  3.965560295360326e-06 step  21\n",
      "cost:  4.380078337159551\n",
      "opt took 0.01min,   21iters\n",
      "Epoch: [0][20/391]Time: 55.044 (29.378) Data: 54.944 (29.278) Loss: 4.8893 (4.9280)\n",
      "error:  1.8522087008587818e-06 step  21\n",
      "cost:  4.400974204580186\n",
      "opt took 0.01min,   21iters\n",
      "error:  2.2487375339608207e-07 step  21\n",
      "cost:  4.420454142184933\n",
      "opt took 0.01min,   21iters\n",
      "error:  1.971432295189146e-07 step  21\n",
      "cost:  4.434739341446379\n",
      "opt took 0.01min,   21iters\n",
      "error:  6.854329180905339e-07 step  21\n",
      "cost:  4.449707410863512\n",
      "opt took 0.01min,   21iters\n",
      "error:  5.471610663443371e-07 step  21\n",
      "cost:  4.461617232281411\n",
      "opt took 0.01min,   21iters\n",
      "Epoch: [0][30/391]Time: 57.879 (28.723) Data: 57.776 (28.623) Loss: 4.8789 (4.9138)\n",
      "error:  1.1874953691481949e-06 step  21\n",
      "cost:  4.473079933951911\n",
      "opt took 0.01min,   21iters\n",
      "error:  6.802932398519701e-07 step  21\n",
      "cost:  4.4829410471218\n",
      "opt took 0.01min,   21iters\n",
      "error:  1.1811008447937255e-06 step  21\n",
      "cost:  4.491282197660413\n",
      "opt took 0.01min,   21iters\n",
      "error:  2.1764204605201343e-07 step  21\n",
      "cost:  4.497823787814681\n",
      "opt took 0.01min,   21iters\n",
      "error:  3.0410223950116944e-07 step  21\n",
      "cost:  4.5057712257218\n",
      "opt took 0.01min,   21iters\n",
      "Epoch: [0][40/391]Time: 55.042 (28.344) Data: 54.943 (28.244) Loss: 4.8708 (4.9033)\n",
      "error:  1.455984067755267e-07 step  21\n",
      "cost:  4.511215723246226\n",
      "opt took 0.01min,   21iters\n",
      "error:  1.5397549490980822e-07 step  21\n",
      "cost:  4.513847992252684\n",
      "opt took 0.01min,   21iters\n",
      "error:  1.8341015994849386e-07 step  21\n",
      "cost:  4.519861613329956\n",
      "opt took 0.01min,   21iters\n",
      "error:  1.455629338176223e-07 step  21\n",
      "cost:  4.522512065053141\n",
      "opt took 0.01min,   21iters\n",
      "error:  9.234747455622028e-08 step  21\n",
      "cost:  4.526245603159695\n",
      "opt took 0.01min,   21iters\n",
      "Epoch: [0][50/391]Time: 0.126 (28.106) Data: 0.022 (28.006) Loss: 4.8686 (4.8963)\n",
      "error:  3.326091702060552e-08 step  21\n",
      "cost:  4.530033623161911\n",
      "opt took 0.01min,   21iters\n",
      "error:  3.092340827848261e-08 step  21\n",
      "cost:  4.532680907313058\n",
      "opt took 0.01min,   21iters\n",
      "error:  0.00822657224712775 step  11\n",
      "cost:  4.538729694039131\n",
      "opt took 0.01min,   11iters\n",
      "error:  0.0046862576396496936 step  11\n",
      "cost:  4.542219203354109\n",
      "opt took 0.01min,   11iters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b27c0c5f1f00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mselflabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselflabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfeature_return_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknn_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfeature_return_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-c7b161dad4af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, selflabels)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhc\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mfeature_return_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mselflabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_sk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselflabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhc\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mfeature_return_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-f54a559fc991>\u001b[0m in \u001b[0;36mopt_sk\u001b[0;34m(model, selflabels_in, epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mPS_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_selected\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    selflabels = train(epoch, selflabels)\n",
    "    feature_return_switch(model, True)\n",
    "    acc = kNN(model, trainloader, testloader, K=10, sigma=0.1, dim=knn_dim)\n",
    "    feature_return_switch(model, False)\n",
    "    writer.add_scalar(\"accuracy kNN\", acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/best_ckpt.t7' % (exp))\n",
    "        best_acc = acc\n",
    "    if epoch % 100 == 0:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'opt': optimizer.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'L': selflabels,\n",
    "        }\n",
    "        if not os.path.isdir(exp):\n",
    "            os.mkdir(exp)\n",
    "        torch.save(state, '%s/ep%s.t7' % (exp, epoch))\n",
    "    if epoch % 50 == 0:\n",
    "        feature_return_switch(model, True)\n",
    "        acc = kNN(model, trainloader, testloader, K=[50, 10],\n",
    "                  sigma=[0.1, 0.5], dim=knn_dim, use_pca=True)\n",
    "        i = 0\n",
    "        for num_nn in [50, 10]:\n",
    "            for sig in [0.1, 0.5]:\n",
    "                writer.add_scalar('knn%s-%s' % (num_nn, sig), acc[i], epoch)\n",
    "                i += 1\n",
    "        feature_return_switch(model, False)\n",
    "    print('best accuracy: {:.2f}'.format(best_acc * 100))\n",
    "\n",
    "checkpoint = torch.load('%s'%exp+'/best_ckpt.t7' )\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "feature_return_switch(model, True)\n",
    "acc = kNN(model, trainloader, testloader, K=10, sigma=0.1, dim=knn_dim, use_pca=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
